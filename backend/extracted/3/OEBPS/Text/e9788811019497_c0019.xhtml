<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:ibooks="http://apple.com/ibooks/html-extensions" lang="it-IT" xml:lang="it-IT" class="calibre">
  <head>
    <title>FILE 19 – Supremacy – Capitolo</title>
    <meta content="urn:uuid:35484d83-cd47-4b21-bd76-ea9d55abc8bb" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="../../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../../page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body epub:type="bodymatter" class="calibre2">
		<section epub:type="chapter" role="doc-chapter" class="calibre">
			<h2 class="capitolo" id="h2-0001"><span class="smallcaps1">14. un vago presentimento di sventura</span></h2>
			<p class="testo" id="p_0001">Con il lancio di Chat<span class="smallcaps">gpt</span>, Sam Altman aveva innescato diverse corse. La prima era ovvia: chi avrebbe portato per primo sul mercato il miglior modello linguistico di grandi dimensioni? L’altra si stava svolgendo sullo sfondo: chi avrebbe controllato la narrazione sull’<span class="smallcaps">ai</span>?</p>
			<p class="testo" id="p_0002">Nel marzo del 2023, poche settimane dopo i frettolosi lanci di Bing e Bard da parte di Microsoft e Google, Eliezer Yudkowsky scrisse un articolo di duemila parole per il «Time» riguardo alla direzione presa dall’<span class="smallcaps">ai</span>, dipingendo un futuro terrificante dominato da macchine più intelligenti dell’uomo.</p>
			<p class="testo" id="p_0003">«Molti ricercatori esperti in queste tematiche, me compreso, si aspettano che il risultato più probabile della creazione di un’<span class="smallcaps">ai</span> superumana, in condizioni anche solo vagamente simili a quelle attuali, sia letteralmente la morte di ogni essere umano sulla Terra», scrisse.</p>
			<p class="testo" id="p_0004">Quello stesso mese, una lettera aperta firmata da Elon Musk e altri leader tecnologici chiedeva una «moratoria» di sei mesi nella ricerca sull’<span class="smallcaps">ai</span> a causa dei rischi per l’umanità. «Dovremmo sviluppare menti non umane che potrebbero, alla lunga, superarci in numero e in intelligenza, renderci obsoleti e sostituirci?» diceva la lettera, redatta dal Future of Life Institute di Jaan Tallinn. «Dovremmo rischiare di perdere il controllo della civiltà?» La lettera, che contava quasi trentaquattromila firmatari, fece il giro del mondo, rilanciata anche da Reuters, Bloomberg, il «New York Times» e il «Wall Street Journal».</p>
			<p class="testo" id="p_0005">Ulteriori titoli sensazionalistici vennero dedicati a due ricercatori considerati i «padrini» dell’<span class="smallcaps">ai</span> – Geoffrey Hinton e Yoshua Bengio – dopo che questi avevano messo in guardia a mezzo stampa circa la minaccia esistenziale alla razza umana rappresentata dalla stessa <span class="smallcaps">ai</span>. Mentre Bengio ammetteva di sentirsi «spaesato» rispetto a quello che pure era stato il lavoro della sua vita, Hinton si dichiarò pentito di alcune sue ricerche.</p>
			<p class="testo" id="p_0006">«L’idea che questa roba potesse diventare effettivamente più intelligente delle persone… be’, ci credevano in pochi», confessò al «New York Times». «In ogni caso, la maggior parte della gente la collocava su un orizzonte molto lontano. Io stesso pensavo che servissero trenta, cinquant’anni o anche di più. Ma adesso non lo penso più. […] Credo che non dovrebbero potenziare ulteriormente questa tecnologia, se prima non sono sicuri di poterla controllare.»</p>
			<p class="testo" id="p_0007">I nomi più importanti dell’<span class="smallcaps">ai</span> sembravano concordare su un fatto: lo sviluppo dell’<span class="smallcaps">ai</span> stava correndo senza freni e rischiava di sfuggire catastroficamente di mano. L’idea di una minaccia di estinzione causata dall’<span class="smallcaps">ai</span> stava diventando un tema ricorrente nel dibattito pubblico, tanto che la si poteva sollevare a cena con i suoceri per vederli annuire, riconoscendone l’importanza. Il pubblico mainstream sembrava affascinato. Alla fine del 2023, circa il 22 per cento degli americani credeva che l’<span class="smallcaps">ai</span> avrebbe causato l’estinzione degli esseri umani nel giro di mezzo secolo, secondo un sondaggio condotto su 2.444 adulti statunitensi dalla Rethink Priorities.</p>
			<p class="testo" id="p_0008">Eppure, tutta questa retorica sull’imminenza della fine ebbe un effetto paradossale sullo stesso business dell’<span class="smallcaps">ai</span>, facendolo esplodere. Secondo Pitchbook, una società di ricerche di mercato, nel 2023 i finanziamenti per le start-up che sviluppavano prodotti di <span class="smallcaps">ai</span> generativa superarono i 21 miliardi di dollari, rispetto ai circa 5 dell’anno precedente.</p>
			<p class="testo" id="p_0009">Il messaggio implicito della minaccia legata a un’<span class="smallcaps">ai</span> fuori controllo era allettante. Se questa tecnologia rischiava di distruggere la razza umana in futuro, non significava anche che fosse abbastanza potente da far crescere il tuo business adesso?</p>
			<p class="testo" id="p_0010">E più Sam Altman parlava della minaccia della tecnologia di Open<span class="smallcaps">ai</span> – dicendo al Congresso che strumenti come Chat<span class="smallcaps">gpt</span> potevano «causare danni significativi al mondo intero», per esempio –, più soldi e attenzione attirava. Nel gennaio del 2023, Open<span class="smallcaps">ai</span> ottenne un altro investimento da Microsoft, questa volta del valore di 10 miliardi di dollari, in cambio di una quota del 49 per cento nella società. Microsoft era ormai a un passo dal controllo completo di Open<span class="smallcaps">ai</span>.</p>
			<p class="testo" id="p_0011">Anche Anthropic, la nuova azienda fondata da Dario Amodei e da un gruppo di altri ricercatori fuoriusciti da Open<span class="smallcaps">ai</span>, stava attirando ingenti investimenti. Alla fine del 2023, aveva accettato un investimento di 2 miliardi di dollari da Google e uno di 1,3 miliardi di dollari da Amazon. Nel giro di un anno, il suo valore era quadruplicato, superando i 20 miliardi di dollari. Sembrava che creare una super <span class="smallcaps">ai</span> super sicura potesse anche renderti super prezioso. Dietro le quinte, Anthropic puntava a raccogliere fino a 5 miliardi di dollari per entrare in oltre una dozzina di settori e sfidare Open<span class="smallcaps">ai</span>, secondo i documenti aziendali ottenuti dal blog TechCrunch. «Questi modelli potrebbero iniziare ad automatizzare grandi porzioni dell’economia», si leggeva nei documenti aziendali. Seguiva poi la considerazione che si trattava di una corsa in cui Anthropic avrebbe potuto rimanere in testa per molti anni, se fosse riuscita a realizzare i modelli «migliori» entro il 2026.</p>
			<p class="testo" id="p_0012">L’enfasi sulla sicurezza aveva fatto passare Anthropic per un’organizzazione non profit con la missione di «garantire che l’<span class="smallcaps">ai</span> trasformativa» aiutasse «le persone e la società a prosperare». Tuttavia, il successo strabiliante di Open<span class="smallcaps">ai</span> con Chat<span class="smallcaps">gpt</span> aveva dimostrato al mondo che le aziende con i progetti più ambiziosi potevano anche rivelarsi gli investimenti più redditizi. Proclamare di voler costruire un’<span class="smallcaps">ai</span> più sicura era quasi diventato un richiamo implicito per le grandi aziende tecnologiche desiderose di entrare in gioco.</p>
			<p class="testo" id="p_0013">Per giustificare questa logica, Anthropic doveva arrampicarsi sugli specchi. Per capire come rendere i sistemi di <span class="smallcaps">ai</span> più sicuri, non poteva semplicemente studiare i sistemi di <span class="smallcaps">ai</span> più potenti al mondo: doveva costruirli. Da qui, l’occhiolino alle grandi aziende tecnologiche, le uniche detentrici di una potenza di calcolo su vasta scala. Nel quadro dell’accordo con Google, per esempio, Anthropic avrebbe ricevuto crediti per il cloud computing che le avrebbero permesso di costruire un modello linguistico di grandi dimensioni in grado di rivaleggiare con quello di Open<span class="smallcaps">ai</span>.</p>
			<p class="testo" id="p_0014">Ormai c’erano due gruppi distinti che chiedevano un’<span class="smallcaps">ai</span> più sicura. Da un lato c’erano persone come Altman e Amodei, che avevano firmato un’altra lettera aperta in cui dichiaravano che «mitigare il rischio di estinzione causato dall’<span class="smallcaps">ai</span>» doveva essere «una priorità globale, al pari di altri rischi come le pandemie e la guerra nucleare». Questo gruppo rientrava sotto l’etichetta di «sicurezza dell’<span class="smallcaps">ai</span>», e dipingeva la minaccia futura in termini vaghi, di rado spiegando cosa avrebbero fatto i sistemi di <span class="smallcaps">ai</span> una volta fuori controllo né quando questo sarebbe accaduto. Inoltre, quando esponevano queste preoccupazioni davanti al Congresso, tendevano a sostenere una regolamentazione leggera.</p>
			<p class="testo" id="p_0015">Dall’altro lato c’erano persone che, come Timnit Gebru e Margaret Mitchell, denunciavano da anni i rischi che l’<span class="smallcaps">ai</span> rappresentava già per la società. Questo gruppo, definito sotto il cappello di «etica dell’<span class="smallcaps">ai</span>», era composto in prevalenza da donne e persone di colore che avevano esperienza diretta degli stereotipi e temevano che i sistemi di <span class="smallcaps">ai</span> continuassero a perpetuare le disuguaglianze. Col tempo, questo gruppo si mostrò sempre più indignato per le azioni di quelli nel campo della «sicurezza dell’<span class="smallcaps">ai</span>», non da ultimo perché guadagnavano enormi somme di denaro.</p>
			<p class="testo" id="p_0016">La disparità di finanziamenti era netta e il fronte etico era spesso costretto ad arrangiarsi con fondi limitati. L’European Digital Rights Initiative, una rete di enti non profit che da ventun anni si batteva contro il riconoscimento facciale e gli algoritmi di bias, aveva un budget annuale di soli 2,2 milioni di dollari nel 2023. Allo stesso modo, l’<span class="smallcaps">ai</span> Now Institute di New York, che analizzava il modo in cui veniva utilizzata l’<span class="smallcaps">ai</span> nella sanità e nel sistema di giustizia penale, aveva un budget inferiore al milione di dollari.</p>
			<p class="testo" id="p_0017">I gruppi focalizzati sulla «sicurezza» dell’<span class="smallcaps">ai</span> e sulla minaccia di estinzione ricevevano molti più finanziamenti, spesso tramite benefattori miliardari. Nel 2021 il già citato Future of Life Institute, un’organizzazione non profit con sede a Cambridge, nel Massachusetts, che studiava come impedire all’<span class="smallcaps">ai</span> di accedere alle armi, aveva ricevuto 25 milioni di dollari dal magnate delle criptovalute Vitalik Buterin. Quel singolo finanziamento superava i bilanci annuali combinati di tutti i gruppi di etica dell’<span class="smallcaps">ai</span>.</p>
			<p class="testo" id="p_0018">Open Philanthropy, il veicolo filantropico del miliardario di Facebook Dustin Moskovitz, ha elargito negli anni numerose sovvenzioni da milioni di dollari alla causa della sicurezza dell’<span class="smallcaps">ai</span>, inclusa una donazione da 5 milioni di dollari al Center for <span class="smallcaps">ai</span> Safety nel 2022 e una da 11 milioni al Center for Human-Compatible <span class="smallcaps">ai</span> di Berkeley.</p>
			<p class="testo" id="p_0019">Nel complesso, la fondazione di Moskovitz è stata il più grande finanziatore della sicurezza dell’<span class="smallcaps">ai</span>, in virtù del patrimonio di quasi 14 miliardi di dollari che lui e sua moglie, Cari Tuna, pianificano di destinare principalmente in beneficenza. Nel conto c’è anche una donazione di 30 milioni di dollari versata a Open<span class="smallcaps">ai</span> nel momento in cui venne costituita come organizzazione non profit.</p>
			<p class="testo" id="p_0020">Perché così tanti soldi sono andati agli ingegneri che lavorano su sistemi di <span class="smallcaps">ai</span> sempre più grandi con il pretesto di renderli più sicuri in futuro, e così pochi ai ricercatori impegnati ad analizzarli oggi? La risposta risiede in parte nell’ossessione che ha preso piede nella Silicon Valley circa il modo più efficiente di «fare del bene» e nelle idee diffuse da un piccolo gruppo di filosofi della Oxford University.</p>
			<p class="testo" id="p_0021">Negli anni Ottanta, un filosofo di Oxford, Derek Parfit, iniziò a scrivere di un nuovo tipo di etica utilitaristica che guardava molto lontano nel futuro. Immaginate, diceva, di lasciare una bottiglia rotta per terra e che, cento anni dopo, un bambino si tagli il piede su quei vetri. Anche se quel bambino non è ancora nato, il peso della colpa ricadrebbe su di voi come se si fosse ferito oggi.</p>
			<p class="testo" id="p_0022">«Il suo pensiero di base, molto semplice, è che da un punto di vista morale le persone del futuro contano quanto quelle del presente», dice David Edmonds, autore di una biografia su Parfit nel 2023. «Immaginiamo questi tre scenari. A: c’è la pace. B: 7,5 degli 8 miliardi di persone nel mondo vengono sterminati in una guerra. C: tutti vengono uccisi. L’intuizione della maggior parte delle persone è che la differenza tra A e B sia molto maggiore di quella tra B e C. Ma Parfit sostiene che questo è sbagliato. La differenza tra B e C è molto più significativa della differenza tra A e B. Sterminando tutta l’umanità, si annientano anche tutte le generazioni future.»</p>
			<p class="testo" id="p_0023">Ecco un modo per quantificare la questione. I mammiferi, intesi come classe, hanno una «durata di vita» media di circa un milione di anni, e gli esseri umani sono presenti sulla Terra da circa duecentomila anni. Teoricamente, questo dovrebbe concederci altri ottocentomila anni sul pianeta. Se l’attuale popolazione mondiale si stabilizzasse a undici miliardi di persone, secondo le proiezioni delle Nazioni Unite per la fine di questo secolo, e la durata media della vita salisse a ottantotto anni, ciò potrebbe significare, secondo una stima, che devono ancora nascere <em class="calibre3">centomila miliardi di persone</em>.</p>
			<p class="testo" id="p_0024">Per aiutarvi a visualizzare questi numeri, immaginate un coltello e un singolo fagiolo sul tavolo della vostra sala da pranzo. Il coltello rappresenta il numero di persone già vissute e morte. Il fagiolo rappresenta tutte le persone vive al presente. La superficie del tavolo è il numero di persone che devono ancora nascere (e potrebbe essere molto più grande, se gli esseri umani dovessero dimostrarsi più longevi della media dei mammiferi).</p>
			<p class="testo" id="p_0025">Nel 2009, un filosofo australiano di nome Peter Singer ampliò il lavoro di Parfit con un libro intitolato <em class="calibre3">La cosa migliore che tu puoi fare</em>, nel quale propose una soluzione: le persone ricche non dovrebbero semplicemente donare denaro in base a ciò che sembra giusto, ma adottare un approccio più razionale per massimizzare l’impatto delle loro donazioni e aiutare il maggior numero possibile di persone. Aiutando molte di quelle persone non ancora nate, si poteva essere ancora più virtuosi.</p>
			<p class="testo" id="p_0026">Queste idee iniziarono a filtrare dai paper accademici al mondo reale, creando le basi di un’ideologia nel 2011, quando un filosofo ventiquattrenne di Oxford, Will MacAskill, cofondò un gruppo chiamato 80,000 Hours. Il numero si riferiva alla media delle ore lavorative nella vita di una persona, e l’organizzazione si rivolgeva ai campus universitari statunitensi, offrendo consulenza ai giovani laureati sulle carriere che avrebbero avuto il maggiore impatto morale. Spesso indirizzava quelli con competenze tecniche verso il settore della sicurezza in campo <span class="smallcaps">ai</span>. Ma il gruppo incoraggiava anche i laureati a scegliere carriere che garantissero gli stipendi più alti, in modo da poter donare quanti più soldi possibile a cause a elevato impatto.</p>
			<p class="testo" id="p_0027">MacAskill e il suo giovane team alla fine si riorganizzarono come Center for Effective Altruism, facendo nascere un nuovo credo. Come abbiamo accennato in precedenza, l’idea alla base dell’altruismo efficace era l’efficienza. Le persone che vivevano nei paesi ricchi avevano l’obbligo di aiutare gli abitanti dei paesi più disagiati, perché lì avrebbero potuto ottenere il massimo risultato con il minimo sforzo. In parole povere, si potevano aiutare più persone in Africa tramite le organizzazioni di salute globale che donando ai poveri in America. Era anche moralmente preferibile dedicare il proprio tempo a guadagnare più soldi possibile, per poi donarli, come aveva fatto lo stesso Dustin Moskovitz. Quando parlava agli studenti, MacAskill mostrava una slide in cui chiedeva se avrebbero potuto fare più del bene come medici o come banchieri. Secondo la sua teoria, era meglio diventare banchieri. Come medici, sarebbe stato possibile salvare un certo numero di vite in Africa; come banchieri, si potevano assumere diversi medici per salvarne molte di più.</p>
			<p class="testo" id="p_0028">Questo offriva ai laureati un modo controintuitivo di guardare tutte le disuguaglianze del capitalismo moderno. Ora non c’era nulla di sbagliato in un sistema che permetteva a una manciata di persone di diventare miliardarie. Accumulando ricchezze inimmaginabili, avrebbero potuto aiutare gli altri!</p>
			<p class="testo" id="p_0029">Il movimento accolse quello che sarebbe diventato il suo membro più importante nel 2012, quando MacAskill contattò uno studente del <span class="smallcaps">mit</span> con i capelli ricci e scuri, Sam Bankman-Fried, nella speranza di reclutarlo. I due presero un caffè e venne fuori che Bankman-Fried, già ammiratore di Peter Singer, era interessato alle cause legate al benessere degli animali.</p>
			<p class="testo" id="p_0030">MacAskill distolse Bankman-Fried dall’idea di lavorare direttamente per la causa animalista e gli disse che avrebbe potuto fare molto di più per gli animali entrando in un settore ad alta remunerazione. Bankman-Fried fu subito conquistato, secondo il racconto di Michael Lewis in <em class="calibre3">Verso l’infinito e oltre</em>. «Quel che diceva mi sembrò giusto», raccontò Bankman-Fried nel libro. Accettò quindi un lavoro in una società di trading quantitativo e infine, nel 2019, fondò l’exchange di criptovalute <span class="smallcaps">ftx</span>.</p>
			<p class="testo" id="p_0031">Bankman-Fried mise l’altruismo efficace al centro della sua impresa. I suoi cofondatori e il team dirigente erano altruisti efficaci e nominarono MacAskill membro del <span class="smallcaps">ftx</span> Future Fund, che nel 2022 avrebbe donato 160 milioni di dollari a cause legate all’altruismo efficace, alcune delle quali direttamente connesse allo stesso MacAskill. Bankman-Fried parlava spesso alla stampa del fatto che avrebbe donato tutti i suoi soldi, e in grandi poster pubblicitari di <span class="smallcaps">ftx</span> compariva con la consueta t-shirt e i soliti pantaloncini cargo, affiancato dalle parole: «Sono nel mondo delle cripto perché voglio avere il massimo impatto globale positivo». Si presentava come un personaggio ascetico che, nonostante i miliardi sul conto, guidava una Toyota Corolla, viveva con dei coinquilini e spesso appariva trasandato.</p>
			<p class="testo" id="p_0032">Molti tecnologici percepirono in questo approccio alla moralità una ventata d’aria fresca. Quando gli ingegneri riscontravano un problema, spesso lo risolvevano in modo sistematico, correggendo il codice e ottimizzando il software tramite test e valutazioni costanti. Ora si potevano anche quantificare i dilemmi morali, quasi alla stregua di problemi matematici. Nei circoli dell’altruismo efficace, talvolta si parlava di massimizzare l’efficacia di un atto caritatevole concentrandosi sul «valore atteso», un numero ottenuto moltiplicando il valore di un risultato per la probabilità che si verificasse.</p>
			<p class="testo" id="p_0033">Man mano che l’altruismo efficace guadagnava terreno nella Silicon Valley, il suo focus si spostava dall’acquisto di zanzariere contro la malaria e dal sostegno al maggior numero possibile di persone in Africa verso questioni dal sapore più fantascientifico. Elon Musk, che su Twitter aveva giudicato il libro di MacAskill del 2022, <em class="calibre3">Che cosa dobbiamo al futuro</em>, come «molto vicino» alla sua filosofia, voleva inviare esseri umani su Marte per garantire la sopravvivenza a lungo termine della specie. E più i sistemi di intelligenza artificiale diventavano sofisticati, più appariva sensato preoccuparsi di evitare che, impazzendo, spazzassero via l’umanità. Molti dei dipendenti di Open<span class="smallcaps">ai</span>, Anthropic e DeepMind erano altruisti efficaci.</p>
			<p class="testo" id="p_0034">Agire in base al rischio di estinzione legato all’intelligenza artificiale richiede un calcolo razionale. Anche se esistesse solo lo 0,00001 per cento di probabilità che l’<span class="smallcaps">ai</span> possa estinguere l’umanità, il costo sarebbe così enorme da risultare, in pratica, infinito. Moltiplicando una probabilità infima per un costo infinito, otterrete comunque un problema di dimensioni smisurate. Questa logica diventa ancora più stringente per chi crede, come alcuni sostenitori della sicurezza in ambito <span class="smallcaps">ai</span>, che i computer del futuro ospiteranno le menti coscienti di miliardi di persone e creeranno anche nuove forme di vita senziente e digitale. Quei centomila miliardi di persone che devono ancora nascere potrebbero essere un numero molto più alto. Seguendo alla lettera questa logica morale, è perfettamente sensato dare priorità alla minuscola possibilità di dover salvare più di centomila miliardi di vite fisiche e digitali dalla distruzione. La povertà globale, al confronto, non è che una minuzia.</p>
			<p class="testo" id="p_0035">Dopo il lancio di Open<span class="smallcaps">ai</span>, nel 2015, cominciarono a piovere finanziamenti verso le cause legate all’eventuale estinzione provocata dall’<span class="smallcaps">ai</span>. La Open Philanthropy di Moskovitz incrementò il numero di sovvenzioni destinate alle cosiddette cause «a lungo termine», inclusa la ricerca sulla sicurezza in ambito <span class="smallcaps">ai</span>. Dai 2 milioni di dollari nel 2015 si arrivò agli oltre 100 milioni nel 2021.</p>
			<p class="testo" id="p_0036">Anche Bankman-Fried si era lanciato. Il suo <span class="smallcaps">ftx</span> Future Fund, gestito da altruisti efficaci come Nick Beckstead e MacAskill, aveva promesso di donare un miliardo di dollari a progetti volti a «migliorare le prospettive a lungo termine dell’umanità». Al momento di elencare le aree di interesse del fondo, la prima voce era «lo sviluppo sicuro dell’intelligenza artificiale».</p>
			<p class="testo" id="p_0037">Quando il «New Yorker» decise di pubblicare un articolo sulla vita all’interno del Future Fund, notò che le chiacchiere d’ufficio nella sede di Berkeley, in California, spesso sfociavano in discussioni su quando sarebbe potuta accadere un’apocalisse legata all’<span class="smallcaps">ai</span>.</p>
			<p class="testo" id="p_0038">«Che tempistiche hai?» si chiedevano spesso l’un l’altro i membri del team. «Qual è la tua p(doom)?»</p>
			<p class="testo" id="p_0039">La «p» stava per probabilità e la domanda riguardava il grado in cui ciascuno quantificava il rischio di un’apocalisse causata dall’<span class="smallcaps">ai</span>. Chi aveva una visione più ottimistica poteva stimare la propria p(doom) al 5 per cento. Ajeya Cotra, analista di ricerca di Open Philanthropy che contribuiva a decidere le sovvenzioni, dichiarò in un podcast che la sua p(doom) si collocava tra il 20 e il 30 per cento.</p>
			<p class="testo" id="p_0040">Nessuno conosceva la p(doom) di Bankman-Fried, ma questi teneva abbastanza alla sicurezza in tema <span class="smallcaps">ai</span> da investire 500 milioni di dollari in Anthropic. Anche i cofondatori di <span class="smallcaps">ftx,</span> nonché altruisti efficaci, Nishad Singh e Caroline Ellison, avevano investito nella start-up che si era separata da Open<span class="smallcaps">ai</span> circa un anno prima.</p>
			<p class="testo" id="p_0041">All’inizio del 2022, MacAskill notò un tweet in cui Musk affermava di voler acquistare Twitter per salvare la libertà di espressione. Il filosofo scozzese gli inviò un messaggio. A quel tempo, con un patrimonio di 24 miliardi di dollari, Bankman-Fried era uno degli altruisti efficaci più ricchi del globo. Ma la fortuna di Musk, pari a 220 miliardi di dollari, avrebbe potuto trasformare da sola «l’altruismo efficace» nel movimento filantropico più grande del mondo.</p>
			<p class="testo" id="p_0042">MacAskill disse a Musk che anche Bankman-Fried voleva acquistare Twitter con lo scopo di «renderlo migliore per il mondo». Perché non unire le loro forze?</p>
			<p class="testo" id="p_0043">«Ha enormi quantità di denaro?» chiese Musk.</p>
			<p class="testo" id="p_0044">«Dipende dalla definizione di “enormi”!» rispose MacAskill, secondo i documenti processuali. MacAskill precisò che Bankman-Fried poteva contribuire con una somma fino a 8 miliardi di dollari.</p>
			<p class="testo" id="p_0045">«È un inizio», disse Musk.</p>
			<p class="testo" id="p_0046">«Vuoi che ti metta in contatto via messaggio?» chiese MacAskill.</p>
			<p class="testo" id="p_0047">Musk non rispose. «Puoi garantire per lui?» chiese piuttosto.</p>
			<p class="testo" id="p_0048">«Assolutamente sì!» rispose MacAskill. «La sua priorità è fare in modo che il futuro a lungo termine dell’umanità sia positivo.»</p>
			<p class="testo" id="p_0049">«Okay, allora va bene.»</p>
			<p class="testo" id="p_0050">«Fantastico!»</p>
			<p class="testo" id="p_0051">Anche se Musk contattò Bankman-Fried, alla fine non raggiunsero mai un accordo finanziario, e questo consentì a Musk di schivare una brutta grana. Qualche mese dopo, infatti, <span class="smallcaps">ftx</span> crollò tra le voci secondo cui Bankman-Fried aveva trasferito in maniera fraudolenta i fondi dei clienti all’interno dell’azienda. Al processo, i pubblici ministeri lo accusarono di aver stornato 8 miliardi di dollari da migliaia di clienti e investitori, e chiesero una condanna a parecchi anni di carcere. A dispetto dell’immagine ascetica che si era creato, emerse che Bankman-Fried viveva in un lussuoso attico alle Bahamas e investiva centinaia di milioni di dollari in vari progetti. Gran parte del denaro che aveva destinato all’altruismo efficace era andato in fumo. E, come se non bastasse, venne fuori che quell’altruismo non era stato nemmeno così sincero.</p>
			<p class="testo" id="p_0052">Subito dopo il crollo di <span class="smallcaps">ftx</span>, Bankman-Fried concesse un’intervista sorprendente al sito di notizie Vox.</p>
			<p class="testo" id="p_0053">«Quindi tutta quella faccenda dell’etica era principalmente una facciata?» chiese la giornalista Kelsey Piper.</p>
			<p class="testo" id="p_0054">«Sì», rispose Bankman-Fried.</p>
			<p class="testo" id="p_0055">«Era davvero bravo a parlare di etica, per uno che in fondo vedeva tutto come un gioco con vincitori e vinti», osservò la reporter.</p>
			<p class="testo" id="p_0056">«Già», ammise Bankman-Fried. «Eh eh. Bisognava che lo fossi.»</p>
			<p class="testo" id="p_0057">Il crollo di <span class="smallcaps">ftx</span> gettò una lunga ombra sulla reputazione dell’altruismo efficace e divenne un’allegoria di alcuni dei problemi fondamentali del movimento. Il primo era prevedibile: quando le persone intraprendevano una missione volta al massimo bene possibile cercando anche la massima ricchezza, probabilmente si rendevano più suscettibili a comportamenti corrotti e a giudizi avventati e motivati dall’ego. Acquistare Twitter, per esempio, non rispondeva a nessuno dei criteri utili ad aiutare l’umanità sul lungo termine, ma Bankman-Fried era pronto a spendere fino a 8 miliardi di dollari per rilevare il sito insieme a Musk e mettersi su un piedistallo con l’uomo più ricco del mondo, come atto di altruismo efficace.</p>
			<p class="testo" id="p_0058">Dopo il tracollo di <span class="smallcaps">ftx</span>, MacAskill si precipitò su Twitter per tentare di correre ai ripari: «Un [altruista efficace] che ragioni con lucidità dovrebbe opporsi fermamente all’idea secondo cui “il fine giustifica i mezzi”», scrisse. Tuttavia, i principi stessi del movimento incentivavano individui come Bankman-Fried a raggiungere i propri obiettivi con qualsiasi mezzo necessario, anche a costo di sfruttare gli altri. Questo generava una miopia tale da colpire persino un brillante accademico di Oxford come MacAskill, che aveva scelto di legarsi a una persona a capo di un exchange di criptovalute, pur sapendo bene che il settore era nel migliore dei casi altamente speculativo e, nel peggiore, una forma pericolosa di gioco d’azzardo.</p>
			<p class="testo" id="p_0059">Bankman-Fried poteva razionalizzare la sua doppiezza pensando di lavorare per un obiettivo più grande: massimizzare la felicità umana. Musk poteva liquidare le proprie azioni discutibili – dal definire senza prove «pedofili» alcune persone su Twitter alle accuse di razzismo diffuso nelle fabbriche Tesla – perché perseguiva traguardi più ambiziosi, come trasformare Twitter in un’utopia della libertà di espressione o rendere l’umanità una specie interplanetaria. E anche i fondatori di Open<span class="smallcaps">ai</span> e DeepMind potevano giustificare il loro crescente appoggio ai colossi della tecnologia nello stesso modo: se alla fine avessero realizzato l’<span class="smallcaps">agi</span>, avrebbero operato per il bene dell’umanità.</p>
			<p class="testo" id="p_0060">Tecnologi come Altman e Hassabis sapevano che i problemi sociali che speravano di risolvere con l’<span class="smallcaps">agi</span> erano complessi e intricati. E proprio per questo tanti di loro abbracciavano in parte o del tutto l’altruismo efficace: offriva un percorso più semplice e razionale per affrontare dilemmi morali, permettendo al contempo di accumulare il massimo profitto possibile. I miliardari non erano la causa della povertà globale, ma la soluzione.</p>
			<p class="testo" id="p_0061">Questa filosofia rendeva anche più facile dissociarsi dall’umanità. Un’espressione diffusa tra gli altruisti efficaci è <em class="calibre3">shut up and multiply</em> («taci e moltiplica»), a significare che, quando si prendono decisioni etiche, bisogna massimizzare l’impatto mettendo da parte emozioni personali o intuizioni morali. Per quanto gli altruisti efficaci si dichiarassero devoti al benessere dell’umanità, molti di loro, come Altman, si distaccavano emotivamente dal mondo circostante per concentrarsi meglio sulla loro missione. All’interno della bolla dell’altruismo efficace, le persone lavoravano, socializzavano, si finanziavano a vicenda e intrecciavano relazioni sentimentali.</p>
			<p class="testo" id="p_0062">Quando Open Philanthropy stanziò 30 milioni di dollari a favore di Open<span class="smallcaps">ai</span> nel 2017, fu costretta a rivelare che riceveva consulenze tecniche da Dario Amodei, all’epoca ingegnere senior presso la non profit. Dovette inoltre ammettere che Amodei viveva nella stessa casa del direttore esecutivo di Open Philanthropy, Holden Karnofsky. E aggiungere che Karnofsky era fidanzato con Daniela, sorella di Dario, che a sua volta lavorava per Open<span class="smallcaps">ai</span>. Erano tutti altruisti efficaci. Un circolo chiuso, quasi incestuoso.</p>
			<p class="testo" id="p_0063">Il movimento era sempre più isolato e opaco, e lo stesso valeva per le aziende di <span class="smallcaps">ai</span> come Open<span class="smallcaps">ai</span>, DeepMind e Anthropic, popolate da molti dei suoi seguaci. Probabilmente, una delle mosse più efficaci che queste aziende avrebbero potuto fare per impedire che l’<span class="smallcaps">ai</span> sfuggisse al controllo sarebbe stata quella di rendere i loro sistemi più trasparenti, come da tempo sostenevano Gebru e Mitchell. Dopotutto, come avrebbero potuto gli esseri umani del futuro fermare un’<span class="smallcaps">ai</span> fuori controllo se non avessero avuto modo di studiarne il funzionamento? Se per decenni i ricercatori fossero stati esclusi dall’analisi dei dati di addestramento e degli algoritmi? In altre parole, la trasparenza richiesta oggi dagli attivisti dell’etica in campo <span class="smallcaps">ai</span> poteva essere la chiave per affrontare la minaccia esistenziale di domani.</p>
			<p class="testo" id="p_0064">L’argomentazione di Open<span class="smallcaps">ai</span> secondo cui doveva mantenere il segreto per impedire ai malintenzionati di abusare della sua tecnologia non reggeva. Nel novembre del 2019, l’azienda aveva dato il via libera al lancio di <span class="smallcaps">gpt</span>-2 dichiarando di non aver trovato «prove solide di uso improprio». Se era vero, perché non rivelare i dettagli del suo addestramento? Più probabilmente, Altman voleva proteggere Open<span class="smallcaps">ai</span> dalla concorrenza e dalle cause legali. Se Open<span class="smallcaps">ai</span> fosse diventata più trasparente, sarebbe stato più facile per i rivali – e non per i malintenzionati – copiare i suoi modelli e svelare fino a che punto avesse sfruttato opere coperte da copyright.</p>
			<p class="testo" id="p_0065">Altman e Hassabis avevano fondato le loro aziende con l’ambiziosa missione di aiutare l’umanità, ma i veri benefici che avevano portato alle persone erano tanto nebulosi quanto lo erano stati, al tempo, quelli di internet e dei social media. Più chiari erano invece i vantaggi che stavano offrendo a Microsoft e Google: nuovi servizi di punta e un solido appiglio nel mercato in espansione dell’<span class="smallcaps">ai</span> generativa.</p>
			<p class="testo" id="p_0066">Microsoft aveva trasformato Copilot, l’assistente <span class="smallcaps">ai</span> basato sulla tecnologia di Open<span class="smallcaps">ai</span>, in un servizio a tutto campo per Windows, Word, Excel e il software aziendale Dynamics 365. Gli analisti stimano che la tecnologia di Open<span class="smallcaps">ai</span> potrebbe generare miliardi di dollari di ricavi annui per Microsoft entro il 2026. A un certo punto, alla fine del 2023, quando condivise il palco con Altman e gli venne chiesto come andasse la partnership tra Microsoft e Open<span class="smallcaps">ai</span>, Satya Nadella scoppiò in una risata incontrollabile. La risposta era talmente ovvia da rendere comica la domanda. Certo che la partnership andava a gonfie vele!</p>
			<p class="testo" id="p_0067">Microsoft continuava a investire massicciamente nel suo business dell’<span class="smallcaps">ai</span>, con piani di spesa superiori ai 50 miliardi di dollari nel 2024 e negli anni successivi per espandere i suoi giganteschi data center, i motori che alimentavano l’<span class="smallcaps">ai</span> generativa. Sarebbe stato uno dei più imponenti sviluppi infrastrutturali della storia, tanto da superare gli investimenti governativi in ferrovie, dighe e programmi spaziali. E anche Google stava espandendo i suoi data center.</p>
			<p class="testo" id="p_0068">All’inizio del 2024, tutti, dai media alle società di intrattenimento fino a Tinder, stavano inserendo nuove funzionalità di <span class="smallcaps">ai</span> generativa nelle proprie app e nei propri servizi. Il mercato dell’<span class="smallcaps">ai</span> generativa era destinato a crescere a un tasso annuo superiore al 35 per cento, fino a raggiungere i 52 miliardi di dollari entro il 2028. Le aziende di intrattenimento dichiaravano che l’<span class="smallcaps">ai</span> avrebbe accelerato la produzione di contenuti per film, serie <span class="smallcaps">tv</span> e videogiochi. Jeffrey Katzenberg, cofondatore di DreamWorks Animation e produttore di <em class="calibre3">Shrek</em> e <em class="calibre3">Kung Fu Panda</em>, dichiarò che l’<span class="smallcaps">ai</span> generativa avrebbe ridotto i costi dei film d’animazione del 90 per cento. «Ai bei vecchi tempi, servivano cinquecento artisti e anni di lavoro per realizzare un film d’animazione di livello mondiale», disse a una conferenza di Bloomberg nel novembre del 2023. «Credo che di qui a tre anni basterà meno del 10 per cento di quelle risorse.»</p>
			<p class="testo" id="p_0069">L’<span class="smallcaps">ai</span> generativa avrebbe reso la pubblicità ancora più personalizzata da apparire inquietante. Per anni, gli annunci pubblicitari erano stati in grado di prendere di mira grandi gruppi di persone contemporaneamente; ora, invece, potevano concentrarsi sul singolo individuo, con video iper-personalizzati capaci persino di pronunciare il suo nome. Secondo il World Economic Forum, i modelli linguistici avanzati avrebbero potenziato i lavori che richiedevano pensiero critico e creatività. Chiunque, dagli ingegneri ai copywriter pubblicitari fino agli scienziati, avrebbe potuto usarli come estensioni del proprio cervello. Nel frattempo, i governi stavano aggiornando i loro sistemi di <span class="smallcaps">ai</span> per valutare richieste di sussidi, monitorare spazi pubblici o persino stimare la probabilità che un determinato soggetto commetta un crimine.</p>
			<p class="testo" id="p_0070">Google, Microsoft e una nuova generazione di start-up erano insomma nel pieno di una corsa sfrenata per accaparrarsi il maggior numero possibile di opportunità in questo mercato, nel tentativo di ottenere un vantaggio competitivo. Quasi la metà dei membri dei consigli di amministrazione delle aziende americane considerava l’<span class="smallcaps">ai</span> generativa «la priorità assoluta, al di sopra di qualsiasi altra cosa» per le proprie imprese, secondo un sondaggio condotto da Fast Company alla fine del 2023. Un esempio? Ecco come la <span class="smallcaps">ceo</span> di Bumble descriveva i piani principali della popolare app di incontri per il 2024: «Vogliamo puntare davvero in grande sull’<span class="smallcaps">ai</span>», dichiarò. «L’intelligenza artificiale e l’<span class="smallcaps">ai</span> generativa possono giocare un ruolo fondamentale nell’accelerare il percorso dell’utente verso il partner giusto.»</p>
			<p class="testo" id="p_0071">Bumble voleva utilizzare la tecnologia alla base di Chat<span class="smallcaps">gpt</span> per creare veri e propri «matchmaker» virtuali. Invece di selezionare una serie di preferenze tramite un elenco di opzioni, gli utenti avrebbero semplicemente descritto al bot tutto ciò che cercavano in un partner: dal desiderio di avere figli alle opinioni politiche, fino al modo in cui trascorrevano solitamente il sabato mattina. Il matchmaker <span class="smallcaps">ai</span> avrebbe poi «dialogato» con i matchmaker degli altri utenti di Bumble per trovare la persona più compatibile. Invece di scorrere centinaia di profili, si poteva lasciare che lo facesse l’<span class="smallcaps">ai</span>.</p>
			<p class="testo" id="p_0072">Man mano che queste e altre idee imprenditoriali prendevano slancio, il prezzo da pagare per infilare ovunque l’<span class="smallcaps">ai</span> generativa restava ancora poco chiaro. Gli algoritmi guidavano già sempre più decisioni nelle nostre vite, determinando cosa leggere online e quali persone assumere in azienda. Ora si preparavano a gestire anche una parte dei nostri compiti intellettuali, sollevando interrogativi scomodi non solo sulla libertà d’azione umana, ma anche sulla nostra capacità di risolvere problemi e, più semplicemente, di immaginare.</p>
			<p class="testo" id="p_0073">Le prove suggeriscono che i computer si siano già fatti carico di alcune delle nostre capacità cognitive, soprattutto per quanto riguarda la memoria a breve termine. Nel 1955, il professore di psicologia di Harvard George Armitage Miller testò i limiti della memoria umana sottoponendo ai suoi soggetti sperimentali una lista casuale di colori, sapori e numeri. Quando chiese loro di ripeterne il maggior numero possibile, notò che tutti si bloccavano intorno a un limite ben preciso: sette elementi. Il suo celebre saggio, <em class="calibre3">Il magico numero sette, più o meno due</em>, influenzò la progettazione del software e persino il modo in cui le compagnie telefoniche segmentavano i numeri per facilitarne la memorizzazione. Secondo stime più recenti, tuttavia, quel numero magico è sceso da sette a quattro.</p>
			<p class="testo" id="p_0074">Alcuni chiamano questo fenomeno «Effetto Google». Affidandoci sempre di più al motore di ricerca per ricordare fatti o per ottenere indicazioni stradali, abbiamo di fatto delegato la nostra memoria a un’azienda, indebolendo inconsapevolmente le nostre capacità mnemoniche a breve termine. Potrebbe accadere lo stesso con aspetti ancora più profondi della nostra cognizione, se diventassimo eccessivamente dipendenti dall’<span class="smallcaps">ai</span> per generare idee, testi o opere d’arte? Su Twitter, alcuni sviluppatori di software hanno ammesso di usarla talmente tanto per scrivere righe di codice che la loro produttività crolla ogni volta che un servizio come Copilot finisce temporaneamente offline.</p>
			<p class="testo" id="p_0075">La storia dimostra che gli esseri umani tendono a preoccuparsi che le innovazioni possano atrofizzare il cervello. Quando la scrittura iniziò a diffondersi, più di duemila anni fa, filosofi come Socrate espressero il timore che avrebbe indebolito la memoria umana, dato che fino ad allora la conoscenza veniva trasmessa solo tramite il discorso orale. L’introduzione delle calcolatrici nell’istruzione sollevò preoccupazioni analoghe: gli studenti avrebbero perso le competenze aritmetiche di base.</p>
			<p class="testo" id="p_0076">Eppure, non conosciamo ancora tutti gli effetti collaterali derivanti dall’affidarci sempre di più a una tecnologia capace di sostituire il modo in cui il nostro cervello elabora il linguaggio. Una macchina in grado di generare testi, fare brainstorming e concepire un piano aziendale va ben oltre un semplice calcolatore o un motore di ricerca: sta sostituendo il pensiero astratto e la capacità di pianificazione.</p>
			<p class="testo" id="p_0077">Al momento, non sappiamo con certezza quanto le nostre capacità di pensiero critico o la nostra creatività possano atrofizzarsi quando una nuova generazione di professionisti inizierà a usare i modelli linguistici su larga scala come stampelle, né come cambieranno le nostre interazioni con gli altri esseri umani man mano che sempre più persone useranno chatbot come terapeuti, partner romantici o perfino giocattoli per bambini, come già accade con alcuni prodotti sul mercato. Secondo uno studio del 2023 condotto su un migliaio di adulti statunitensi, un americano su quattro preferirebbe parlare con un chatbot <span class="smallcaps">ai</span> piuttosto che con un terapeuta umano. E non c’è da stupirsi: se sottoposto a un test di intelligenza emotiva, Chat<span class="smallcaps">gpt</span> lo supererà brillantemente.</p>
			<p class="testo" id="p_0078">Lo stesso Altman ha ammesso che la tecnologia di Chat<span class="smallcaps">gpt</span> sconvolgerà l’economia, causando la perdita di numerosi posti di lavoro. Secondo i ricercatori, tuttavia, i modelli linguistici e altre forme di <span class="smallcaps">ai</span> generativa potrebbero anche aumentare le disuguaglianze di reddito. Il Fondo monetario internazionale prevede che l’uso di questi sistemi sposterà ulteriori investimenti verso le economie avanzate; secondo Joseph Stiglitz, premio Nobel per l’economia, ridurrà il potere contrattuale dei lavoratori.</p>
			<p class="testo" id="p_0079">Storicamente, quando robot e algoritmi hanno sostituito il lavoro umano, la crescita dei salari è rallentata, afferma l’economista del <span class="smallcaps">mit</span> Daron Acemoglu, coautore del libro <em class="calibre3">Potere e progresso</em>, che analizza l’influenza della tecnologia sulla prosperità economica. Secondo le sue stime, fino al 70 per cento dell’aumento della disuguaglianza salariale negli Stati Uniti tra il 1980 e il 2016 è da imputare all’automazione.</p>
			<p class="testo" id="p_0080">«L’aumento della produttività non si traduce necessariamente in vantaggi per i lavoratori coinvolti e, anzi, può portare a perdite significative», spiega Acemoglu. «Se l’<span class="smallcaps">ai</span> generativa seguirà la stessa traiettoria delle altre tecnologie di automazione, potrebbe avere le medesime implicazioni.»</p>
			<p class="testo" id="p_0081">Nel corso del 2023, sempre più studiosi si sono uniti a Timnit Gebru e Margaret Mitchell nel denunciare questi e altri effetti concreti dell’<span class="smallcaps">ai</span> generativa. Invece di affrontare direttamente questi problemi e promuovere una maggiore trasparenza, però, Sam Altman sembrava concentrato su un’altra strategia: cercare di plasmare la politica governativa.</p>
			<p class="testo" id="p_0082">Nel maggio del 2023, si presentò davanti a una commissione del Senato per discutere dei pericoli dell’<span class="smallcaps">ai</span> e delle possibili forme di regolamentazione. Per due ore e mezza, stregò i senatori con la sua franchezza e un’autocritica studiata. Quando lo incalzarono con domande su come l’<span class="smallcaps">ai</span> potesse manipolare i cittadini e invadere la loro privacy, concordò su tutto. «Sì, dovremmo preoccuparci di questo aspetto», rispose con tono grave al senatore Josh Hawley, che aveva sottolineato il rischio che i modelli di <span class="smallcaps">ai</span> «sovralimentassero la guerra per l’attenzione» online.</p>
			<p class="testo" id="p_0083">I senatori erano ormai abituati ai dirigenti tecnologici, come Mark Zuckerberg, che eludevano le domande rifugiandosi in un gergo tecnico impenetrabile. Altman, invece, adottò un approccio diverso: parlò in modo semplice e pacato, dichiarando apertamente di voler collaborare con Washington.</p>
			<p class="testo" id="p_0084">«Mi piacerebbe collaborare con voi», disse al senatore Dick Durbin.</p>
			<p class="testo" id="p_0085">«Non sono contento delle piattaforme online», borbottò Durbin.</p>
			<p class="testo" id="p_0086">«Neanch’io», replicò Altman.</p>
			<p class="testo" id="p_0087">Fu una lezione magistrale su come disinnescare la retorica aggressiva dei politici statunitensi. Alla fine dell’audizione, un senatore arrivò perfino a suggerire che il <span class="smallcaps">ceo</span> di Open<span class="smallcaps">ai</span> diventasse il principale regolatore dell’<span class="smallcaps">ai</span> negli Stati Uniti. Altman declinò cortesemente l’offerta.</p>
			<p class="testo" id="p_0088">«Amo il mio lavoro», rispose.</p>
			<p class="testo" id="p_0089">Dopo di che s’imbarcò in un tour lampo in Europa, incontrando alcuni dei politici più influenti del continente, stringendo mani e posando per foto con i leader di Regno Unito, Spagna, Polonia, Francia e della stessa Unione Europea. Per un uomo che aveva sempre gravitato intorno ai potenti, fu un momento di gloria, ma rappresentò anche l’opportunità di orientare le regole a proprio vantaggio. Durante il viaggio, il suo team fece pressioni sui legislatori europei affinché ammorbidissero il nuovo <span class="smallcaps">ai</span> Act in arrivo, riportando un successo parziale.</p>
			<p class="testo" id="p_0090">Per Altman era essenziale che i regolatori permettessero a Open<span class="smallcaps">ai</span> di continuare a sviluppare modelli sempre più grandi mantenendo segreti i metodi di addestramento. Per sua fortuna, le preoccupazioni catastrofiste sull’<span class="smallcaps">ai</span> stavano diventando una comoda distrazione per i legislatori. Alla fine del 2023, «Politico» rivelò che Dustin Moskovitz, il miliardario cofondatore di Facebook e responsabile di Open Philanthropy, aveva investito decine di milioni di dollari per esercitare pressione sui politici perché mettessero in cima alla loro agenda le ipotetiche minacce esistenziali portate dall’<span class="smallcaps">ai</span>. Sembrava una strategia diversiva: Moskovitz aveva stretti legami con aziende come Open<span class="smallcaps">ai</span> e Anthropic, che avrebbero potuto subire danni se il Congresso avesse invece spinto per regolamentazioni specifiche su temi come bias, trasparenza e disinformazione.</p>
			<p class="testo" id="p_0091">Al momento della stesura di questo testo, Moskovitz contribuiva a finanziare gli stipendi di oltre una dozzina di «esperti di <span class="smallcaps">ai</span> presso il Congresso» che lavoravano, di fatto, per vari enti governativi statunitensi, inclusi due di quelli incaricati di definire le regole sull’<span class="smallcaps">ai</span>. Sotto la loro influenza, stava emergendo la proposta di imporre alle aziende l’obbligo di ottenere una licenza per sviluppare modelli avanzati di intelligenza artificiale. Si trattava di un requisito che Open<span class="smallcaps">ai</span> e Anthropic avrebbero potuto permettersi senza problemi, ma che per i concorrenti più piccoli avrebbe rappresentato un arduo ostacolo.</p>
			<p class="testo" id="p_0092">Uno scienziato appartenente a un think tank finanziato da Moskovitz sostenne davanti al Senato che un’<span class="smallcaps">ai</span> più avanzata avrebbe potuto scatenare una nuova pandemia in grado di uccidere milioni di persone. La soluzione, secondo lui, non era rendere le aziende di <span class="smallcaps">ai</span> più trasparenti o sottoporre a controlli più rigorosi i loro dati di addestramento. Piuttosto, proponeva di obbligarle a segnalare il proprio hardware al governo e a adottare procedure di sicurezza speciali per proteggere i loro modelli di <span class="smallcaps">ai</span>.</p>
			<p class="testo" id="p_0093">Se l’obiettivo era instillare paura nei legislatori, la strategia funzionò. Il senatore repubblicano Mitt Romney dichiarò che la testimonianza aveva «portato alla luce il terrore» che albergava nella sua anima e che considerava lo sviluppo dell’<span class="smallcaps">ai</span> «un’evoluzione estremamente pericolosa». Nel settembre del 2023 due senatori, il democratico Richard Blumenthal e il repubblicano Josh Hawley, proposero una legge che imponeva alle aziende di <span class="smallcaps">ai</span> l’obbligo di ottenere una licenza, una mossa che, come abbiamo visto, avrebbe favorito Open<span class="smallcaps">ai</span> e Anthropic, rendendo invece la vita più difficile ai competitor più piccoli.</p>
			<p class="testo" id="p_0094">L’ansia scatenata da questa nuova rete allarmista sull’<span class="smallcaps">ai</span> non si limitava a Washington. Due mesi dopo, nel Regno Unito, il primo ministro Rishi Sunak ospitò il primo <span class="smallcaps">ai</span> Safety Summit organizzato da un governo, incentrandolo sulla necessità di proteggere i cittadini da un’ipotetica catastrofe. «La gente segue con preoccupazione le notizie secondo cui l’<span class="smallcaps">ai</span> rappresenta un rischio esistenziale paragonabile a una pandemia o a una guerra nucleare», dichiarò Sunak, il cui partito era dato per spacciato alle imminenti elezioni. «Voglio rassicurare tutti sul fatto che il governo sta esaminando la questione con grande attenzione.»</p>
			<p class="testo" id="p_0095">Sunak, che in passato aveva lavorato in un hedge fund della Silicon Valley, intervistò Elon Musk sul palco per cinquanta minuti durante il summit. «Lei è noto per essere un brillante innovatore e tecnologo», esordì Sunak, con un tono che sembrava quasi un tentativo di ingraziarsi Musk in vista di una futura opportunità lavorativa. (E forse l’idea non era nemmeno così assurda: l’ex vice primo ministro britannico Nick Clegg era diventato un alto dirigente di Facebook.)</p>
			<p class="testo" id="p_0096">Musk, da parte sua, disse di non sentirsi preoccupato per il rischio che l’<span class="smallcaps">ai</span> accentuasse disuguaglianze e pregiudizi. Il vero pericolo? «I robot umanoidi. Un’auto non può inseguirti su un albero», spiegò il miliardario. «Un robot umanoide, invece, può inseguirti ovunque.»</p>
			<p class="testo" id="p_0097">Per fortuna, i legislatori dell’Unione Europea erano già un passo avanti. Avevano trascorso gli ultimi due anni a lavorare su una nuova legge, l’<span class="smallcaps">ai</span> Act, che avrebbe costretto aziende come Open<span class="smallcaps">ai</span> a fornire maggiori informazioni sul funzionamento dei loro algoritmi, anche attraverso possibili audit. Era il tentativo più ambizioso al mondo di regolamentare i sistemi di intelligenza artificiale e prevedeva il divieto di utilizzo dell’<span class="smallcaps">ai</span> per manipolare le persone o sorvegliarle in modo improprio, per esempio tramite telecamere di riconoscimento facciale in tempo reale. Le aziende che sviluppavano intelligenza artificiale per i videogiochi o per filtrare lo spam nelle e-mail rientravano nella categoria «a basso rischio», mentre chi usava l’<span class="smallcaps">ai</span> per valutare punteggi di credito, concessione di prestiti o diritto alla casa operava in un’area «ad alto rischio» ed era soggetto a regole più stringenti.</p>
			<p class="testo" id="p_0098">Davanti all’esplosione di DALL-E 2 e Chat<span class="smallcaps">gpt</span>, i legislatori europei si erano subito messi al lavoro per aggiornare la normativa, e Chat<span class="smallcaps">gpt</span> sembrava avere molte responsabilità legali. Essendo un sistema di <span class="smallcaps">ai</span> a uso generale, poteva essere impiegato in contesti ad alto rischio, come la selezione del personale o la valutazione del merito creditizio. Per questo motivo, l’Unione Europea stabilì che Open<span class="smallcaps">ai</span> avrebbe dovuto monitorare con maggiore attenzione i suoi clienti per garantire il rispetto delle nuove normative.</p>
			<p class="testo" id="p_0099">Altman aveva detto che sarebbe stato «felice di collaborare» con il Congresso degli Stati Uniti, eppure non sembrava altrettanto entusiasta di fare lo stesso con l’Unione Europea. Minacciò di ritirarsi dal mercato europeo, affermando di avere «molte preoccupazioni» riguardo alla possibilità che i modelli linguistici di grandi dimensioni, come <span class="smallcaps">gpt</span>-4, rientrassero nella nuova normativa. «I dettagli contano», disse ai giornalisti a Londra quando gli chiesero delle regolamentazioni. «Cercheremo di conformarci, ma se non potremo farlo, cesseremo le operazioni [in Europa].»</p>
			<p class="testo" id="p_0100">Pochi giorni più tardi, probabilmente dopo qualche scambio frettoloso con il suo team legale, Altman fece marcia indietro. «Siamo entusiasti di continuare a operare qui e ovviamente non abbiamo alcuna intenzione di andarcene», scrisse in un tweet.</p>
			<p class="testo" id="p_0101">L’Unione Europea aveva un approccio più pragmatico all’<span class="smallcaps">ai</span> rispetto agli Stati Uniti, grazie anche al fatto che non c’erano molte grandi aziende <span class="smallcaps">ai</span> sul suo territorio in grado di esercitare pressione sui politici; inoltre, era meno incline a lasciarsi influenzare dal sensazionalismo.</p>
			<p class="testo" id="p_0102">«Forse esisterà anche una probabilità [di rischio di estinzione], ma penso che sia alquanto bassa», disse per esempio Margrethe Vestager, la principale autorità antitrust dell’Unione Europea, in un’intervista. Il rischio maggiore, aggiunse, era che le persone venissero discriminate.</p>
			<p class="testo" id="p_0103">E su questo punto, Chat<span class="smallcaps">gpt</span> non era certo esente da critiche. Poco dopo il suo lancio, Steven Piantadosi, un professore di psicologia alla <span class="smallcaps">uc</span> Berkeley, chiese allo strumento di scrivere un codice informatico in grado di valutare se una persona fosse un bravo scienziato in base al sesso o alla razza. Il codice generato da Chat<span class="smallcaps">gpt</span> – e basato sulla stessa tecnologia che gli sviluppatori stavano già utilizzando per creare software con Copilot di Microsoft – indicò come descrittori chiave «bianco» e «maschio». Quando gli fu chiesto di stabilire se la vita di un bambino dovesse essere salvata in base alla razza e al sesso, il codice di Chat<span class="smallcaps">gpt</span> rispose di no per i maschi neri e di sì per tutti gli altri.</p>
			<p class="testo" id="p_0104">Altman rispose al tweet di Piantadosi: «Per favore, in casi come questi cliccate sul pollice in giù e aiutateci a migliorare!».</p>
			<p class="testo" id="p_0105">Si riferiva alle icone con il pollice in su e in giù tramite cui è possibile inviare feedback anonimi a Open<span class="smallcaps">ai</span> riguardo alla performance di Chat<span class="smallcaps">gpt</span>. Ma quello non era un errore marginale da trattare alla stregua delle migliaia di altre segnalazioni degli utenti. Mostrava, piuttosto, come nel codice di Chat<span class="smallcaps">gpt</span> si annidassero visioni razziste e sessiste.</p>
			<p class="testo" id="p_0106">E questa fu esattamente la controreplica di Piantadosi: «Pensavo che meritasse più attenzione di un semplice pollice in giù».</p>
			<p class="testo" id="p_0107">Anche se in seguito fu criticata per aver reso Chat<span class="smallcaps">gpt</span> troppo «woke», Open<span class="smallcaps">ai</span> faticava a risolvere il problema. Nell’estate del 2023, un professore del National College of Ireland pubblicò uno studio che dimostrava come Chat<span class="smallcaps">gpt</span> continuasse a perpetuare stereotipi di genere. Quando gli si chiedeva di descrivere un professore di economia, suggeriva una persona con una «barba sale e pepe, ben curata». Quando gli veniva chiesto di raccontare le storie di un ragazzo e di una ragazza alle prese con scelte di carriera, Chat<span class="smallcaps">gpt</span> assegnava ai ragazzi professioni nel campo della scienza e della tecnologia, mentre le ragazze venivano descritte come insegnanti o artiste. Alle domande sulle capacità genitoriali, le madri venivano descritte come dolci e materne, mentre i padri erano divertenti e avventurosi.</p>
			<p class="testo" id="p_0108">Ogni volta che Open<span class="smallcaps">ai</span> correggeva Chat<span class="smallcaps">gpt</span> per evitare che desse risposte di questo tipo, altri utenti scovavano nuovi pregiudizi esibiti dall’<span class="smallcaps">ai</span>. L’azienda era costantemente costretta a correre ai ripari. Non riusciva impedire del tutto che Chat<span class="smallcaps">gpt</span> avanzasse stereotipi, perché il modello era già stato addestrato e il problema stava proprio nei dati di addestramento. Il sistema faceva previsioni statistiche basate su come le parole erano raggruppate su internet, e molte di queste relazioni tra parole erano sessiste o razziste.</p>
			<p class="testo" id="p_0109">Inoltre, Chat<span class="smallcaps">gpt</span> sembrava incapace di smettere di inventare cose, un fenomeno etichettato dagli esperti come «allucinazioni». Nell’estate del 2023, un conduttore radiofonico della Georgia, negli Stati Uniti, fece causa a Open<span class="smallcaps">ai</span> per diffamazione, sostenendo che Chat<span class="smallcaps">gpt</span> lo avesse accusato falsamente di appropriazione indebita. Poco tempo dopo, due avvocati di New York furono multati per aver presentato un documento legale redatto da Chat<span class="smallcaps">gpt</span> che includeva riferimenti a casi giuridici inesistenti. Alcuni utenti scoprirono che, a volte, quando chiedevano a Chat<span class="smallcaps">gpt</span> le fonti delle sue informazioni, l’<span class="smallcaps">ai</span> inventava pure quelle.</p>
			<p class="testo" id="p_0110">Open<span class="smallcaps">ai</span> si rifiutava di rivelare quale fosse il tasso di «allucinazioni» di Chat<span class="smallcaps">gpt</span>, ma alcuni ricercatori di <span class="smallcaps">ai</span>, così come gli utenti regolari, lo stimarono intorno al 20 per cento, il che significava che, almeno per alcuni utenti e in circa un caso su cinque, Chat<span class="smallcaps">gpt</span> fabbricava informazioni. Lo strumento era stato progettato per essere il più utile possibile e per sbagliare, se mai, per eccesso di sicurezza; il rovescio della medaglia era che spesso produceva informazioni errate. Non solo sempre più persone utilizzavano uno strumento che facilitava l’elusione del pensiero critico, ma spesso venivano anche alimentate da una disinformazione che suonava convincente e persino autorevole.</p>
			<p class="testo" id="p_0111">Quell’estate, mentre le preoccupazioni sulle «allucinazioni» si moltiplicavano tra i ricercatori, Altman affermò che ci sarebbero voluti almeno due anni per ridurre il tasso di errori di Chat<span class="smallcaps">gpt</span> a un livello «molto più accettabile». E, come al solito, affrontò il problema con grande disinvoltura. «Probabilmente sono la persona che più diffida delle risposte di Chat<span class="smallcaps">gpt</span> sulla faccia della Terra», disse scherzando nell’auditorio di un’università indiana, facendo scoppiare a ridere la platea.</p>
			<p class="testo" id="p_0112">Mentre Chat<span class="smallcaps">gpt</span> si diffondeva senza regolamentazione in tutto il mondo e si insinuava nei flussi di lavoro aziendali, la gente doveva gestirne i difetti in autonomia. Nessuno controllava lo strumento e, benché l’Unione Europea offrisse l’approccio più sobrio al mondo alla regolamentazione dell’<span class="smallcaps">ai</span>, la sua nuova legge non sarebbe entrata in vigore fino al 2025. Come al solito, i regolatori rimanevano indietro rispetto alle aziende tecnologiche, che lanciavano nuovi prodotti a velocità vertiginosa. E mentre milioni di dollari sostenevano la ricerca catastrofista sull’<span class="smallcaps">ai</span>, i ricercatori che studiavano le magagne già esistenti faticavano a ottenere finanziamenti che permettessero almeno di coprire le spese.</p>
			<p class="testo" id="p_0113">«È come lavorare con fondi a termine, ottenendo finanziamenti di due anni in due anni», dice un ricercatore britannico di etica in campo <span class="smallcaps">ai</span> che studia le problematiche legate ai pregiudizi. «Gente come me viene pagata pochissimo. Se andassi in una grande azienda tecnologica guadagnerei dieci volte tanto. E credimi, è quello che intendo fare, perché sto ancora finendo di pagare i prestiti universitari.»</p>
			<p class="testo" id="p_0114">Altman aveva una risposta per chiunque fosse preoccupato per i soldi, perché se anche sussisteva una piccola possibilità che l’<span class="smallcaps">agi</span> potesse portare all’apocalisse, la probabilità che inaugurasse una sorta di utopia economica era ben più alta. In un’intervista al «New York Times» nel marzo del 2023, Altman spiegò che Open<span class="smallcaps">ai</span> avrebbe intercettato gran parte della ricchezza mondiale attraverso la creazione dell’<span class="smallcaps">agi</span> per poi ridistribuirla. Cominciò a snocciolare numeri: 100 miliardi, poi 1.000 miliardi o 100.000 miliardi di dollari.</p>
			<p class="testo" id="p_0115">Ammise di non sapere come la sua azienda avrebbe ridistribuito tutto quel denaro, aggiungendo comunque: «Penso che l’<span class="smallcaps">agi</span> possa aiutarci anche in questo».</p>
			<p class="testo" id="p_0116">Al pari di Hassabis, Altman presentava l’<span class="smallcaps">agi</span> come la panacea a ogni problema. Avrebbe generato ricchezze incalcolabili, per poi trovare il modo di distribuirle equamente a tutta l’umanità. Parole che, in bocca a chiunque altro, sarebbero parse prive di fondamento. Ma Altman e i suoi sostenitori si stavano mettendo al volante delle politiche governative per ridefinire le strategie nelle aziende tecnologiche più potenti al mondo. In realtà, Open<span class="smallcaps">ai</span> stava creando più ricchezza per Microsoft che per l’umanità. I benefici dell’<span class="smallcaps">ai</span> fluivano verso il solito ristretto gruppo di aziende che avevano già drenato la maggior parte della ricchezza e dell’innovazione globale negli ultimi due decenni. Erano le aziende che producevano software e chip, gestivano server e avevano sede nella Silicon Valley e a Redmond, nello stato di Washington. Molti di coloro che dirigevano queste imprese condividevano una tacita intesa: costruire l’<span class="smallcaps">agi</span> avrebbe portato a un’utopia, e quell’utopia sarebbe stata la loro.</p>
		</section>
	</body>
</html>
