<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:ibooks="http://apple.com/ibooks/html-extensions" lang="it-IT" xml:lang="it-IT" class="calibre">
  <head>
    <title>FILE 15 – Supremacy – Capitolo</title>
    <meta content="urn:uuid:35484d83-cd47-4b21-bd76-ea9d55abc8bb" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="../../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../../page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body epub:type="bodymatter" class="calibre2">
		<section epub:type="chapter" role="doc-chapter" class="calibre">
			<h2 class="capitolo" id="h2-0001"><span class="smallcaps1">11. vincolati a big tech</span></h2>
			<p class="testo" id="p_0001">Da fuori, il passaggio di Open<span class="smallcaps">ai</span> da organizzazione filantropica impegnata a salvare l’umanità a società alleata con Microsoft sembrava strano, se non addirittura sospetto. Per molti dei suoi dipendenti, però, la prospettiva di lavorare con un colosso tecnologico dalle risorse illimitate era una bella notizia. Non solo il loro datore di lavoro avrebbe avuto più probabilità di rimanere solvente, ora c’era anche una maggiore opportunità di raccogliere i frutti finanziari di ingenti investimenti, piuttosto che di donazioni. Negli anni successivi, Microsoft avrebbe investito ancora più capitale nella società di Sam Altman, offrendo ai dipendenti di Open<span class="smallcaps">ai</span> la possibilità di vendere azioni e diventare milionari. Molti dei suoi ricercatori non ritenevano che la loro missione fosse stata compromessa. Avevano abbracciato l’idea che i benefici derivanti dal raggiungimento dell’<span class="smallcaps">agi</span> superassero qualsiasi scrupolo riguardo al modo in cui arrivarci. Finché si fossero attenuti allo statuto, non importava da dove arrivasse il denaro. E, d’altra parte, questa era la Silicon Valley, dove i programmatori entravano in start-up orientate a migliorare il mondo, guadagnando stipendi a sette cifre e stock option che potevano garantire loro una seconda casa nel mercato immobiliare più costoso d’America.</p>
			<p class="testo" id="p_0002">Eppure, non tutti erano contenti del nuovo stato di cose. Dario Amodei, l’ingegnere con gli occhiali e i capelli ricci che fin dall’inizio aveva messo in discussione Open<span class="smallcaps">ai</span> riguardo ai suoi reali obiettivi, apprezzava l’idea di proteggere l’umanità da eventuali danni causati dall’<span class="smallcaps">ai</span>, anche se lo stesso Brockman aveva ammesso che, all’epoca, era comunque «un po’ vaga». Amodei era un fisico laureato a Princeton che non aveva paura di porre domande difficili e che, a quanto pareva, ne aveva parecchie riguardo a Microsoft. Era evidente che Open<span class="smallcaps">ai</span> e Microsoft avessero obiettivi differenti; quindi, come avrebbe fatto Open<span class="smallcaps">ai</span> a rimanere fedele allo scopo di realizzare un’<span class="smallcaps">ai</span> sicura quando doveva anche aiutare Microsoft a fare più soldi? «Stiamo costruendo l’<span class="smallcaps">ai</span> per l’umanità, ma siamo diventati fornitori di tecnologia per un’azienda che cerca di massimizzare i profitti», faceva notare ai colleghi. La cosa non quadrava.</p>
			<p class="testo" id="p_0003">Amodei dirigeva ampie sezioni della ricerca di Open<span class="smallcaps">ai</span>, inclusa quella sui modelli linguistici. Lui e il suo team stavano lavorando alla nuova iterazione, chiamata <span class="smallcaps">gpt</span>-3. Per quanto si sentisse a disagio all’idea di essere vincolato a Microsoft, doveva ammettere che il gigante del software stava fornendo loro risorse informatiche senza pari. Pochi mesi dopo l’investimento, infatti, Microsoft annunciò di aver costruito un supercomputer appositamente per consentire a Open<span class="smallcaps">ai</span> di addestrare la sua <span class="smallcaps">ai</span>.</p>
			<p class="testo" id="p_0004">Amodei non aveva praticamente mai lavorato con un sistema così potente. Un computer domestico ha in genere una sola unità di elaborazione centrale, o <span class="smallcaps">cpu</span>: il potente chip di silicio rettangolare e ricoperto da miliardi di minuscoli transistor. È il cervello del computer e, di solito, ha tra i quattro e gli otto core, ognuno dei quali si occupa delle operazioni necessarie. Il nuovo supercomputer di Microsoft, invece, aveva una <span class="smallcaps">cpu</span> composta da 285.000 core. Se un normale computer domestico era come una macchinina giocattolo, quello era un carro armato.</p>
			<p class="testo" id="p_0005">Inoltre, per addestrare l’<span class="smallcaps">ai</span> venivano usati gli stessi chip <span class="smallcaps">gpu</span> contenuti nei computer progettati per offrire il massimo dell’esperienza videoludica. Questi chip possono infatti elaborare i dati visivi complessi per rendere le immagini dei videogiochi fluide e dettagliate, essendo in grado di eseguire un numero elevatissimo di calcoli in parallelo. Ora, il nuovo supercomputer di Microsoft ne aveva 10.000. E poteva spostare i dati centinaia di volte più velocemente rispetto ai computer normali, grazie a una connettività ultraveloce.</p>
			<p class="testo" id="p_0006">Approfittando di tutta questa nuova potenza di calcolo, Open<span class="smallcaps">ai</span> stava anche raccogliendo enormi quantità di testo da internet per addestrare i suoi nuovi modelli linguistici <span class="smallcaps">gpt</span>. Agiva come un cercatore d’oro del <span class="smallcaps">xix</span> secolo, sfruttando le vaste riserve dei contenuti online e trasformandole in un’<span class="smallcaps">ai</span> sempre più abile. I suoi ricercatori avevano già estratto circa quattro miliardi di parole da Wikipedia; la nuova fonte più ovvia, quindi, era rappresentata dai miliardi di commenti pubblicati sui social media. Facebook non era un’opzione poiché, dopo lo scandalo di Cambridge Analytica del 2018, la piattaforma di Mark Zuckerberg aveva impedito ad altre aziende di accedere ai dati degli utenti. Ma Twitter era ancora in gran parte terra di nessuno, così come Reddit.</p>
			<p class="testo" id="p_0007">Conosciuto come la homepage di internet, Reddit era un forum che copriva ogni possibile argomento, dalle auto agli appuntamenti, fino alle foto che sembravano dipinti rinascimentali. L’azienda aveva stretti legami con Altman, poiché lui e i suoi fondatori avevano partecipato insieme alla prima classe di Y Combinator, e Altman sarebbe poi diventato il terzo maggiore azionista di Reddit, arrivando a possederne l’8,7 per cento, secondo una dichiarazione depositata nel 2024 in vista della sua quotazione in Borsa. Altman aveva ottime ragioni per amare Reddit: era una miniera d’oro di dialoghi umani con cui addestrare l’<span class="smallcaps">ai</span>, grazie ai commenti che milioni di utenti pubblicavano e votavano ogni giorno. Non c’era da stupirsi, dunque, che Reddit fosse diventato una delle fonti più importanti per l’addestramento dell’<span class="smallcaps">ai</span> di Open<span class="smallcaps">ai</span>, al punto che i suoi testi rappresentavano tra il 10 e il 30 per cento dei dati utilizzati per allenare <span class="smallcaps">gpt</span>-4, secondo una persona vicina al forum online. Più testo Open<span class="smallcaps">ai</span> utilizzava per addestrare il suo modello linguistico, e più i suoi computer erano potenti, tanto più fluida diventava la sua <span class="smallcaps">ai</span>.</p>
			<p class="testo" id="p_0008">Ma Amodei non riusciva a scrollarsi di dosso il disagio. Lui e sua sorella Daniela, che guidava i team di policy e sicurezza di Open<span class="smallcaps">ai</span>, vedevano i modelli dell’azienda diventare sempre più grandi e avanzati, e nessuno del loro gruppo di lavoro o dell’azienda conosceva davvero le conseguenze del rilascio di simili sistemi al pubblico. Ora che erano legati a doppio filo a una potente corporazione, avrebbero potuto subire pressioni via via maggiori per rilasciare la tecnologia prima di testarla adeguatamente.</p>
			<p class="testo" id="p_0009">Le preoccupazioni di Amodei erano condivise anche da Demis Hassabis a Londra. Proprio mentre Open<span class="smallcaps">ai</span> stava preparando il rilascio di <span class="smallcaps">gpt</span>-3, Sam Altman, Greg Brockman e Ilya Sutskever andarono a cena con i fondatori di DeepMind, nel tentativo di migliorare i rapporti tra le due aziende rivali. L’incontro, tuttavia, fu teso. Demis Hassabis fece notare ad Altman che Open<span class="smallcaps">ai</span> stava rilasciando i suoi modelli di <span class="smallcaps">ai</span> al mondo intero, dando la possibilità a soggetti pericolosi di abusarne per diffondere disinformazione o creare strumenti di <span class="smallcaps">ai</span> ancora più dannosi. Sottolineò, inoltre, come DeepMind fosse stata molto più attenta a mantenere segreti i suoi modelli di <span class="smallcaps">ai</span>, tenendoli al sicuro da potenziali abusi.</p>
			<p class="testo" id="p_0010">Altman rispose educatamente che questa era un’assurdità. Poi, con sottile ironia, ricordò le battute con cui, tempo prima, Elon Musk aveva definito Hassabis «genio del male». Tanta segretezza, secondo Altman, dava un potere eccessivo al singolo individuo a capo di un’azienda di <span class="smallcaps">ai</span>, come nel caso di DeepMind. Dunque, anche quell’approccio non era poi così sicuro.</p>
			<p class="testo" id="p_0011">Tornato a San Francisco, Altman cominciò a sentire argomentazioni simili da Amodei, che si lamentava della nuova direzione commerciale di Open<span class="smallcaps">ai</span>. Altman si rivolse allora al sempre ottimista Reid Hoffman, fiducioso che le sue capacità di mediatore potessero contribuire a risolvere la questione. Hoffman parlò con Amodei per capire quale fosse il problema e gli consigliò con tatto di avere fiducia nel processo.</p>
			<p class="testo" id="p_0012">«È questa la strada per portare a termine la missione», gli spiegò. Amodei e la sorella erano scettici. Sapevano quanto questi modelli linguistici stessero diventando grandi e difficili da gestire, e sapevano anche che Hoffman faceva parte del consiglio di amministrazione di Microsoft. Non aveva forse un interesse personale in tutta questa faccenda?</p>
			<p class="testo" id="p_0013">Gli Amodei avevano buone ragioni per diffidare del crescente legame di Open<span class="smallcaps">ai</span> con Microsoft. Nell’arco di tempo trascorso dalla fondazione di Open<span class="smallcaps">ai</span>, le grandi aziende tecnologiche avevano centralizzato il controllo dello sviluppo dell’<span class="smallcaps">ai</span>, trascurando di condurre una ricerca adeguata sui rischi mentre spingevano la tecnologia verso una potenza e una capacità sempre maggiori. Uno studio condotto dal <span class="smallcaps">mit</span> nel 2023 ha rilevato che, nel corso dell’ultimo decennio, le grandi aziende erano arrivate a dominare la proprietà dei modelli di <span class="smallcaps">ai</span>, passando dal controllarne l’11 per cento nel 2010 alla quasi totalità, il 96 per cento, nel 2021. Persino i progetti governativi apparivano insignificanti rispetto alle enormi somme di denaro che Big Tech stava investendo nell’<span class="smallcaps">ai</span>. Nel 2021, per esempio, le agenzie governative statunitensi non coinvolte nella difesa avevano destinato 1,5 miliardi di dollari all’<span class="smallcaps">ai</span>, mentre il settore privato ne aveva investiti più di 340 nello stesso anno.</p>
			<p class="testo" id="p_0014">Nel frattempo, i meccanismi di quei sistemi commerciali di <span class="smallcaps">ai</span> rimanevano segreti. Mentre Open<span class="smallcaps">ai</span> rilasciava sempre più tecnologia al pubblico, manteneva il più stretto riserbo riguardo a come creava quei sistemi, rendendo più difficile per i ricercatori indipendenti esaminarli in cerca di potenziali danni e pregiudizi. Immaginate che un grande produttore alimentare come Unilever confezioni snack sempre più deliziosi, rifiutandosi però di riportare gli ingredienti sulla confezione o di illustrare il processo di produzione. Questo è essenzialmente ciò che stava facendo Open<span class="smallcaps">ai</span>. Era più facile venire a sapere cosa c’era in una confezione di Doritos che in un grande modello linguistico.</p>
			<p class="testo" id="p_0015">Amodei non era tanto preoccupato per i bias quanto per la minaccia esistenziale che l’<span class="smallcaps">ai</span> rappresentava per l’umanità. Aveva scritto un articolo di ricerca intitolato «Problemi concreti in materia di sicurezza <span class="smallcaps">ai</span>», in cui evidenziava i potenziali incidenti che rischiavano di verificarsi con sistemi di <span class="smallcaps">ai</span> mal progettati. Se i costruttori di <span class="smallcaps">ai</span> avessero specificato l’obiettivo sbagliato nei loro progetti, il sistema avrebbe potuto causare danni anche solo accidentalmente. Premiando un robot domestico per aver spostato una scatola da un lato all’altro di una stanza, c’era il rischio che questi rovesciasse un vaso che si trovava nel suo cammino, concentrato com’era sul suo obiettivo. Amodei sosteneva dunque la necessità di esaminare con attenzione gli incidenti reali che l’<span class="smallcaps">ai</span> avrebbe potuto causare una volta integrata nei sistemi di controllo industriale e nella sanità.</p>
			<p class="testo" id="p_0016">In ogni caso, non si lasciò convincere dal ragionamento di Hoffman e alla fine decise di lasciare Open<span class="smallcaps">ai</span>, insieme a sua sorella Daniela e a circa una mezza dozzina di altri ricercatori. Questa scelta non era solo frutto di una protesta per la sicurezza o per la commercializzazione dell’<span class="smallcaps">ai</span>. Anche tra coloro che si dicevano più seriamente preoccupati regnava un certo opportunismo. Amodei aveva visto personalmente Sam Altman negoziare un enorme investimento da un miliardo di dollari da parte di Microsoft e intuiva che, probabilmente, sarebbe fluito altro capitale. Non si sbagliava: stava assistendo all’inizio di un nuovo boom nel campo dell’<span class="smallcaps">ai</span>. Lui e i suoi colleghi decisero di fondare una nuova azienda chiamata Anthropic, un nome ispirato al termine filosofico che fa riferimento all’esistenza umana, a sottolineare la loro particolare attenzione verso il genere umano. Avrebbe fatto da contrappeso a Open<span class="smallcaps">ai</span>, proprio come Open<span class="smallcaps">ai</span> lo aveva fatto a DeepMind e Google. Naturalmente, volevano anche cogliere un’opportunità di business.</p>
			<p class="testo" id="p_0017">«All’epoca non pensavamo che ci fossero particolari barriere difensive nel campo dell’<span class="smallcaps">ai</span>», dice uno dei fondatori di Anthropic. In altre parole, il settore era ampio e aperto. «Sembrava che una nuova organizzazione ben strutturata potesse emergere rapidamente con la stessa forza delle organizzazioni già affermate. E quindi abbiamo ritenuto preferibile costruire un’organizzazione basata sulla nostra visione, mettendo al centro la ricerca sulla sicurezza.»</p>
			<p class="testo" id="p_0018">Amodei aveva svolto un ruolo chiave nella costruzione dei modelli linguistici di Open<span class="smallcaps">ai</span>. Ora, poteva fare lo stesso sotto il proprio nome e il proprio marchio. Lui e il suo team ripensarono a come Open<span class="smallcaps">ai</span> fosse passata da organizzazione non profit a realtà commerciale e decisero di non volersi ritrovare a operare la stessa scelta, convinti che ciò li avrebbe fatti apparire inaffidabili. Così si costituirono come una «public benefit corporation», la stessa struttura legale utilizzata da Ben &amp; Jerry’s per mettere le preoccupazioni sociali e ambientali allo stesso livello degli azionisti.</p>
			<p class="testo" id="p_0019">Sam Altman ora aveva un altro rivale con cui confrontarsi, oltre a DeepMind. Un rivale che, per di più, vantava una comprensione molto più profonda della «formula segreta» di Open<span class="smallcaps">ai</span>. Proprio come aveva previsto Amodei, Anthropic riuscì a raccogliere enormi quantità di denaro quasi immediatamente, grazie al solito gruppo di ricchi sostenitori della sicurezza in ambito <span class="smallcaps">ai</span>, tra cui Jaan Tallinn e Dustin Moskovitz, il miliardario cofondatore di Facebook che era stato compagno di stanza di Mark Zuckerberg alla Harvard University. Il denaro nella Silicon Valley circola spesso tra piccoli gruppi di reti elitarie, incluse quelle con rivalità di lunga data. Il veicolo filantropico di Moskovitz, Open Philanthropy, aveva investito 30 milioni di dollari in Open<span class="smallcaps">ai</span>, e Altman aveva finanziato il software Asana di Moskovitz; tuttavia, Moskovitz voleva sostenere anche il nuovo concorrente di Open<span class="smallcaps">ai</span>. (Tallinn avrebbe poi dichiarato di essersi pentito di aver alimentato così tanta competizione nel mondo dell’<span class="smallcaps">ai</span>, rendendo quest’ultima potenzialmente più pericolosa.)</p>
			<p class="testo" id="p_0020">In meno di un anno, Anthropic aveva raccolto altri 580 milioni di dollari, per lo più dai giovani e ricchi fondatori della piattaforma di scambi di criptovalute <span class="smallcaps">ftx</span>, che erano arrivati ad Amodei grazie all’interesse condiviso per l’altruismo efficace. Ironia della sorte, due anni dopo aver lamentato i legami commerciali di Open<span class="smallcaps">ai</span> con Microsoft, Amodei avrebbe ottenuto più di 6 miliardi di dollari in investimenti da Google e Amazon, allineandosi con entrambe le aziende e confermando che, in questo nuovo mondo dove costruire l’<span class="smallcaps">agi</span> richiedeva risorse pressoché illimitate, le persone non dicevano di no ai giganti della tecnologia.</p>
			<p class="testo" id="p_0021">Oltreoceano, a Londra, quell’associazione stava diventando una responsabilità per DeepMind. Demis Hassabis stava cercando nuovi traguardi scientifici raggiungendo i quali l’azienda potesse dimostrare di essere avanti rispetto a Open<span class="smallcaps">ai</span>, così da stupire il mondo dopo AlphaGo. Ma il suo cofondatore Mustafa Suleyman era ancora ansioso di comprovare che l’<span class="smallcaps">ai</span> potesse essere usata a fin di bene. Da anni, ormai, si sentiva a disagio riguardo alla direzione verso cui l’amico stava guidando l’azienda. Mentre il genio degli scacchi sembrava concentrato sull’uso di giochi e simulazioni per sviluppare l’<span class="smallcaps">ai</span>, Suleyman pensava che dovessero studiare il mondo reale, anche se ciò significava affrontare dati disorganizzati. D’altronde, come avrebbero potuto risolvere i problemi della società, in futuro, se non ci lavoravano subito?</p>
			<p class="testo" id="p_0022">Suleyman avviò collaborazioni con diversi ospedali a Londra per usare l’<span class="smallcaps">ai</span> di DeepMind a supporto di medici e infermieri. Il progetto iniziò con un’app che inviava un avviso quando i pazienti sembravano a rischio di sviluppare un’insufficienza renale acuta. Non utilizzava le tecniche avanzate dell’<span class="smallcaps">ai</span> di DeepMind, a causa di tutti gli ostacoli normativi imposti nell’ambito medico, ma Suleyman era convinto che i suoi scienziati <span class="smallcaps">ai</span> avrebbero potuto rendere lo strumento più sofisticato, una volta che fosse stato addestrato sui giusti dati medici.</p>
			<p class="testo" id="p_0023">Il personale ospedaliero apprezzava l’app e il progetto sembrava promettente. Ma poi accadde l’impensabile. Sui giornali cominciarono a circolare notizie secondo cui «Google» stava ottenendo l’accesso alle cartelle cliniche di 1,6 milioni di pazienti a Londra, nel tentativo di estrapolarne dati sensibili. L’esperimento di Suleyman si era improvvisamente trasformato in uno scandalo. Era a tal punto convinto che lo scorporo di DeepMind fosse una buona idea da dimenticare che l’azienda era ancora, tecnicamente, di proprietà di un colosso pubblicitario che guadagnava raccogliendo i dati delle persone per condividerli con gli inserzionisti. Per il mondo esterno, l’impegno di DeepMind per risolvere un problema sanitario con l’<span class="smallcaps">ai</span> sembrava improvvisamente sospetto, a causa della presenza minacciosa di Google sullo sfondo.</p>
			<p class="testo" id="p_0024">Hassabis era sconvolto dalla copertura mediatica negativa sullo scandalo ospedaliero, che sembrava offuscare la reputazione stellare guadagnata con le partite di AlphaGo in Asia. L’intera esperienza confermò che cercare di addestrare un modello di <span class="smallcaps">ai</span> con l’insieme caotico di dati che rappresentavano il mondo reale – proprio come stava facendo Open<span class="smallcaps">ai</span> setacciando il web per addestrare i suoi modelli linguistici – poteva compromettere la reputazione di DeepMind, soprattutto per via del suo legame con Google.</p>
			<p class="testo" id="p_0025">Hassabis sembrava anche dubitare dell’efficacia dei comitati etici indipendenti, incluso quello che lui e Suleyman avevano immaginato per guidare DeepMind una volta che si fosse separata da Google. Ma Suleyman era ansioso di sperimentare nuove forme di governance. Aveva istituito un comitato di revisione più ristretto per analizzare i progetti sanitari di DeepMind e assicurarsi che venissero portati avanti in maniera virtuosa. Il comitato era composto da otto professionisti britannici provenienti dal mondo dell’arte, della scienza e della tecnologia, incluso un ex politico. Si riunivano quattro volte l’anno per esaminare le ricerche sanitarie dell’azienda, parlare con gli ingegneri e segnalare eventuali problemi etici nei piani di collaborazione con ospedali e pazienti.</p>
			<p class="testo" id="p_0026">Fu un esperimento nobile ma destinato al fallimento, quanto ad autoregolamentazione. In aziende come Open<span class="smallcaps">ai</span>, DeepMind e altre realtà tecnologiche come Facebook, l’idea dominante era che i comitati indipendenti fossero il modo migliore per trovare un equilibrio tra la costruzione di un’<span class="smallcaps">ai</span> a beneficio dell’umanità e la ricerca del profitto, specialmente in assenza di una regolamentazione adeguata. Open<span class="smallcaps">ai</span>, per esempio, aveva un consiglio di amministrazione il cui unico compito era assicurarsi che l’azienda realizzasse l’<span class="smallcaps">agi</span> per il bene dell’umanità. DeepMind voleva creare un comitato simile che fungesse da coscienza etica una volta avvenuto lo scorporo da Google. Ma queste strutture di governance, per quanto ben intenzionate, non erano sostenibili all’interno di un colosso globale con interessi economici da tutelare. Sam Altman lo avrebbe imparato a proprie spese, e quella realtà colpì anche Suleyman. Non voleva costringere i membri del comitato che esaminavano la divisione sanitaria di DeepMind a firmare accordi di non divulgazione, in modo che potessero criticare liberamente e pubblicamente l’azienda, se lo desideravano. Ma questo significava anche che non avevano accesso completo all’intero lavoro di DeepMind, di cui spesso erano all’oscuro. E poiché i loro giudizi non erano legalmente vincolanti, i membri del comitato si lamentavano di avere le mani legate. In pratica, il comitato non poteva fare granché. Ed era proprio questo il problema dell’autoregolamentazione nel settore tech: l’impossibilità di esercitare un effettivo controllo sulla stessa azienda che ti aveva assunto e sulla quale non avevi alcuna autorità legale.</p>
			<p class="testo" id="p_0027">Alla fine, l’esperimento naufragò. Google decise di sviluppare la propria divisione sanitaria e di prendere in carico il lavoro di DeepMind con i medici e i professionisti del settore. Il colosso delle ricerche non voleva un gruppo di esterni che mettessero costantemente in dubbio il suo operato, ragion per cui sciolse il comitato di Suleyman. Fu l’ennesimo tentativo fallito da parte di Google – e più in generale del mondo tech – di autoregolamentarsi. Pochi mesi prima, Google aveva chiuso un altro comitato consultivo sull’<span class="smallcaps">ai</span> dopo appena una settimana a causa delle proteste pubbliche per le posizioni anti-<span class="smallcaps">lgbtq</span> di uno dei membri. Tutto ciò evidenziava un problema sistemico più ampio. Lo sviluppo dell’<span class="smallcaps">ai</span> procedeva così rapidamente da superare la capacità delle agenzie di regolamentazione e dei legislatori di tenere il passo. Le aziende tecnologiche operavano in un vuoto normativo e quindi, tecnicamente, potevano fare qualunque cosa volessero con l’<span class="smallcaps">ai</span>. I tecnologi stavano cercando in buona fede di autoregolamentare le proprie aziende attraverso comitati e strutture giuridiche, ma alla fine agivano in un sistema nel quale dovevano dare priorità agli obblighi finanziari verso gli azionisti e alla crescita. Ed è per questo che anche il prolungato e faticoso tentativo di DeepMind di affrancarsi da Google finì per naufragare.</p>
			<p class="testo" id="p_0028">In una nuvolosa mattina di aprile del 2021, il volto rotondo di Demis Hassabis si piegò in un sorriso durante una videochiamata con tutti i suoi collaboratori, mentre si preparava a fare ciò che gli riusciva meglio: trasformare una brutta notizia in qualcosa di positivo. Ormai, era da oltre sette anni che DeepMind cercava di ottenere l’indipendenza da Google. Avevano provato a diventare prima un’«unità autonoma», poi una «società di Alphabet», quindi una «global interest company» (una società orientata all’interesse globale) e, più di recente, si erano indirizzati verso una «company limited by guarantee», ovvero una società limitata da garanzia, un’etichetta giuridica britannica solitamente usata da enti di beneficenza e associazioni, ma che avrebbe consentito loro di combinare obiettivi commerciali, ricerca scientifica e altruismo. I piani erano ancora segreti. I mille dipendenti di DeepMind non ne parlavano con nessuno al di fuori dell’azienda.</p>
			<p class="testo" id="p_0029">Facendo un passo indietro per contemplare a distanza ciò che avevano cercato di fare Hassabis e Suleyman in tutti quegli anni, sembrava proprio che alla fine avessero ceduto alla sindrome del «rimpianto del venditore». Un fenomeno, questo, abbastanza tipico nel settore tecnologico: in molti casi, i fondatori di una start-up rimangono sgomenti nel constatare come l’azienda acquirente abbia distorto la loro missione originale. I fondatori di WhatsApp, per esempio, erano stati irremovibili per anni nel sostenere che la loro app di messaggistica dovesse rimanere privata e senza pubblicità, ponendo tutti i messaggi della piattaforma sotto una solida crittografia. Jan Koum era cresciuto nell’Ucraina comunista, dove le telefonate venivano regolarmente intercettate, e aveva incollato alla scrivania un biglietto scritto dal cofondatore Brian Acton che recitava: «Niente pubblicità! Niente giochi! Niente trucchetti!». Ma dopo aver venduto a Facebook per 19 miliardi di dollari, Koum e Acton furono costretti a fare concessioni sui precedenti standard in merito alla privacy. A un certo punto, per esempio, aggiornarono le policy in modo tale da consentire che gli account WhatsApp degli utenti potessero essere collegati dietro le quinte ai relativi profili Facebook. Ne seguì un duro scontro tra Acton e i dirigenti di Facebook, a causa del quale lui lasciò l’azienda prima che terminasse il periodo di maturazione delle sue azioni, rinunciando a 850 milioni di dollari e ammettendo di aver profondamente rimpianto la vendita.</p>
			<p class="testo" id="p_0030">Hassabis non era il tipo di persona che andava allo scontro con i superiori. Era strategico e molto più abile nei rapporti con i dirigenti di Google. Invece di litigare e dimettersi, cercava soluzioni più intelligenti per salvare la faccia, come aveva fatto con AlphaGo, ma il suo ottimismo lo aveva reso cieco di fronte al bisogno di Google di far crescere costantemente il proprio business. Benché la società madre avesse firmato un protocollo d’intesa in cui offriva a DeepMind 16 miliardi di dollari in dieci anni perché operasse in piena indipendenza, quel documento non era legalmente vincolante. Per di più, Hassabis aveva perso il suo punto di riferimento nella dirigenza di Google. Negli ultimi anni, infatti, Larry Page era scomparso dalla scena pubblica, pur rimanendo <span class="smallcaps">ceo</span> di Alphabet. Durante un’audizione congressuale sulla sicurezza elettorale, per esempio, non si era nemmeno presentato, lasciando che la stampa fotografasse una sedia vuota. Nel dicembre del 2019, Page si dimise ufficialmente e Sundar Pichai divenne il <span class="smallcaps">ceo</span> di Alphabet. Era il segnale più chiaro del fatto che l’azienda stava crescendo e che iniziava a comportarsi sempre di più come un’impresa tradizionale.</p>
			<p class="testo" id="p_0031">Per anni, i fondatori di Google avevano coltivato idee futuristiche come le auto a guida autonoma, i computer indossabili e un progetto che mirava a sconfiggere la morte, ma nessuna di queste iniziative stava generando un profitto reale. Nel 2019, i progetti «moonshot» di Google avevano prodotto circa 155 milioni di dollari in ricavi, ma erano costati all’azienda quasi un miliardo di dollari, secondo quanto riportato dal «Wall Street Journal». Nel frattempo, il core business di Google, insieme al suo browser Chrome, alla divisione hardware e a YouTube, generava circa 155 miliardi di dollari l’anno in entrate. Pichai voleva consolidare il controllo di attività chiave come la pubblicità e la ricerca, nonché sulla tecnologia che le supportava: l’intelligenza artificiale. Mentre Hassabis voleva costruire un’<span class="smallcaps">ai</span> capace di svelare i misteri dell’universo, Pichai intendeva utilizzarla per potenziare il business pubblicitario di Google. Voleva che Google smettesse di sperimentare ai margini con «scommesse» come i servizi di consegna tramite droni e la tecnologia quantistica, concentrandosi invece sui suoi settori cruciali.</p>
			<p class="testo" id="p_0032">L’uscita definitiva di Page fu un duro colpo per Hassabis. Nonostante tutte le tensioni con Google, era sempre stato un suo sostenitore leale. «Avevamo perso il nostro protettore», ricorda un ex dirigente di DeepMind. «Ci dicevano sempre: “Non temete, Larry ci copre le spalle”.» In passato, ogni volta che Pichai aveva cercato di spingere DeepMind a lavorare di più per Google, Hassabis si era rivolto proprio a quel protettore. «Demis aggirava sempre [Pichai] e andava da Larry per ottenere quello che voleva», ricorda un altro ex membro di DeepMind.</p>
			<p class="testo" id="p_0033">Hassabis e Sundar Pichai avevano un buon rapporto lavorativo, ma mentre Larry Page era un sognatore come Hassabis, Pichai era più un dirigente tecnologico di indole pragmatica: il suo interesse era capitalizzare al meglio le competenze di DeepMind. E, nel 2019, le perdite annuali ante imposte di DeepMind erano aumentate a circa 600 milioni di dollari, quasi quanto Google aveva pagato per acquistarla. Insomma, al colosso della ricerca stava costando una fortuna.</p>
			<p class="testo" id="p_0034">Reid Hoffman, nel suo ruolo di mediatore, aveva cercato di convincere i fondatori di DeepMind a rimanere con Google e ad accettare lo <em class="calibre3">status quo</em>. Aveva visto le bozze dettagliate preparate dai legali che delineavano come sarebbe stato il loro nuovo modello aziendale, e aveva notato le centinaia di ore che Suleyman e Hassabis stavano dedicando al progetto. E si era reso conto del fatto che stavano sbattendo la testa contro un muro.</p>
			<p class="testo" id="p_0035">«Voi due e Google avete interessi completamente diversi», aveva commentato, mettendoli in guardia. Non avrebbero dovuto investire così tanto tempo e fiducia nello scorporo finché non fossero stati assolutamente certi del benestare di Google. Inoltre, aggiunse, non dovevano per forza fondare un’organizzazione in stile non profit per sviluppare un’<span class="smallcaps">ai</span> sicura. Anche lui desiderava contribuire a elevare il genere umano, ma era un capitalista convinto e credeva che il miglior modo di perseguire obiettivi altruistici fosse attraverso i mezzi commerciali. A suo dire, avevano già tutto quello che serviva loro: Google! Trasformarsi in una nuova struttura giuridica era complicato e irrealistico, e nessuno lo aveva mai fatto prima.</p>
			<p class="testo" id="p_0036">Da questo punto di vista, Hoffman aveva ragione. Se stavano cercando di sottrarsi all’influenza delle grandi aziende, i fondatori di DeepMind, Altman, e persino Dario Amodei e i cofondatori di Anthropic erano solo degli illusi. Il settore dell’<span class="smallcaps">ai</span> era ormai appannaggio delle più grandi aziende tecnologiche che stavano assumendo un controllo sempre maggiore sulla ricerca, lo sviluppo, l’addestramento e la diffusione nel mondo.</p>
			<p class="testo" id="p_0037">Collegandosi alla videoconferenza con il suo staff, quella mattina di aprile, Hassabis annunciò due notizie. La prima era che sarebbe stato istituito un comitato etico per supervisionare lo sviluppo sicuro dell’<span class="smallcaps">ai</span> di DeepMind, ma questo non avrebbe avuto nulla a che fare con il comitato indipendente che lui e Suleyman avevano immaginato all’inizio. Anzi, non sarebbe stato affatto indipendente, visto che sarebbe stato composto da dirigenti di Google e senza alcun membro di DeepMind.</p>
			<p class="testo" id="p_0038">La seconda notizia era ancora più deludente. Google stava annullando qualsiasi progetto che prevedesse di trasformare DeepMind in un’entità separata. Un ingegnere di DeepMind mandò un messaggio a un collega con la notizia: «Demis sta rivelando i risultati delle trattative con Google», scrisse. «Non abbiamo ottenuto nulla.»</p>
			<p class="testo" id="p_0039">Mentre i dipendenti elaboravano le nuove informazioni, Hassabis non smetteva comunque di ostentare ottimismo. Negli anni, era diventato un maestro nel marketing. Riusciva a far sembrare uno sviluppo mediocre nel campo dell’<span class="smallcaps">ai</span>, pubblicato su una rivista peer-reviewed tipo «Nature», come una scoperta sconvolgente; sul fronte interno, riusciva a trasformare un passo falso in un’opportunità. Rimanendo parte di Google, disse allo staff, DeepMind avrebbe ottenuto i finanziamenti necessari per avvicinare l’<span class="smallcaps">agi</span> alla realtà. Inoltre, avrebbe potuto lavorare in modo indipendente: tutti avrebbero ricevuto nuove e-mail con il dominio DeepMind.com al posto di Google.com. I membri del team fissavano lo schermo con lo sguardo vuoto, come se Hassabis avesse lanciato loro un contentino. Molti sospettavano già che Google non avrebbe lasciato andare un laboratorio <span class="smallcaps">ai</span> costato 650 milioni di dollari, ma avevano coltivato comunque la speranza di far parte di un progetto più altruistico, capace di trasformare la società in meglio (garantendo, nel frattempo, uno stipendio a sei cifre). Ora era chiaro che stavano semplicemente lavorando per un gigante della pubblicità, proprio come i loro colleghi in California.</p>
			<p class="testo" id="p_0040">Non c’erano più dubbi sul fatto che Google avesse abbindolato i fondatori di DeepMind, forse di proposito, fin dall’inizio. «È stata una strategia di sfinimento lunga cinque anni: ci mostravano la carota senza mai concedercela», dice un ex dirigente senior. «Ci hanno lasciato crescere sempre di più e diventare sempre più dipendenti da loro. Ci hanno manipolato.» I fondatori di DeepMind non si resero conto di ciò che stava succedendo fino a quando non fu troppo tardi. I luminari politici che avevano accettato di diventare direttori indipendenti della nuova DeepMind furono informati, con un certo imbarazzo, che il progetto era stato annullato.</p>
			<p class="testo" id="p_0041">Dall’altra parte dell’oceano, a Mountain View, Google aveva capito che i suoi esperimenti con unità aziendali autonome non funzionavano. I consigli consultivi indipendenti non funzionavano. E i comitati etici con autorità legale avevano così poche probabilità di funzionare che non valeva nemmeno la pena di provarci. Erano disordinati e potenzialmente dannosi per la reputazione dell’azienda.</p>
			<p class="testo" id="p_0042">I ripetuti fallimenti di Big Tech nel compito di governarsi in maniera responsabile stavano aprendo la strada a un cambiamento radicale. Per anni, aziende come Google, Facebook e Apple si erano presentate come autentiche pioniere del progresso umano. Apple creava prodotti che «funzionavano, semplicemente». Facebook «connetteva le persone». Google «organizzava le informazioni del mondo». Ma ora la Silicon Valley si trovava di fronte a una reazione globale contro il suo crescente potere. Lo scandalo di Cambridge Analytica di Facebook aveva fatto capire alle persone che venivano usate per vendere pubblicità. I detrattori accusavano Apple di aver portato più di 250 miliardi di dollari in contanti all’estero, esentasse, e di limitare la durata degli iPhone per far sì che la gente continuasse a comprarli. E dietro le quinte, le ricercatrici Timnit Gebru e Margaret Mitchell mettevano in guardia su come i modelli linguistici potessero amplificare i pregiudizi.</p>
			<p class="testo" id="p_0043">I giganti tecnologici avevano accumulato una ricchezza spropositata, e man mano che schiacciavano i competitor e violavano la privacy degli utenti, il pubblico diventava sempre più scettico riguardo alle loro promesse di rendere il mondo un posto migliore. Non c’era esempio più chiaro di questi mutati obiettivi del modo in cui Alphabet stava limitando i suoi esperimenti con i consigli etici e i progetti «moonshot», respingendo le aspirazioni di DeepMind di risolvere i problemi globali grazie all’<span class="smallcaps">agi</span>. Mentre lavorava per centralizzare il controllo del conglomerato, il nuovo <span class="smallcaps">ceo</span> di Alphabet, Sundar Pichai, stava anche cercando di capire come DeepMind potesse supportare meglio i profitti di Google. La tecnologia <span class="smallcaps">ai</span> di DeepMind veniva già utilizzata per affinare le ricerche su Google, le raccomandazioni di YouTube e per rendere più naturale la voce di Google Assistant. Ma doveva fare di più. Mentre Pichai stringeva la presa di Google sul laboratorio di <span class="smallcaps">ai</span>, anche il rapporto tra Hassabis e Suleyman si stava deteriorando.</p>
			<p class="testo" id="p_0044">Negli ultimi anni, i due stavano mettendo a dura prova i loro limiti personali: la crescente minaccia di Open<span class="smallcaps">ai</span>, lo scandalo e il fallimento delle collaborazioni di DeepMind con gli ospedali, e la pressione sempre maggiore di Google affinché costruissero strumenti di <span class="smallcaps">ai</span> più orientati al business. Suleyman poi si era fatto una reputazione di bullo in seno a DeepMind, e si erano levate anche diverse accuse di molestia. Alla fine del 2019, dopo un’indagine legale indipendente, fu rimosso dai suoi ruoli di gestione.</p>
			<p class="testo" id="p_0045">Apparentemente indifferente a tali accuse, a quel punto Google gli affidò il prestigioso incarico di vicepresidente dell’<span class="smallcaps">ai</span> nella sede di Mountain View. E lui, da parte sua, sembrò ben lieto di trasferirsi in California e abbracciare la cultura dell’hacking tipica della Silicon Valley, lasciandosi alle spalle, in Inghilterra, i valori scientifici e gerarchici di DeepMind.</p>
			<p class="testo" id="p_0046">Presso la sede principale di Google, Suleyman concentrò la propria attenzione sui modelli linguistici, un campo che DeepMind aveva in gran parte trascurato e che Open<span class="smallcaps">ai</span> perseguiva in maniera aggressiva. Lavorò con un team di ingegneri di Google che stavano sviluppando <span class="smallcaps">l</span>a<span class="smallcaps">mda,</span> il progetto di modello linguistico dell’azienda basato sul trasformatore, e si avvicinò anche a Reid Hoffman, uomo dalle numerose entrature. I due discussero della possibilità di avviare una propria azienda di <span class="smallcaps">ai</span> focalizzata sui modelli linguistici e i chatbot.</p>
			<p class="testo" id="p_0047">L’angoscia provata da Suleyman nei confronti di Big Tech stava svanendo, e le sue convinzioni sui rischi dei monopoli aziendali erano cambiate. Ora si sentiva più a suo agio con l’idea che Google controllasse l’<span class="smallcaps">agi</span>, piuttosto che lasciarla nelle mani di Hassabis, di sé stesso e di pochi altri colleghi fidati. Se DeepMind si fosse separata, un consiglio di sei fiduciari avrebbe supervisionato l’utilizzo dell’<span class="smallcaps">ai</span>. Sarebbe stata un’enorme quantità di potere concentrata in un numero ristretto di persone. Almeno in un’azienda quotata in Borsa, pensava Suleyman, c’erano migliaia di azionisti e impiegati che, insieme, esercitavano una certa influenza. Dopotutto, quando migliaia di dipendenti di Google avevano protestato contro il contratto dell’azienda con il Pentagono, Google aveva fatto un passo indietro sull’accordo militare.</p>
			<p class="testo" id="p_0048">Tuttavia, Suleyman giudicava la situazione dalla prospettiva di un imprenditore. Non sapeva davvero cosa significasse costruire l’<span class="smallcaps">ai</span> nel cuore di un’azienda come Google, né quanto fosse arduo ed estenuante, in realtà, lanciare segnali di allarme. Due ricercatrici nel campo dell’<span class="smallcaps">ai</span> che lavoravano nella sede di Google a Mountain View lo avevano sperimentato sulla propria pelle. Erano preoccupate per gli effetti collaterali che i grandi modelli linguistici potevano avere sulla società, ben prima di qualsiasi scenario apocalittico, e non riuscivano a capire perché nessuno ne parlasse. Questi modelli stavano diventando così simili agli esseri umani che le persone erano vittime dell’illusione già insita nel loro nome: vale a dire, l’idea che fossero «intelligenti». Alcuni cominciavano a credere che questi modelli non solo potessero «pensare», ma che addirittura fossero senzienti. Quando tentarono di lanciare l’allarme e avvertire il mondo sull’illusione nella quale stava per cadere, le due ricercatrici si ritrovarono sotto tiro. Si stava tessendo una storia sulle capacità quasi umane dell’<span class="smallcaps">ai,</span> una storia che avrebbe finito per servire alla perfezione gli interessi delle grandi aziende tecnologiche.</p>
		</section>
	</body>
</html>
