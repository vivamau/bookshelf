<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:ibooks="http://apple.com/ibooks/html-extensions" lang="it-IT" xml:lang="it-IT" class="calibre">
  <head>
    <title>FILE 14 – Supremacy – Capitolo</title>
    <meta content="urn:uuid:35484d83-cd47-4b21-bd76-ea9d55abc8bb" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="../../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../../page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body epub:type="bodymatter" class="calibre2">
		<section epub:type="chapter" role="doc-chapter" class="calibre">
			<h2 class="capitolo" id="h2-0001"><span class="smallcaps1">10. le dimensioni contano</span></h2>
			<p class="testo" id="p_0001">Uscendo dal quartier generale di Google, nella soleggiata Mountain View, in California, e guidando verso nord per circa un’ora, alla fine arrivereste a San Francisco e, una volta scesi dall’auto, sareste attraversati da un brivido. Qui, in genere, la temperatura è di diversi gradi più bassa, con nuvole grigie a incombere basse nel cielo. Mentre nella città natale di Google si gira in maglietta, nel microclima urbano di Open<span class="smallcaps">ai</span> serve una giacca. Un’altra grande differenza, all’epoca, era questa: i ricercatori di Open<span class="smallcaps">ai</span> erano entusiasti della tecnologia dei trasformatori, che invece la dirigenza di Google intendeva tenere chiusa in un armadio metaforico. Ai ricercatori con base nella fredda San Francisco stava per venire un’idea.</p>
			<p class="testo" id="p_0002">Le due dozzine circa di ricercatori del laboratorio non profit erano ancora impegnate a cercare di eguagliare il successo di DeepMind e non vedevano l’ora di realizzare un grande progresso nel campo dell’<span class="smallcaps">ai</span>. Avevano assistito alla vittoria di AlphaGo contro i migliori giocatori di Go a livello globale e ora stavano addestrando i loro agenti <span class="smallcaps">ai</span> a giocare a <em class="calibre3">Dota 2</em>, un complesso videogioco strategico simile a <em class="calibre3">World of Warcraft</em>. Se un agente <span class="smallcaps">ai</span> fosse riuscito a guidare un elfo in un mondo fantastico, forse avrebbe potuto catturare la natura caotica e incessante del mondo reale meglio di quanto avesse fatto AlphaGo di DeepMind. A prima vista, sembrava ben più impressionante che muovere un mucchio di pietre bianche e nere su una tavola da gioco.</p>
			<p class="testo" id="p_0003">Nel frattempo, tra Sam Altman e Demis Hassabis era in corso una sorta di mini guerra fredda, e il conviviale membro del comitato di Open<span class="smallcaps">ai</span> Reid Hoffman stava cercando un modo per convincerli a «fumare il calumet della pace», secondo quanto riferito da una fonte che ha sentito il commento in prima persona. Nel 2017, sia Altman sia Hassabis parteciparono a una conferenza sulla sicurezza dell’<span class="smallcaps">ai</span>, organizzata in California dal Future of Life Institute. Hoffman era presente e, dopo l’evento, tentò di portare a cena il guru delle start-up americane e il neuroscienziato britannico. Altman non gradì l’idea, sostenendo che Hassabis fosse poco collaborativo e apparentemente indifferente ai rischi esistenziali dell’<span class="smallcaps">ai</span> che lui, invece, stava cercando di prevenire. Così, Hoffman invitò Mustafa Suleyman al suo posto. I due concordarono sulla volontà di rendere il mondo un posto migliore, e per un po’ sembrò che le rispettive organizzazioni potessero trovare un punto d’intesa.</p>
			<p class="testo" id="p_0004">Dietro le quinte, però, Altman e Hassabis si contendevano i migliori ingegneri. Grazie al suo finanziatore Big Tech, Hassabis si trovava in una posizione di vantaggio e poteva offrire ai ricercatori dell’<span class="smallcaps">ai</span> compensi ben più elevati di quelli che poteva permettersi Altman, oltre a stock option di Google. Hassabis non disdegnava di inviare qualche e-mail alla dirigenza di Open<span class="smallcaps">ai</span> per ricordarle che non aveva rivali in quella corsa ai talenti. I manager di Open<span class="smallcaps">ai</span> mostravano quelle e-mail agli ingegneri che stavano cercando di reclutare. «Se era convinto che non avessimo possibilità di successo, perché mai inviarci queste e-mail?» ricorda un ex dipendente di Open<span class="smallcaps">ai</span>.</p>
			<p class="testo" id="p_0005">Forse lo faceva perché lo stesso Altman, a sua volta, aveva l’abitudine di contattare direttamente gli ingegneri di DeepMind per vedere se fossero disposti a cambiare squadra. In ogni caso, stando a un altro ex dipendente, adottava un approccio prudente e mirato al reclutamento, dedicandovi circa il 30 per cento del suo tempo e parlando a lungo con ogni candidato. «Siamo andati a casa sua e abbiamo camminato un’ora intera per Russian Hill, a San Francisco», racconta un ex membro del team ricordando il suo colloquio con Altman. Per chi entrava nella squadra, poi, Altman rimaneva largamente accessibile, seduto con il suo laptop nell’ufficio open space dell’azienda. «Chiunque poteva scrivergli su Slack e parlargli», ricordano in molti. «Non c’era nulla di male.» Nella struttura più gerarchica di DeepMind, invece, Hassabis tendeva a rintanarsi nel suo ufficio o in una sala riunioni ed era più difficile da avvicinare. Per ottenere un incontro con lui, bisognava passare attraverso altri manager e intermediari.</p>
			<p class="testo" id="p_0006">Open<span class="smallcaps">ai</span> stava per distinguersi da DeepMind in un altro modo. Ilya Sutskever, il suo scienziato di punta, non riusciva a smettere di pensare a cosa potesse fare il trasformatore con il linguaggio. Google lo stava usando per comprendere meglio i testi. E se Open<span class="smallcaps">ai</span> lo avesse usato per <em class="calibre3">generarli</em>, quei testi? Sutskever parlò con un giovane ricercatore di Open<span class="smallcaps">ai</span>, Alec Radford, che stava sperimentando con i modelli linguistici di grandi dimensioni. Benché Open<span class="smallcaps">ai</span> sia oggi principalmente conosciuta per Chat<span class="smallcaps">gpt</span>, nel 2017 stava ancora esplorando diversi scenari, cercando di capire cosa potesse funzionare, e Radford era uno dei pochi a concentrarsi sulla tecnologia che alimentava i chatbot.</p>
			<p class="testo" id="p_0007">I modelli linguistici di grandi dimensioni, all’epoca, erano ancora considerati una barzelletta. Le loro risposte erano per lo più preimpostate e spesso commettevano errori madornali. Radford, che portava gli occhiali e una zazzera rossiccia da liceale, era impaziente di superare tutti i precedenti tentativi accademici di rendere i computer più capaci di parlare e ascoltare, ma era anche un ingegnere e puntava a progressi più rapidi. Da almeno sei mesi si scontrava con ostacoli invalicabili nei suoi esperimenti, passando settimane su un progetto per poi abbandonarlo in favore del successivo. Aveva addestrato un modello linguistico su due miliardi di commenti prelevati dal forum online Reddit, ma era stata un’operazione infruttuosa.</p>
			<p class="testo" id="p_0008">In un primo momento, con l’invenzione del trasformatore da parte di Google Radford accusò il colpo. Era evidente che un’azienda di quelle dimensioni avesse più competenze nel campo dell’<span class="smallcaps">ai</span>. Dopo un po’, tuttavia, divenne altrettanto palese che Google non avesse grandi piani per la sua nuova invenzione, e Radford e Sutskever si resero conto che avrebbero potuto sfruttarne l’architettura a vantaggio di Open<span class="smallcaps">ai</span>. Dovevano solo darle un’impronta personale. Il modello trasformatore che alimentava Google Translate usava una struttura composta da un encoder e un decoder per elaborare le parole. L’encoder elaborava la frase in ingresso, magari in inglese, e il decoder generava l’output, per esempio una frase in francese.</p>
			<p class="testo" id="p_0009">L’idea era paragonabile a una conversazione tra due robot. Il primo, l’encoder, ti ascoltava e prendeva appunti, poi li passava al secondo, il decoder, che li leggeva e ti rispondeva. Radford e Sutskever capirono di poter eliminare il primo robot e lasciare, invece, che fosse il decoder ad ascoltare e rispondere. I primi test mostrarono che l’idea funzionava anche nella pratica, il che significava che avrebbero potuto costruire un modello linguistico più snello, più veloce da correggere e alimentare. E il fatto che fosse basato solo sul decoder si rivelò rivoluzionario. Combinando la capacità del modello di «comprendere» e parlare in un unico processo fluido, si poteva arrivare a generare testi molto più simili a quelli umani.</p>
			<p class="testo" id="p_0010">Il passo successivo sarebbe stato aumentare enormemente la quantità di dati, la potenza di calcolo e la capacità del loro modello linguistico. Sutskever era fermamente convinto che, nel campo dell’<span class="smallcaps">ai,</span> aumentando la scala «il successo era sempre garantito», soprattutto con i modelli linguistici. Quanti più dati erano disponibili, combinati alla massima potenza di calcolo e a un modello grande e complesso, maggiore sarebbe stata la capacità del sistema.</p>
			<p class="testo" id="p_0011">Lo stesso Radford rimase sbalordito dagli esiti di questi esperimenti in cui usava il trasformatore con il solo decoder addestrato su enormi quantità di testo. Dopo tutti i tentativi falliti di modificare nuovi algoritmi, scoprì che la strategia di Sutskever gli dava finalmente risultati. E sembrava anche più semplice, perché bastava alimentare il sistema con un numero sempre maggiore di dati. Ed era quello che Sutskever chiedeva ai colleghi in ufficio, come ricorda qualcuno che lavorava lì, all’epoca: «Puoi farlo più in grande?».</p>
			<p class="testo" id="p_0012">Il trasformatore consentì a Radford di fare, in due settimane, più progressi nei suoi esperimenti sui modelli linguistici di quanti ne avesse fatti nei due anni precedenti. Lui e i suoi colleghi iniziarono a lavorare su un nuovo modello linguistico che chiamarono <em class="calibre3">generative pre-trained transformer</em> («trasformatore generativo pre-addestrato») o <span class="smallcaps">gpt</span>. Lo addestrarono su un corpus online di circa settemila libri, per lo più autopubblicati su internet, molti dei quali orientati verso il genere romantico e la narrativa sui vampiri. Molti scienziati nel campo dell’<span class="smallcaps">ai</span> avevano usato lo stesso dataset, noto come BooksCorpus, e chiunque poteva scaricarlo gratuitamente. Radford e il suo team credevano di avere tutti gli ingredienti giusti per fare in modo che, questa volta, il loro modello fosse in grado di inferire il contesto.</p>
			<p class="testo" id="p_0013">Con il tempo, man mano che il sistema di Radford diventava più sofisticato, le persone di Open<span class="smallcaps">ai</span> – e non solo – avrebbero cominciato a interrogarsi sul fatto che questi nuovi modelli linguistici di grandi dimensioni comprendessero davvero il linguaggio, invece che inferirlo semplicemente. Potreste pensare che si tratti di un problema semantico di poco conto, ma la distinzione è importante, perché potrebbe far sembrare i sistemi di <span class="smallcaps">ai</span> più potenti di quanto non siano realmente. Considerate la frase: «Fuori piove, perciò non dimenticare l’ombrello». Il modello <span class="smallcaps">gpt</span> su cui Radford stava lavorando poteva inferire una probabile connessione tra portare un ombrello e la pioggia, e che la parola «ombrello» fosse anche associata al linguaggio relativo al rimanere asciutti. Ma il modello non comprendeva il concetto di bagnarsi come lo comprendono gli esseri umani. Si limitava a inferire quella connessione in maniera più accurata.</p>
			<p class="testo" id="p_0014">Man mano che gli esperimenti di Radford mostravano progressi sempre maggiori, Open<span class="smallcaps">ai</span> andava alimentando i suoi modelli con ulteriori testi tratti da internet. E se questo rendeva il sistema sempre più realistico, in modi che le macchine non avevano mai raggiunto prima, i modelli stavano semplicemente diventando più bravi nel prevedere quale testo dovessero seguire in una sequenza, in base ai dati su cui erano stati addestrati.</p>
			<p class="testo" id="p_0015">La questione sarebbe diventata motivo di divisioni anche nella comunità dell’<span class="smallcaps">ai</span>. La maggiore sofisticazione di questi modelli significava che stavano diventando senzienti? La risposta era probabilmente no, ma anche ingegneri e ricercatori esperti avrebbero presto cominciato a credere il contrario, dato che alcuni di loro cedevano all’incantesimo emotivo generato dal testo – carico di empatia e personalità – prodotto dall’<span class="smallcaps">ai</span>.</p>
			<p class="testo" id="p_0016">Per affinare il loro nuovo modello <span class="smallcaps">gpt</span>, Radford e i colleghi estrassero più contenuti da internet, addestrando il modello con domande e risposte tratte dal forum online Quora, insieme a migliaia di passaggi da esami di inglese sostenuti da studenti cinesi. Nel giugno del 2018, Radford e il suo team pubblicarono un articolo, dichiarando che il loro modello aveva acquisito una «significativa conoscenza del mondo» grazie a tutti i dati che stava elaborando. Inoltre, il modello fece qualcosa che li entusiasmò: generò testi su argomenti sui quali non era stato specificamente addestrato. Benché non riuscissero a spiegarne esattamente il funzionamento, era una buona notizia. Significava che erano sulla strada giusta per costruire un sistema generale. Più ampio era il corpus di addestramento e più il sistema sarebbe diventato «esperto».</p>
			<p class="testo" id="p_0017">Anche con i brevi passaggi di testo che riusciva a produrre, il primo <span class="smallcaps">gpt</span> era più performante di molti altri programmi informatici che elaboravano il linguaggio; questi ultimi, fino a quel momento, si basavano su milioni di esempi di testo taggati manualmente da operatori umani, nel più classico lavoro di inserimento dati. La maggior parte di questi programmi non veniva nemmeno utilizzata per i chatbot, ma per analizzare, per esempio, le recensioni dei prodotti. Gli operatori umani dovevano etichettare commenti tipo «Adoro questo prodotto» come positivi e quelli tipo «Va bene» come neutrali, per esempio. Era un metodo lento e costoso. Ma <span class="smallcaps">gpt</span> era diverso perché stava imparando da una mole di testi apparentemente casuali e non taggati per comprendere come funzionasse il linguaggio. Non aveva la guida di quei tag inseriti dagli umani.</p>
			<p class="testo" id="p_0018">Potete immaginare questi diversi approcci come un nuovo modo di educare gli esseri umani. Per esempio, supponiamo che due gruppi di studenti di arte stiano imparando a dipingere. Il primo gruppo riceve un libro con le immagini dei dipinti, ognuna etichettata con didascalie come «alba», «ritratto» o «astratto». È così che i modelli di <span class="smallcaps">ai</span> tradizionali apprendevano dai dati taggati. Si trattava di un metodo strutturato e preciso – come dire agli studenti di arte cosa rappresentava precisamente ogni immagine –, ma limitava anche ciò che le macchine potevano inferire, potendo soltanto ricordare ciò che era stato etichettato. Gli studenti di questo primo gruppo avrebbero probabilmente difficoltà a realizzare un dipinto non specificamente descritto nel loro libro.</p>
			<p class="testo" id="p_0019">Ora supponete che al secondo gruppo di studenti d’arte venga dato accesso a un’intera galleria d’arte con una vasta collezione di dipinti e nessuna etichetta. Gli studenti avrebbero la libertà di vagare, osservare e interpretare le opere d’arte da soli. Questo è per certi versi paragonabile al modo in cui <span class="smallcaps">gpt</span> stava imparando da enormi quantità di testo privo di tag. Gli studenti d’arte potrebbero cercare modelli, stili e tecniche in modo autonomo, finendo per assimilare quest’ampia varietà di esempi e le connessioni tra questi, senza che qualcuno indichi loro cosa inferire da ciascuno. La loro sarebbe una forma di apprendimento molto più ricca. Lo stesso valeva per il modello di <span class="smallcaps">ai</span>: il team di Radford si rese conto che, esponendo <span class="smallcaps">gpt</span> a una vasta gamma di usi e sfumature linguistiche, il modello stesso sarebbe stato in grado di generare risposte più creative.</p>
			<p class="testo" id="p_0020">Una volta completato l’addestramento iniziale, perfezionarono il nuovo modello utilizzando alcuni esempi etichettati per affinarlo in compiti specifici. Questo approccio in due fasi rese <span class="smallcaps">gpt</span> più flessibile e meno vincolato alla necessità di avere una gran quantità di esempi taggati.</p>
			<p class="testo" id="p_0021">Nel frattempo, Sutskever teneva d’occhio quanto stava accadendo in Google, dove gli ingegneri avevano infine iniziato a usare il trasformatore. Oltre che per migliorare il suo servizio di traduzione, Google lo aveva usato per creare un nuovo programma chiamato <span class="smallcaps">bert</span> e destinato a migliorare il suo motore di ricerca. Ora, il motore riusciva a comprendere meglio il contesto delle query, distinguendo se l’utente cercava informazioni su Apple, l’azienda, o sulla mela intesa come frutto. <span class="smallcaps">bert</span> avrebbe avuto un grosso impatto nel campo dell’elaborazione del linguaggio naturale.</p>
			<p class="testo" id="p_0022">«È stato allora che la gente si è detta: “Okay, puoi ottenere prestazioni sovrumane semplicemente prendendo questi modelli preaddestrati e perfezionando un po’ i dati”», dice Aravind Srinivas, ricercatore di <span class="smallcaps">ai</span> che ha lasciato Google nel 2021 per contribuire a costruire modelli linguistici presso Open<span class="smallcaps">ai</span>, prima di avviare la sua azienda Perplexity. «E questo ha cambiato l’elaborazione del linguaggio naturale.»</p>
			<p class="testo" id="p_0023">Google non avrebbe cominciato a utilizzare <span class="smallcaps">bert</span> per le sue ricerche in lingua inglese fino alla fine del 2019, ma per gli ingegneri di Open<span class="smallcaps">ai</span> fu comunque una nuova scossa. Il team era ancora in gran parte composto da sognatori con una missione, ma il budget risicato era una frazione di quello di Google Brain o DeepMind. Se Open<span class="smallcaps">ai</span> aveva speso circa 30 milioni di dollari in stipendi e potenza di calcolo nel 2017, DeepMind aveva sborsato oltre 440 milioni.</p>
			<p class="testo" id="p_0024">I migliori ricercatori di <span class="smallcaps">ai</span> avevano stipendi simili a quelli dei giocatori della <span class="smallcaps">nfl</span>, a volte milioni di dollari l’anno. Eppure, uno dei cofondatori di Open<span class="smallcaps">ai</span>, Wojciech Zaremba, avrebbe successivamente ammesso di aver rifiutato offerte «quasi folli», pari a due o tre volte il suo valore di mercato, per entrare in Open<span class="smallcaps">ai</span>. Altri che aderirono al progetto lo fecero perché desideravano lavorare accanto a stelle come Sutskever e spesso anche perché credevano sinceramente nella missione di creare un’<span class="smallcaps">ai</span> per il bene dell’umanità. Tuttavia, questo obiettivo poteva motivare le persone solo per un periodo limitato, e Google sembrava sempre più una minaccia incombente. Il gigante della ricerca aveva tutti i mattoni necessari per costruire un’<span class="smallcaps">agi,</span> se lo avesse voluto, dal trasformatore al <span class="smallcaps">tpu</span>, un potente chip proprietario per l’addestramento dei modelli di <span class="smallcaps">ai</span>.</p>
			<p class="testo" id="p_0025">«Mi svegliavo nervoso al pensiero che Google stesse per tirar fuori qualcosa di molto migliore rispetto a ciò che avevamo noi», ricorda un ex dirigente di Open<span class="smallcaps">ai</span>. Sfruttando le invenzioni di Google, come il trasformatore, sembrava che Open<span class="smallcaps">ai</span> si stesse sollazzando con i giocattoli del colosso tecnologico, riuscendo in qualche modo a passarla liscia. «Non facevamo che ripeterci: “Non c’è modo di spuntarla”.»</p>
			<p class="testo" id="p_0026">Anche Altman era nel panico. Perso ormai Musk, ovvero il loro finanziatore più facoltoso, lui, Brockman e il team fondatore capirono che rimanere un’organizzazione senza scopo di lucro non sarebbe stato sostenibile. Se volevano davvero costruire un’<span class="smallcaps">agi</span>, avrebbero avuto bisogno di molti più soldi. Il solo Sutskever percepì 1,9 milioni di dollari nel 2016, secondo la sua dichiarazione fiscale, una cifra comunque inferiore a quella che avrebbe potuto guadagnare in Google Brain o in Facebook. Ma pagare stipendi da star era la voce di spesa più grande per Open<span class="smallcaps">ai</span>, seguita dal costo della potenza di calcolo.</p>
			<p class="testo" id="p_0027">Un’azienda come Open<span class="smallcaps">ai</span> non poteva addestrare i suoi modelli di <span class="smallcaps">ai</span> sugli stessi laptop utilizzati dai membri del team. Per elaborare con tanta rapidità i miliardi di unità di dati necessari per l’addestramento servivano chip che si trovano solo nei server, generalmente affittati da fornitori di cloud come Amazon Web Services, Google Cloud o Microsoft Azure. Queste erano le aziende che possedevano distese di computer in enormi magazzini e che, grazie al possesso di questi computer «cloud», sarebbero diventate le principali vincitrici finanziarie del boom dell’<span class="smallcaps">ai</span>. All’inizio del 2024, la capitalizzazione di mercato di Nvidia si sarebbe avvicinata ai 2.000 miliardi di dollari, spinta dalla domanda sempre crescente dei suoi chip <span class="smallcaps">gpu</span> per l’addestramento dei modelli di <span class="smallcaps">ai</span>. Era praticamente impossibile costruire l’<span class="smallcaps">ai</span> al di fuori dell’orbita dei giganti tecnologici, perciò gli sviluppatori non avevano altra scelta che appoggiarsi a quelle aziende per realizzare i loro sistemi.</p>
			<p class="testo" id="p_0028">Questa era la situazione in cui versava Open<span class="smallcaps">ai</span>. Doveva affittare più computer cloud, e stava anche esaurendo i fondi. «Avremo bisogno di raccogliere molti più soldi di quanto possiamo fare come [organizzazione senza scopo di lucro]», disse Brockman agli altri dirigenti. «Molti miliardi di dollari.»</p>
			<p class="testo" id="p_0029">Nella necessità di ripensare la strategia aziendale, il team fondatore iniziò a lavorare su un documento interno riguardante il percorso verso l’<span class="smallcaps">agi</span>. Nell’aprile del 2018 pubblicarono sul loro sito web quello che definirono un nuovo statuto. Era un mix di obiettivi e impegni fin troppo ambiziosi, con un indizio su come la non profit stesse per compiere la madre di tutte le inversioni di rotta.</p>
			<p class="testo" id="p_0030">Per chiunque cercasse una maggior chiarezza sulla direzione di Open<span class="smallcaps">ai</span>, lo statuto si rivelò una delusione. Offriva una definizione di <span class="smallcaps">agi</span>, ma in termini succinti e fumosi: «Sistemi altamente autonomi che sovraperformano gli esseri umani nella maggior parte dei lavori economicamente più rilevanti». Come avrebbe fatto Open<span class="smallcaps">ai</span> a misurare questa sovraperformance? La non profit non lo specificava. Lo statuto affermava inoltre che Open<span class="smallcaps">ai</span> aveva un «dovere fiduciario verso l’umanità» e che non avrebbe usato la sua <span class="smallcaps">ai</span> per contribuire a «concentrare il potere». La maggior parte delle aziende ha notoriamente un dovere fiduciario o un obbligo legale verso azionisti e investitori, ma qui Open<span class="smallcaps">ai</span> enfatizzava il fatto di voler andare nella direzione opposta. Era dalla parte della gente.</p>
			<p class="testo" id="p_0031">Realizzare l’<span class="smallcaps">agi</span> doveva essere uno sforzo collaborativo e non una «gara competitiva», aggiungeva lo statuto. «Pertanto, se un progetto allineato ai nostri valori e attento alla sicurezza dovesse avvicinarsi alla realizzazione dell’<span class="smallcaps">agi</span> prima di noi, ci impegniamo a smettere di competere per prestare la nostra assistenza a quel progetto.» In altre parole, Open<span class="smallcaps">ai</span> si sarebbe fatta da parte e avrebbe dato una mano ad altri ricercatori sul punto di raggiungere l’<span class="smallcaps">agi</span>.</p>
			<p class="testo" id="p_0032">Tutto ciò suonava magnanimo. Open<span class="smallcaps">ai</span> si presentava come un’organizzazione così evoluta da porre gli interessi dell’umanità al di sopra di quegli obiettivi tradizionali della Silicon Valley come il profitto e il prestigio. Una frase chiave era «benefici ampiamente distribuiti», ovvero l’idea di condividere i frutti dell’<span class="smallcaps">agi</span> con tutta l’umanità. Era un eco dell’approccio nobile di Altman alla creazione di nuove tecnologie, coltivato dopo anni trascorsi come oggetto di venerazione in quanto guru delle start-up.</p>
			<p class="testo" id="p_0033">Leggendo tra le righe, però, sembrava anche che Altman e Brockman stessero preparando il terreno per abbandonare i principi fondanti di Open<span class="smallcaps">ai</span>. Tre anni prima, lanciando la non profit, avevano affermato che la ricerca di Open<span class="smallcaps">ai</span> sarebbe stata «libera da obblighi finanziari». Ora, invece, lo statuto di Open<span class="smallcaps">ai</span> accennava al fatto che sarebbero serviti molti soldi: «Prevediamo di dover mobilitare risorse considerevoli per adempiere alla nostra missione», scrivevano, «ma agiremo sempre con diligenza per ridurre al minimo quei conflitti di interesse tra i nostri dipendenti e stakeholder che potrebbero compromettere il beneficio collettivo».</p>
			<p class="testo" id="p_0034">Mentre lo statuto veniva pubblicato, Altman cercava disperatamente un modo per piegare le regole originarie di Open<span class="smallcaps">ai</span> pur di ottenere le ingenti risorse di cui aveva bisogno. Quando Musk se n’era andato, due mesi prima, Altman aveva subito chiamato uno dei suoi sostenitori più fedeli, il miliardario Reid Hoffman, per chiedergli consiglio. Hoffman era un propugnatore ottimista dell’<span class="smallcaps">ai</span> e aveva piena fiducia nella visione di Altman riguardo all’<span class="smallcaps">agi</span>. Si offrì di mantenere in vita Open<span class="smallcaps">ai</span> coprendo i costi e i salari nell’immediato, ma entrambi sapevano che non sarebbe stata una soluzione sul lungo periodo.</p>
			<p class="testo" id="p_0035">Altman disse a Hoffman che forse aveva trovato una soluzione al problema: una partnership strategica. L’espressione «partnership strategica» viene spesso usata dalle aziende per indicare una vasta gamma di relazioni societarie, da una collaborazione a distanza a un controllo molto stretto. Può significare condivisione di denaro e tecnologia tra due aziende o un semplice accordo di licenza. È abbastanza ambigua da nascondere la vera natura di una relazione aziendale scomoda, magari con legami finanziari complessi o con una delle aziende che esercita un controllo pressoché totale sull’altra. «Partnership» fa pensare a una relazione paritaria, anche quando non lo è, e contribuisce a evitare troppe domande imbarazzanti. Proprio ciò di cui Altman aveva bisogno.</p>
			<p class="testo" id="p_0036">Non voleva perdere il controllo completo di Open<span class="smallcaps">ai</span> vendendola a una grande azienda tecnologica, come aveva fatto DeepMind con Google. Ma una partnership strategica poteva creare l’illusione di una maggiore indipendenza da una grande azienda tecnologica, pur fornendo la potenza di calcolo necessaria. Altman e Hoffman discussero della possibilità di collaborare con Google e Amazon, ma Microsoft emerse rapidamente come la scelta più ovvia. Sia Hoffman sia Altman avevano legami personali con l’azienda: entrambi conoscevano bene il direttore tecnico di Microsoft, Kevin Scott, e Hoffman era vicino al <span class="smallcaps">ceo</span> di Microsoft, Satya Nadella.</p>
			<p class="testo" id="p_0037">Hoffman era un uomo tarchiato e gioviale con un sorriso da ragazzo, e il suo vero valore per Open<span class="smallcaps">ai</span> non stava tanto nei soldi quanto nella sua rete di rapporti. Era talmente bravo a stringere amicizie e allacciare conoscenze che aveva fondato il sito di networking professionale numero uno al mondo, LinkedIn. Nel 2016 aveva venduto l’azienda a Microsoft per 26,2 miliardi di dollari, portando il suo patrimonio netto a circa 3,7 miliardi di dollari e avviando una nuova carriera come finanziatore di start-up presso la leggendaria società di venture capital Greylock Partners.</p>
			<p class="testo" id="p_0038">Diventare miliardario, e poi investitore, aveva i suoi pro e i suoi contro. Hoffman era ormai così ricco da poter investire in altri imprenditori senza preoccuparsi troppo di andare incontro a una serie di flop. Altri investitori nella Bay Area in cerca del nuovo successo tecnologico consideravano Hoffman uno a cui non importava poi granché del risultato. Non sempre si fidavano delle sue scelte d’investimento, ma dovevano ammettere che era più disposto di altri a correre rischi, anche mettendo in contatto gli imprenditori con i membri dell’establishment della Silicon Valley. Dopo la vendita a Microsoft, Hoffman era entrato a far parte del consiglio di amministrazione dell’azienda e dunque godeva di un filo diretto con il <span class="smallcaps">ceo</span> Nadella.</p>
			<p class="testo" id="p_0039">«Devi fare in modo di parlare con lui», disse Hoffman ad Altman, riferendosi appunto a Nadella.</p>
			<p class="testo" id="p_0040">Quando Open<span class="smallcaps">ai</span> era ormai prossima a esaurire i fondi, il <span class="smallcaps">ceo</span> di Microsoft stava cercando di trasformare l’azienda già da quattro anni. Nadella non aveva il carisma di altre figure tecnologiche come Steve Jobs, ma era un negoziatore talentuoso e un acuto osservatore. «Non lo vedi mai senza un taccuino in cui prendere appunti su ciò che dice la gente durante le cene nel mondo tech», racconta Sheila Gulati, una venture capitalist di Seattle che è stata dirigente di Microsoft per circa un decennio. «Ma non è il tipo che fa sentire di più la sua voce. È il migliore a facilitare, collaborare e ascoltare.»</p>
			<p class="testo" id="p_0041">L’azienda fondata da Bill Gates aveva innescato la rivoluzione del personal computing con programmi iconici come Windows, Word ed Excel, ma si era trasformata in una corporation lenta e isolata che aveva mancato il treno della rivoluzione mobile. Nel 2014 aveva acquistato Nokia, senza però riuscire a trarne alcun vantaggio. Ma Nadella sembrava sulla buona strada per rimettere le cose a posto. Spingeva per un approccio più collaborativo tra i suoi manager (storicamente territoriali) e aveva convinto tutti a concentrarsi sul cloud computing, vendendo l’accesso a computer ultrapotenti che i clienti usavano per gestire le loro aziende.</p>
			<p class="testo" id="p_0042">Fu una mossa intelligente. Il cloud computing non era il business più sexy del mondo, ma era in crescita, visto che sempre più aziende trasferivano online i loro inventari di prodotti o i servizi di assistenza ai clienti. Microsoft sviluppava software specializzati per supportare questo genere di attività sotto un prodotto ombrello chiamato Azure; con il suo logo blu a forma di triangolo, Azure era destinato a diventare il nuovo grande successo di Microsoft dopo Windows. Utilizzava enormi server farm per alimentare gli asset digitali di centinaia di migliaia di clienti aziendali, e la potenza grezza di quei server era esattamente ciò di cui Altman aveva bisogno.</p>
			<p class="testo" id="p_0043">Nel luglio del 2018, Altman volò in Idaho per la conferenza annuale di Sun Valley. L’evento su invito ospitato dalla società di investimenti Allen &amp; Company era noto come un «campo estivo per miliardari», una riunione informale di networking in cui i ricchi tecnologi indossavano giacche Patagonia e mangiavano insalate di cavolo riccio accanto alla direttrice operativa di Facebook Sheryl Sandberg o al fondatore di Amazon Jeff Bezos. I partecipanti provenivano dal mondo della tecnologia e dei media, e a volte facevano accordi direttamente sul posto, davanti a una tazza di caffè o, nel caso di Altman e Nadella, nel vano delle scale.</p>
			<p class="testo" id="p_0044">Durante la conferenza, i due, entrambi alti e slanciati, si incrociarono sulle scale e iniziarono a chiacchierare. Altman ricordò il consiglio di Hoffman e colse l’opportunità per proporre Open<span class="smallcaps">ai</span> a Nadella.</p>
			<p class="testo" id="p_0045">A molti, il programma di Altman – usare un team di circa cento persone per costruire macchine superintelligenti – sarebbe parso una follia. Ma Nadella sapeva che Altman era profondamente integrato nella rete della Silicon Valley, molto più di quanto lo fosse lui stesso, da Seattle, e probabilmente doveva prenderlo sul serio.</p>
			<p class="testo" id="p_0046">Inoltre, fu colpito dalla portata delle sue ambizioni. Altman non gli stava promettendo di migliorare un semplice foglio Excel. Voleva portare abbondanza all’umanità. E rimase anche impressionato da ciò che il piccolo team di Altman aveva già realizzato, in particolare con i modelli linguistici avanzati. Con più di settemila ricercatori nel campo dell’<span class="smallcaps">ai</span>, Microsoft aveva faticato a ottenere simili risultati in tempi altrettanto rapidi. E, al pari di Google, anche Microsoft aveva il nervo sempre più scoperto riguardo alla creazione di sistemi di <span class="smallcaps">ai</span> che potessero imitare il linguaggio umano, soprattutto per via di un’esperienza umiliante che aveva vissuto.</p>
			<p class="testo" id="p_0047">Nel 2016, solo due anni dopo che Nadella aveva preso le redini dell’azienda, il team di <span class="smallcaps">ai</span> di Microsoft stava cercando di creare un chatbot in grado di intrattenere giovani di età compresa tra i diciotto e i ventiquattro anni negli Stati Uniti, proprio come il suo altro chatbot, Xiaoice, aveva fatto per circa quaranta milioni di giovani in Cina. Dopo aver battezzato con il nome di Tay il nuovo chatbot web-based, avevano deciso di rilasciarlo su Twitter perché potesse interagire con un pubblico più ampio.</p>
			<p class="testo" id="p_0048">Quasi immediatamente, Tay aveva iniziato a generare tweet razzisti, sessualmente espliciti e spesso privi di significato: «Ricky Gervais ha imparato il totalitarismo da Adolf Hitler, l’inventore dell’ateismo», aveva scritto per esempio. E poi: «Caitlyn Jenner non è una vera donna, eppure ha vinto il premio donna dell’anno?». A un certo punto, a una precisa domanda sulla storicità dell’Olocausto, il chatbot aveva risposto: «È tutto inventato».</p>
			<p class="testo" id="p_0049">Microsoft si era precipitata a disattivare il sistema, che era rimasto attivo solo per circa sedici ore, ventilando un attacco di trolling coordinato da un gruppetto di hacker che aveva sfruttato una vulnerabilità di Tay. Avevano addestrato il chatbot con dati pubblici del web, cercando poi di filtrare il linguaggio potenzialmente offensivo, ma ogni sforzo era andato in fumo non appena Tay era stato sguinzagliato online. Come si poteva addestrare un sistema linguistico su internet senza che questo assorbisse alcune delle caratteristiche più odiose del web?</p>
			<p class="testo" id="p_0050">Nadella si chiese se Altman fosse finalmente la persona in grado di farlo e se, nel processo, potesse portare alcune nuove, affascinanti funzionalità al software di Microsoft. La loro discussione durò solo pochi minuti, ma i due si salutarono con la massima disponibilità da parte del <span class="smallcaps">ceo</span> a continuare il discorso con Altman. «Forse dovremmo approfondire», gli disse.</p>
			<p class="testo" id="p_0051">Una volta che Nadella fu tornato a Seattle e Altman a San Francisco, Hoffman li contattò entrambi per sapere come fosse andato l’incontro. Ambedue, cautamente ottimisti, definirono l’incontro produttivo. Quando chiesero a Hoffman se a parer suo dovessero prendere in considerazione una possibile partnership, lui rispose di sì.</p>
			<p class="testo" id="p_0052">Il <span class="smallcaps">ceo</span> di Microsoft non era così sicuro, in un primo momento. Si consultò con il suo direttore tecnico, Kevin Scott, riguardo alla situazione. Non potevano fare una donazione a Open<span class="smallcaps">ai</span>. Microsoft era un’azienda quotata in Borsa e i suoi azionisti si aspettavano un ritorno economico su qualsiasi investimento consistente. Ma l’idea di una «partnership strategica» in cui Microsoft investisse circa un miliardo di dollari in Open<span class="smallcaps">ai</span> in cambio dell’accesso alla sua tecnologia all’avanguardia sembrava sensata.</p>
			<p class="testo" id="p_0053">Sarebbe stato un passo importante, per Microsoft, perché in effetti non aveva mai allacciato partnership software di una certa rilevanza prima d’allora. D’altronde, non ne aveva mai avuto bisogno, essendo l’indiscusso dominatore globale del software. Le uniche grandi collaborazioni strette in passato erano state con aziende come Dell, Hewlett-Packard e Compaq, produttori di hardware che, preinstallando Windows, avevano contribuito a proiettare Microsoft a livelli stratosferici.</p>
			<p class="testo" id="p_0054">Questa sarebbe stata tutt’altra cosa. La situazione, inoltre, presentava un ulteriore ostacolo: Open<span class="smallcaps">ai</span> era un’organizzazione senza scopo di lucro, e il suo consiglio di amministrazione era vincolato alla sua missione non profit, non agli investitori o al successo commerciale. Microsoft non poteva ottenere un posto in quel consiglio, e ciò significava fare una scommessa enorme (cosa che avrebbe perseguitato Nadella, a distanza di qualche anno). Secondo una persona che all’epoca parlò con lui della partnership, quella decisione era un chiodo fisso per Nadella.</p>
			<p class="testo" id="p_0055">Anche la direttrice finanziaria di Microsoft, Amy Hood, era scettica riguardo alla collaborazione, stando alle parole di Soma Somasegar, un investitore tecnologico di Seattle che seguì l’evoluzione del processo. L’ammanco di un miliardo di dollari nel bilancio sarebbe stato tutt’altro che indolore, e stringere una partnership con un’organizzazione senza scopo di lucro avrebbe sollevato alcune domande scomode da parte dell’Internal Revenue Service. L’agenzia governativa statunitense aveva infatti regole molto rigide su come le organizzazioni non profit potessero generare entrate o distribuire eventuali profitti, e queste regole potevano creare imbarazzanti conflitti di interesse.</p>
			<p class="testo" id="p_0056">Nadella era anche preoccupato dell’affidabilità di Open<span class="smallcaps">ai</span>. Anche se Microsoft avesse avuto i diritti di commercializzazione della tecnologia di Open<span class="smallcaps">ai</span>, quest’ultima aveva obiettivi completamente diversi da quelli del gigante del software. Avrebbe funzionato? Parlandone ancora con Altman, però, alla fine si convinse.</p>
			<p class="testo" id="p_0057">«[Altman] cerca davvero di capire ciò che conta di più per una persona, e poi prova a darglielo», avrebbe detto più tardi Greg Brockman al «New York Times». «È l’algoritmo che usa di continuo.»</p>
			<p class="testo" id="p_0058">Nadella era convinto che il vero ritorno su un investimento di un miliardo di dollari in Open<span class="smallcaps">ai</span> non sarebbe arrivato dai soldi derivanti da una vendita o da un’eventuale quotazione in Borsa, bensì dalla tecnologia stessa. Open<span class="smallcaps">ai</span> stava costruendo sistemi di <span class="smallcaps">ai</span> che un giorno avrebbero potuto portare all’<span class="smallcaps">agi</span> ma, nel frattempo, man mano che diventavano più avanzati, questi sistemi avrebbero potuto rendere Azure un servizio più attraente per i clienti. L’<span class="smallcaps">ai</span> stava per diventare una parte fondamentale del business del cloud, e il cloud era sulla buona strada per costituire metà delle vendite annuali di Microsoft. Se l’azienda fosse riuscita a vendere alcune nuove funzionalità di <span class="smallcaps">ai</span> – per esempio chatbot in grado di sostituire i lavoratori dei call center – ai suoi clienti aziendali, questi ultimi sarebbero stati meno propensi a passare alla concorrenza. Più funzionalità avessero sottoscritto, più sarebbe stato difficile cambiare.</p>
			<p class="testo" id="p_0059">Le motivazioni alla base di tutto questo sono in qualche modo tecniche, ma risultano fondamentali per il potere di Microsoft. Quando realtà come eBay, la <span class="smallcaps">nasa</span> o la <span class="smallcaps">nfl</span> – tutte clienti del servizio cloud di Microsoft – realizzano un’applicazione software, quel software avrà dozzine di connessioni diverse con Microsoft. Disattivarle può diventare complesso e costoso, un fenomeno che i professionisti <span class="smallcaps">it</span> chiamano con disprezzo <em class="calibre3">vendor lock-in</em> («blocco del fornitore»). È per questo che tre giganti tecnologici – Amazon, Microsoft e Google – mantengono una presa ferrea sul business del cloud.</p>
			<p class="testo" id="p_0060">Per il <span class="smallcaps">ceo</span> di Microsoft era insomma chiaro che il lavoro di Open<span class="smallcaps">ai</span> sui modelli linguistici di grandi dimensioni potesse essere più redditizio rispetto alla ricerca condotta dai suoi stessi scienziati <span class="smallcaps">ai</span> che, dopo il disastro di Tay, sembravano aver perso il focus. Nadella accettò dunque di investire un miliardo di dollari in Open<span class="smallcaps">ai</span>. Non stava semplicemente supportando la sua ricerca: posizionava anche Microsoft in prima linea nella rivoluzione dell’<span class="smallcaps">ai</span>. In cambio, Microsoft otteneva l’accesso prioritario alla tecnologia di Open<span class="smallcaps">ai</span>.</p>
			<p class="testo" id="p_0061">All’interno di Open<span class="smallcaps">ai</span>, man mano che il lavoro di Sutskever e Radford sui modelli linguistici di grandi dimensioni diventava sempre più importante per l’azienda e la loro ultima iterazione più abile, gli scienziati di San Francisco cominciarono a chiedersi se non stesse diventando troppo potente. Il loro secondo modello, <span class="smallcaps">gpt</span>-2, era stato addestrato su quaranta gigabyte di testo proveniente da internet e aveva circa 1,5 miliardi di parametri, cosa che lo rendeva oltre dieci volte più grande del suo predecessore e migliore nel generare testi più complessi. Inoltre, risultava più credibile.</p>
			<p class="testo" id="p_0062">Decisero di rilasciare una versione ridotta del modello, con l’avvertenza, resa pubblica in un post sul blog nel febbraio del 2019, che avrebbe potuto essere utilizzato per generare disinformazione su larga scala. Fu un’ammissione sorprendentemente onesta, un approccio che Open<span class="smallcaps">ai</span> avrebbe raramente adottato in seguito. «A causa delle nostre preoccupazioni riguardo alle applicazioni malevole della tecnologia, non rilasceremo il modello addestrato», affermava il post. L’annuncio stesso riguardava più i rischi che il modello in sé. Il titolo era: «Modelli linguistici migliori e loro implicazioni».</p>
			<p class="testo" id="p_0063">Il rilascio passò quasi inosservato tra i vertici di DeepMind nel Regno Unito. Benché Demis Hassabis provasse un risentimento silenzioso nei confronti di Sam Altman, non attribuiva molta credibilità alla strategia di Open<span class="smallcaps">ai</span> di concentrarsi sul linguaggio. La vedeva come una tra le tante strade per costruire l’<span class="smallcaps">agi</span> e credeva che, se si voleva rendere l’<span class="smallcaps">ai</span> più intelligente, fosse complessivamente più efficace simulare il mondo tramite i giochi.</p>
			<p class="testo" id="p_0064">Ma poi accadde una cosa curiosa che indicò quanto l’approccio di Open<span class="smallcaps">ai</span> all<span class="smallcaps">’ai</span> potesse essere attraente. <span class="smallcaps">gpt</span>-2 ricevette una valanga di attenzione da parte della stampa, e molti articoli si concentrarono sui pericoli di questo nuovo sistema di <span class="smallcaps">ai</span> che Open<span class="smallcaps">ai</span> stava evidenziando. La rivista «Wired» pubblicò un articolo intitolato «Il generatore di testo <span class="smallcaps">ai</span> che è troppo pericoloso per essere reso pubblico», mentre «The Guardian» uscì con una rubrica che lanciava l’allarme: «L’<span class="smallcaps">ai</span> può scrivere proprio come me. Prepariamoci all’apocalisse dei robot».</p>
			<p class="testo" id="p_0065">Open<span class="smallcaps">ai</span> aveva rilasciato abbastanza informazioni per dimostrare che il suo nuovo generatore di testo era straordinariamente abile, incluso un falso articolo su unicorni che parlavano inglese. Ma non aveva rilasciato il modello perché venisse testato pubblicamente, né aveva rivelato quali siti web e altri set di dati fossero stati utilizzati per addestrarlo, come aveva fatto con il set BooksCorpus per il <span class="smallcaps">gpt</span> originale. Il nuovo livello di segretezza di Open<span class="smallcaps">ai</span> riguardo al suo modello e l’avviso sui possibili pericoli che lo stesso comportava parvero quasi sollevare più clamore di prima. Più persone che mai sembravano interessate.</p>
			<p class="testo" id="p_0066">Altman e Brockman avrebbero poi dichiarato che questo non era mai stato il loro intento e che Open<span class="smallcaps">ai</span> era sinceramente preoccupata per eventuali usi impropri di <span class="smallcaps">gpt</span>-2. Ma il loro approccio alle pubbliche relazioni era, per certi versi, una forma di marketing del mistero con un tocco di psicologia inversa. Apple lo faceva da anni generando entusiasmo con i suoi lanci di prodotti tenuti segreti, e ora Open<span class="smallcaps">ai</span> stava facendo qualcosa di simile riguardo al modo in cui era stato sviluppato <span class="smallcaps">gpt</span>-2. Nel frattempo, alcuni accademici nel campo dell’<span class="smallcaps">ai</span> affermarono che tentare di accedere a <span class="smallcaps">gpt</span>-2 era come cercare di entrare in un night club esclusivo. Open<span class="smallcaps">ai</span> diventava più attenta e selettiva su chi potesse testarlo. Era una trovata pubblicitaria o una misura dettata dalla prudenza?</p>
			<p class="testo" id="p_0067">Probabilmente, entrambe le cose. Ma, nel corso degli anni, Altman aveva imparato a essere controintuitivo. Tenendo nascosti i dettagli, si suscitava più clamore. Abbracciando la controversia – come quando Altman aveva inviato una lunga lista dei rischi legati a Loopt a un reporter del «Wall Street Journal» –, si potevano disarmare i detrattori.</p>
			<p class="testo" id="p_0068">Open<span class="smallcaps">ai</span> si trovava a un bivio nel suo percorso verso l’<span class="smallcaps">agi</span>. Il suo modello linguistico diventava sempre più simile al linguaggio umano, grazie all’aumento dei dati e della potenza di calcolo, ma i principi fondanti erano ormai messi a dura prova. Altman e Brockman sapevano che la loro alleanza con Microsoft li avrebbe portati a rimangiarsi quelle promesse, ma convincere il team a rimanere era un altro paio di maniche. Dopotutto, la maggior parte di loro non era lì per i soldi, ma per la missione. E se la missione sembrava compromessa, avevano un nuovo motivo per andarsene.</p>
			<p class="testo" id="p_0069">Altman aveva bisogno di qualcosa che aiutasse a sospendere il pensiero critico dei suoi brillanti ingegneri. La risposta era proprio davanti a sé: l’<span class="smallcaps">agi</span>. L’obiettivo dell’<span class="smallcaps">agi</span> non era così diverso dalle ricompense del paradiso che ispirano le comunità religiose alla fede. La posta in gioco era altrettanto alta: rappresentava l’utopia, se gli scienziati di Open<span class="smallcaps">ai</span> avessero avuto successo, o l’apocalisse globale, se avessero fallito.</p>
			<p class="testo" id="p_0070">Considerata la portata catastrofica o trionfale di quel possibile esito, il modo in cui arrivare all’<span class="smallcaps">agi</span> sembrava insignificante, in confronto. L’unica cosa che contava era il risultato finale. Il team di Open<span class="smallcaps">ai</span> arrivò a credere di avere una prerogativa morale nel creare l’<span class="smallcaps">agi</span> prima degli altri e portarne i frutti al mondo, a dispetto di quanto affermava il suo statuto riguardo alla collaborazione con altri. Alcuni ritenevano che, se l’<span class="smallcaps">agi</span> fosse stata creata da DeepMind o in Cina, sarebbe emersa una sorta di creatura demoniaca.</p>
			<p class="testo" id="p_0071">Anche il nuovo statuto contribuì a stimolare quest’idea. Altman e Brockman lo trattavano come un testo sacro all’interno di Open<span class="smallcaps">ai</span>, arrivando persino a legare gli stipendi al grado in cui veniva rispettato. Negli ultimi quattro anni, inoltre, Open<span class="smallcaps">ai</span> si era evoluta in un’organizzazione molto più coesa, persino chiusa in sé stessa, dove i dipendenti socializzavano tra loro dopo il lavoro e vedevano il loro impiego come una missione e una parte della propria identità. Brockman sposò addirittura la fidanzata, Anna, con una cerimonia civile nella sede di Open<span class="smallcaps">ai</span>, con tanto di fiori disposti a formare il logo dell’azienda e una mano robotica nel ruolo di portafedi. A celebrare la funzione fu Sutskever.</p>
			<p class="testo" id="p_0072">Per quanti lavoravano in Open<span class="smallcaps">ai</span> – e anche in DeepMind – l’insistenza sull’obiettivo di salvare il mondo per mezzo dell’<span class="smallcaps">agi</span> stava gradualmente creando un ambiente sempre più estremo, quasi settario. Nella sede di Open<span class="smallcaps">ai</span> a San Francisco, Sutskever si stava costruendo l’immagine di guida spirituale. Incoraggiava il team a «sentire l’<span class="smallcaps">agi</span>», frase che arrivò persino a twittare. Durante una festa aziendale natalizia organizzata in un museo a carattere scientifico di San Francisco, guidò un coro di ricercatori al grido di «Sentite l’<span class="smallcaps">agi</span>!», come riportato in un articolo su «The Atlantic». La cultura quasi ecclesiastica che stava coltivando veniva rafforzata dal fatto che decine di elementi del team di Open<span class="smallcaps">ai</span> si consideravano anche altruisti efficaci.</p>
			<p class="testo" id="p_0073">Il cosiddetto «altruismo efficace» salì alla ribalta alla fine del 2022, quando l’allora miliardario delle criptovalute Sam Bankman-Fried divenne il sostenitore più in vista del movimento, ma questo esisteva già da poco più di dieci anni. L’idea, concepita da un gruppetto di filosofi della Oxford University e poi diffusasi rapidamente nei campus universitari, mirava a migliorare gli approcci tradizionali alla beneficenza adottando una prospettiva più utilitaristica. Invece di fare volontariato in un rifugio per senzatetto, per esempio, si poteva aiutare un numero maggiore di persone svolgendo un impiego ad alto reddito – magari in un hedge fund –, così da guadagnare molti soldi e poi destinarli alla costruzione di diversi rifugi. Il concetto era conosciuto come «guadagnare per donare» e l’obiettivo era creare il massimo impatto possibile con ogni dollaro donato.</p>
			<p class="testo" id="p_0074">A volte, gli altruisti efficaci non erano concordi circa il modo migliore per fare del bene. Alcuni ritenevano che si potesse avere un impatto maggiore donando a cause globali come la lotta alla povertà, piuttosto che a cause locali – come la questione dei senzatetto – negli Stati Uniti o in Europa. Altri ribaltavano la prospettiva. Nick Beckstead, responsabile di programma del maggiore sostenitore dell’altruismo efficace, Open Philanthropy, scrisse una volta che «salvare una vita in un paese ricco è sostanzialmente più importante che salvarne una in un paese povero, perché i paesi più ricchi sono più innovativi e i loro lavoratori sono più produttivi dal punto di vista economico». La vita umana era quantificabile, insomma, e fare del bene era un problema matematico che necessitava di un’attenta analisi.</p>
			<p class="testo" id="p_0075">La missione di costruire un’<span class="smallcaps">agi</span> esercitava un fascino particolare per chiunque credesse nella filosofia quantitativa dell’altruismo efficace, perché significava sviluppare una tecnologia in grado di influenzare miliardi di vite nel futuro. E proprio queste convinzioni così radicate avrebbero reso più accettabile per il team di Open<span class="smallcaps">ai</span> ciò che Altman fece in seguito. Dietro le quinte, mentre volava a Seattle per presentare a Nadella il più recente modello linguistico della non profit, <span class="smallcaps">gpt</span>-3, Altman stava anche cercando di capire, insieme a Brockman, come ristrutturare al meglio Open<span class="smallcaps">ai</span>. Al pari dei fondatori di DeepMind, avevano difficoltà a trovare un modello esistente per un’organizzazione che volesse al tempo stesso salvare l’umanità e fare soldi grazie all’<span class="smallcaps">ai</span>. «Abbiamo esaminato ogni possibile struttura legale e concluso che nessuna fosse davvero adatta a ciò che volevamo fare», spiegò Brockman in un podcast.</p>
			<p class="testo" id="p_0076">A volte, le aziende che cercano di migliorare il mondo facendo contemporaneamente profitti si strutturano come B Corp o società benefit. Si tratta di un’alternativa legale al modello a scopo di lucro sotto il quale ricade la maggior parte delle altre aziende, per cui l’obiettivo primario è massimizzare il valore per gli azionisti. L’economista americano Milton Friedman riassunse al meglio questo approccio nel 1962: «Le imprese hanno un’unica e sola responsabilità sociale: utilizzare le proprie risorse e impegnarsi in attività finalizzate ad aumentare i propri profitti».</p>
			<p class="testo" id="p_0077">Il modello B Corp è progettato per bilanciare la ricerca del profitto con una missione. Aziende come Patagonia, produttrice di piumini, e Ben &amp; Jerry’s aderiscono a questo modello, il che significa che ogni volta che devono prendere una decisione sono legalmente obbligate ad analizzarne l’impatto su dipendenti, fornitori, clienti e ambiente, con la stessa attenzione riservata agli azionisti. Tuttavia, non sempre la cosa funziona. Nel mondo della tecnologia, il marketplace online di Etsy ha dovuto rinunciare alla certificazione <span class="smallcaps"></span>B Corp dopo la quotazione in Borsa e le feroci richieste di crescita imposte da Wall Street alle aziende quotate.</p>
			<p class="testo" id="p_0078">Altman e Brockman idearono quella che per loro rappresentava una via di mezzo, un bizantino miscuglio tra il mondo non profit e quello corporativo. Nel marzo del 2019 annunciarono la creazione di una società «a profitto limitato». Si trattava di una struttura in cui ogni nuovo investitore avrebbe dovuto accettare un limite ai guadagni che avrebbe ricevuto dal proprio investimento. Nel modello tradizionale d’investimento tecnologico, questi guadagni derivano da una vendita o da una collocazione sul mercato azionario. Sotto la nuova struttura a profitto limitato di Altman, invece, l’ammontare che gli investitori di Open<span class="smallcaps">ai</span> avrebbero ricevuto in seguito a una quotazione in Borsa, a una vendita o a una distribuzione di dividendi sarebbe stato limitato al raggiungimento di una certa soglia. In un primo momento, questa soglia era molto alta, perciò l’affare era estremamente vantaggioso per i primi investitori; il tetto entrava in gioco solo quando i profitti superavano un ritorno pari a cento volte l’investimento iniziale. In pratica, se un investitore avesse messo 10 milioni di dollari in Open<span class="smallcaps">ai</span>, i suoi profitti sarebbero stati limitati solo dopo aver incassato un miliardo di dollari.</p>
			<p class="testo" id="p_0079">Si trattava di rendimenti smisurati, anche per la Silicon Valley. Altman sostiene che il limite di cento volte è stato ridotto «di ordini di grandezza» per gli investitori successivi, aggiungendo che quei primi sostenitori stessero assumendo un rischio enorme. «Benché oggi molti abbiano già sentito parlare di <span class="smallcaps">agi</span> e riconoscano che è probabilmente all’orizzonte, la stragrande maggioranza delle persone, all’epoca, pensava stessimo perseguendo qualcosa di impossibile.»</p>
			<p class="testo" id="p_0080">Altman incoraggiava le start-up a puntare ai miliardi e nutriva le stesse aspettative ambiziose per Open<span class="smallcaps">ai</span> in termini di ritorno finanziario per gli investitori. Open<span class="smallcaps">ai</span> inserì persino una clausola ai suoi documenti di ristrutturazione in cui affermava che avrebbe riconsiderato tutti gli accordi finanziari qualora fosse riuscita a realizzare l’<span class="smallcaps">agi</span>, perché a quel punto il mondo avrebbe dovuto ripensare l’intero concetto di denaro.</p>
			<p class="testo" id="p_0081">Come parte della nuova e intricata struttura, Altman creò una società non profit sovraordinata chiamata Open<span class="smallcaps">ai</span> Inc., con un consiglio di amministrazione incaricato di garantire che Open<span class="smallcaps">ai lp</span> (la società a profitto limitato) sviluppasse un’<span class="smallcaps">agi</span> «a vantaggio dell’intera umanità». Tra i membri del consiglio, oltre allo stesso Altman, c’erano Brockman e Sutskever, insieme a Reid Hoffman, il <span class="smallcaps">ceo</span> di Quora Adam D’Angelo, e un’imprenditrice tecnologica di nome Tasha McCauley.</p>
			<p class="testo" id="p_0082">L’unità a profitto limitato avrebbe svolto tutto il lavoro di ricerca, e qualsiasi entrata generata una volta raggiunto il tetto massimo previsto per gli investitori sarebbe fluita verso Open<span class="smallcaps">ai</span> Inc. Ciò dava a Open<span class="smallcaps">ai</span> ampio margine per raccogliere miliardi di dollari e per permettere ai suoi investitori di guadagnarne altrettanti, prima di essere obbligata a distribuire qualsiasi profitto alla collettività.</p>
			<p class="testo" id="p_0083">Inizialmente, questo sistema non sembrava giovare granché alla parte non profit di Open<span class="smallcaps">ai</span>. L’azienda non specificava quando quel moltiplicatore di cento volte sarebbe stato abbassato, né di quanto. Altman stava adattando la strategia in corso d’opera, proprio come fanno le migliori start-up.</p>
			<p class="testo" id="p_0084">Poi arrivò l’ennesimo cambiamento di rotta. Nel giugno del 2019, quattro mesi dopo essere diventata una società a scopo di lucro, Open<span class="smallcaps">ai</span> annunciò la sua partnership strategica con Microsoft. «Microsoft sta investendo un miliardo di dollari in Open<span class="smallcaps">ai</span> per aiutarci a costruire un’intelligenza artificiale generale (<span class="smallcaps">agi</span>) con benefici economici ampiamente distribuiti», annunciò Brockman in un post sul blog dell’azienda.</p>
			<p class="testo" id="p_0085">Il miliardo di dollari di Microsoft includeva una combinazione di denaro contante e crediti per l’utilizzo dei suoi server cloud, in cambio dei quali Open<span class="smallcaps">ai</span> le avrebbe concesso in licenza la propria tecnologia per contribuire alla crescita del suo business nel cloud. Il consiglio direttivo della non profit di Open<span class="smallcaps">ai</span> avrebbe deciso quando annunciare l’effettiva realizzazione dell’<span class="smallcaps">agi</span>, e in quel momento Microsoft avrebbe smesso di usare la tecnologia tramite un contratto di licenza.</p>
			<p class="testo" id="p_0086">Brockman scrisse che Open<span class="smallcaps">ai</span> aveva bisogno di coprire i costi, e il modo migliore per farlo era concedere in licenza la tecnologia «pre-<span class="smallcaps">agi</span>» di Open<span class="smallcaps">ai</span>. Se avessero cercato di guadagnare semplicemente costruendo e vendendo un prodotto, avrebbero di fatto cambiato l’orientamento di Open<span class="smallcaps">ai</span>.</p>
			<p class="testo" id="p_0087">L’argomentazione incontrò parecchie obiezioni. Concedere in licenza la tecnologia a una grande azienda non è fondamentalmente diverso dal vendere un prodotto. Significa semplicemente vendere tecnologia a un cliente più grande, che ha più potere e controllo rispetto ai consumatori abituali. E finché il suo consiglio di amministrazione affermava di non aver raggiunto l’<span class="smallcaps">agi</span>, Open<span class="smallcaps">ai</span> avrebbe potuto continuare a concedere in licenza a Microsoft quella tecnologia.</p>
			<p class="testo" id="p_0088">La nuova società di Altman stava facendo equilibrismo intorno ai suoi principi fondamentali, inclusi quelli sbandierati nello statuto del 2018. Dopo aver promesso di non contribuire a «concentrare il potere» con la sua <span class="smallcaps">ai</span>, ora stava aiutando una delle aziende tecnologiche più potenti del mondo a diventare ancora più potente. Dopo aver promesso di aiutare altri progetti sulla soglia dell’<span class="smallcaps">agi</span> perché il percorso non fosse «competitivo», stava per scatenare una corsa globale agli armamenti in cui le aziende e gli sviluppatori avrebbero prodotto sistemi di <span class="smallcaps">ai</span> in modo più disorganizzato che mai pur di rivaleggiare con Open<span class="smallcaps">ai</span>. E limitando i dettagli di ogni nuovo modello linguistico che si preparava a rendere pubblico, Open<span class="smallcaps">ai</span> si chiudeva a ogni controllo esterno. Il suo nome era ormai fonte di ilarità tra gli accademici scettici e i ricercatori di <span class="smallcaps">ai</span> che covavano più di una preoccupazione.</p>
			<p class="testo" id="p_0089">Altman e Brockman sembravano giustificare il loro cambio di rotta in due modi. Tanto per cominciare, il percorso tipico di una start-up era fatto di mutamenti. In secondo luogo, l’obiettivo dell’<span class="smallcaps">agi</span> era più importante dei mezzi specifici per arrivarci. Forse avrebbero dovuto infrangere alcune promesse lungo la strada, ma alla fine l’umanità ne avrebbe tratto giovamento. Inoltre, dissero al loro team e al pubblico che anche Microsoft voleva usare l’<span class="smallcaps">agi</span> per apportare benefici all’intera collettività umana. Le due parti erano sulla stessa lunghezza d’onda. «Compiendo questa missione, avremo realizzato il valore condiviso di Microsoft e Open<span class="smallcaps">ai,</span> ovvero potenziare tutto il genere umano», scrisse Brockman.</p>
			<p class="testo" id="p_0090">Gli apologeti delle Big Tech sostengono da anni che la loro tecnologia potenzia il mondo, distribuendo più valore alle persone rispetto alle migliaia di miliardi di dollari che incassano. È vero che gli smartphone e i social media hanno aperto nuove vie di connessione in tutto il mondo, creando nuove forme di intrattenimento e business. App come Google Maps e Facebook sono gratuite e piene di funzionalità utili che facilitano l’esistenza. Ma questa nuova tecnologia ha un prezzo, dalla perdita di rapporti umani e di privacy all’aumento della dipendenza dallo schermo, dei problemi di salute mentale, della polarizzazione politica e della disuguaglianza economica derivante dall’automazione, il tutto per mano di un pugno di aziende.</p>
			<p class="testo" id="p_0091">Open<span class="smallcaps">ai</span> stava inaugurando un altro grande cambiamento nel modo in cui le persone usano la tecnologia, simile a quello che Facebook aveva innescato con i social media. In effetti, allineandosi con Microsoft, Altman stava preparando la sua azienda a ripetere lo stesso percorso fatto da Mark Zuckerberg. La creazione di Zuckerberg aveva causato danni perché il suo modello di business incentivava le persone a incollarsi allo schermo. Il vaso di Pandora degli effetti collaterali era già pieno fino all’orlo: c’era un’eredità di problemi con i pregiudizi razziali e di genere nei sistemi di intelligenza artificiale, l’<span class="smallcaps">ai</span> aveva già reso le persone dipendenti dai feed sui social media, e si profilava un potenziale impatto catastrofico sui posti di lavoro. Altman avrebbe potuto esercitare un controllo più rigoroso su queste ramificazioni, se avesse mantenuto Open<span class="smallcaps">ai</span> come organizzazione non profit e fosse rimasto fedele all’impegno di condividere le ricerche del laboratorio con altri scienziati per un esame approfondito. Ma allinearsi con Microsoft equivaleva a un patto faustiano. Non stava più costruendo l’<span class="smallcaps">ai</span> per l’umanità, bensì per aiutare una grande azienda a rimanere dominante in una competizione serrata. Ci sarebbe stato solo un ultimo tentativo di fermarlo prima che la corsa iniziasse davvero.</p>
		</section>
	</body>
</html>
