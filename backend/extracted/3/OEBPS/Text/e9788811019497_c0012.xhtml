<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:ibooks="http://apple.com/ibooks/html-extensions" lang="it-IT" xml:lang="it-IT" class="calibre">
  <head>
    <title>FILE 12 – Supremacy – Capitolo</title>
    <meta content="urn:uuid:35484d83-cd47-4b21-bd76-ea9d55abc8bb" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="../../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../../page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body epub:type="bodymatter" class="calibre2">
		<section epub:type="chapter" role="doc-chapter" class="calibre">
			<h2 class="capitolo" id="h2-0001"><span class="smallcaps1">9. il paradosso di golia</span></h2>
			<p class="testo" id="p_0001">Nel 2017, Google contava circa 80.000 dipendenti. Non tutti erano ingegneri. C’erano i curatori del Google Doodle che appariva ogni giorno sopra la barra di ricerca. C’erano chiropratici e responsabili dei massaggi in ufficio, «snackologi» con l’incarico di assicurarsi che i tre pasti caldi serviti in mensa tenessero il personale ben nutrito, orticoltori che si prendevano cura delle piante e addetti delle pulizie che lucidavano i tavoli da calcetto.</p>
			<p class="testo" id="p_0002">Il modello di business di Google era una gallina dalle uova d’oro. Quell’anno, il suo settore pubblicitario generò quasi 100 miliardi di dollari – cifra più che raddoppiata di lì al 2024 –, quindi era naturale che gran parte di quei profitti venisse destinata a potenziare il talento interno. Nella Silicon Valley si tendeva a misurare il successo con due parametri: quanto denaro raccoglievi dagli investitori e quante persone assumevi. Un numero spropositato di impiegati rifletteva i sogni di espansione imperiale di <span class="smallcaps">ceo</span> come Larry Page e Sergey Brin, anche se non sempre era chiaro cosa facessero molti dei loro manager di medio livello.</p>
			<p class="testo" id="p_0003">L’eccessiva espansione aziendale di Google non era un caso così insolito. All’epoca, Facebook contava circa 40.000 dipendenti e Microsoft 124.000, mentre i fondatori di start-up sognavano di gestire i propri campus aziendali con tanto di palestre e chioschi di gelato gratis. Demis Hassabis era l’eccezione alla regola, forse perché si trovava dall’altra parte dell’oceano. Non voleva che DeepMind venisse risucchiata dalle distrazioni della Silicon Valley o dalle sue ossessioni per i benefit e le dimensioni.</p>
			<p class="testo" id="p_0004">Il problema, in un gigante come Google, era che, se qualcuno al suo interno avesse inventato qualcosa di rivoluzionario, avrebbe faticato comunque a emergere. Il business pubblicitario digitale di Google era sacro. Non era possibile toccare gli algoritmi che lo alimentavano, a meno che non fosse strettamente necessario. Benché la Silicon Valley venisse celebrata come il centro nevralgico dell’innovazione mondiale, le sue aziende più grandi non erano poi così innovative. La homepage di Google era rimasta pressoché invariata nell’ultimo decennio. L’iPhone era sempre il medesimo rettangolo di metallo. E quasi ogni nuova funzione di Facebook era copiata da competitor come Snapchat o TikTok. Una volta che queste aziende avevano raggiunto un fatturato nell’ordine delle decine di miliardi, modificare la formula del loro successo diventava troppo rischioso.</p>
			<p class="testo" id="p_0005">Ecco perché, quando un gruppo di ricercatori di Google fece una delle scoperte più importanti degli ultimi dieci anni nel campo dell’<span class="smallcaps">ai</span>, l’azienda lasciò che languisse inutilizzata. La loro storia, in sintesi, mostrava come la scala monopolistica delle Big Tech ne limitasse la capacità di innovare, costringendole a reagire alle innovazioni altrui copiandole o acquisendole direttamente. Ma questa specifica negligenza si rivelò particolarmente dannosa per Google. Alla fine, Open<span class="smallcaps">ai</span> non solo avrebbe sfruttato a proprio vantaggio la grande invenzione di Google, ma l’avrebbe usata anche per lanciare la prima vera minaccia al gigante della ricerca.</p>
			<p class="testo" id="p_0006">La «<span class="smallcaps">t»</span> di Chat<span class="smallcaps">gpt</span> sta per <em class="calibre3">Transformer</em> («Trasformatore<span class="smallcaps">»)</span>. Ovviamente questo non ha nulla a che fare con i robot alieni che si trasformano in autoarticolati, ma si riferisce a un sistema che consente alle macchine di generare testi simili a quelli umani. Il transformer è diventato fondamentale per la nuova ondata di <span class="smallcaps">ai</span> <em class="calibre3">generativa</em> che può produrre testi realistici, immagini, video, sequenze di <span class="smallcaps">dna</span> e molti altri tipi di dati. L’invenzione del trasformatore, nel 2017, ha avuto sul campo dell’<span class="smallcaps">ai</span> un impatto simile a quello degli smartphone sui consumatori. Prima degli smartphone, i telefoni cellulari potevano fare ben poco oltre a effettuare chiamate, inviare messaggi e far girare la solita partita di <em class="calibre3">Snake</em>. L’arrivo degli smartphone con touchscreen ha permesso agli utenti di navigare su internet, usare il <span class="smallcaps">gps</span>, scattare foto di alta qualità e utilizzare milioni di app.</p>
			<p class="testo" id="p_0007">I trasformatori hanno anche ampliato le possibilità per gli ingegneri di <span class="smallcaps">ai</span>, consentendo loro di gestire una quantità di dati molto maggiore e di processare il linguaggio umano assai più rapidamente. Prima dei trasformatori, parlare con un chatbot era come interagire con una macchina ottusa, perché i sistemi precedenti si basavano su set di regole e alberi decisionali. Se chiedevi qualcosa che non era già previsto nel suo programma (circostanza molto probabile), il bot andava in tilt o commetteva errori bizzarri. Questo era il modo in cui erano stati inizialmente progettati assistenti digitali come Siri di Apple, Alexa di Amazon e persino Google Assistant. Trattavano ogni richiesta come una singola istanza isolata, il che non consentiva loro di comprendere il contesto. Non riuscivano a ricordare le domande che avevi posto in precedenza nello stesso modo in cui lo farebbe una persona durante una conversazione. Per esempio:</p>
			<blockquote class="citazione" id="blq-1">
			<p class="testo" id="p_0008">«Alexa, com’è il tempo a Indianapolis in questo momento?»</p>
			<p class="testo" id="p_0009">«In questo momento a Indianapolis ci sono -4 gradi Celsius con cielo nuvoloso.»</p>
			<p class="testo" id="p_0010">«Quante ore impiegherei per volare da Londra a lì?»</p>
			<p class="testo" id="p_0011">«Da Londra, per volare fino alla tua posizione attuale, impiegheresti circa quarantacinque minuti.»</p>
			</blockquote>
			<p class="testo" id="p_0012">Mentre scrivevo queste pagine mi trovavo nel Surrey, che presumibilmente dista circa quarantacinque minuti di volo dall’aeroporto di Heathrow. Poco importava come Alexa avesse elaborato quel piano di volo contorto: il problema era che non riusciva a capire che «lì<span class="smallcaps">»</span> si riferiva a Indianapolis, di cui avevo chiesto appena due secondi prima. I sistemi dietro la maggior parte di questi assistenti digitali tradizionali erano limitati e ancora basati principalmente su parole chiave. Ecco perché continuavano a fornire risposte preconfezionate.</p>
			<p class="testo" id="p_0013">I trasformatori hanno liberato i chatbot da queste limitazioni. All’improvviso, ecco che erano in grado di gestire le sfumature e gli slang. Potevano fare riferimento a ciò che avevi detto poche frasi prima. Potevano affrontare quasi ogni tipo di domanda casuale e fornire una risposta personalizzata. Una sola parola riassumeva il miglioramento: erano più <em class="calibre3">generali</em>. E, per molti ricercatori nel campo dell’<span class="smallcaps">ai</span>, questo rappresentava un passo verso l’<span class="smallcaps">agi</span>, oltre ad aprire un dibattito su un tema fondamentale: i computer cominciavano davvero a «comprendere<span class="smallcaps">»</span> il linguaggio allo stesso modo degli esseri umani o lo stavano semplicemente elaborando attraverso previsioni matematiche?</p>
			<p class="testo" id="p_0014">In un certo senso, è sorprendente che questa invenzione sia venuta fuori da Google. Nonostante il talento e le risorse a disposizione, la complessità della sua struttura e la paura di danneggiare il business pubblicitario costituivano un ostacolo per i dipendenti che cercavano di introdurre innovazioni. Google Brain vantava i ricercatori più avanzati nel campo dell’apprendimento automatico, ma questi ultimi dovevano misurarsi con obiettivi e strategie poco chiari imposti dalla dirigenza. La cultura dell’autocompiacimento derivava in parte dal fatto di poter contare su tanti scienziati di talento, come Geoffrey Hinton. L’asticella era alta, e Google usava già tecniche di <span class="smallcaps">ai</span> all’avanguardia come le reti neurali ricorrenti per processare miliardi di parole di testo ogni giorno.</p>
			<p class="testo" id="p_0015">Se foste stati giovani ricercatori come Illia Polosukhin, vi sareste ritrovati accanto alle persone che avevano inventato queste tecniche. Nei primi mesi del 2017, Polosukhin si stava accingendo a lasciare Google ed era disposto a correre qualche rischio. In una delle mense aziendali, due piani sotto l’ufficio di Larry Page, il venticinquenne ucraino stava facendo brainstorming con altri due ricercatori, Ashish Vaswani e Jakob Uszkoreit. Nemmeno i suoi compagni di pranzo amavano seguire le convenzioni adottate dagli altri scienziati dell’azienda. Vaswani desiderava occuparsi di un progetto ambizioso. Uszkoreit, che lavorava per Google da più di dieci anni, guardava con sospetto al modo in cui la struttura degli incentivi di Google Brain si era trasformata in una sorta di istituzione accademica glorificata; dopo aver assunto decine di neolaureati e accademici, si ritrovava circondato da persone il cui primo pensiero era pubblicare articoli o figurare negli atti di una conferenza. Che fine aveva fatto l’obiettivo di creare prodotti fuori dall’ordinario?</p>
			<p class="testo" id="p_0016">Uszkoreit riceveva sguardi ammirati alle feste, non appena rivelava dove lavorava. Ma, ogni volta che precisava di occuparsi di Google Translate, ecco che la gente scoppiava a ridere. Il servizio era macchinoso e spesso impreciso, specialmente con lingue non latine come il cinese. Polosukhin concordò sul fatto che Google Translate facesse abbastanza schifo. Aveva amici, in Cina, che si lamentavano del servizio. E, in quel momento, Uszkoreit si chiese ad alta voce se non ci fosse un modo per migliorarlo. Gli ingegneri di Google erano per lo più convinti di lavorare già con la tecnologia più avanzata, quindi seguivano il motto: «Se non è rotto, non aggiustarlo<span class="smallcaps">»</span>. Uszkoreit vedeva le cose in maniera diversa: «Se non è rotto, rompilo<span class="smallcaps">»</span>.</p>
			<p class="testo" id="p_0017">«E se eliminassimo le reti neurali ricorrenti nel decodificatore della traduzione automatica e usassimo solo il meccanismo di attenzione?<span class="smallcaps">»</span> fu la domanda successiva. «Non velocizzerebbe i tempi di inferenza?<span class="smallcaps">»</span></p>
			<p class="testo" id="p_0018">In termini di <span class="smallcaps">ai</span>, i ricercatori si stavano chiedendo se fosse possibile sfruttare meglio i potentissimi chip di calcolo. Fino a quel momento, per analizzare le parole, Google aveva utilizzato la tecnica delle reti neurali ricorrenti. Il sistema esaminava ogni parola in una sequenza, come fareste voi leggendo una frase da sinistra a destra. Questo era il metodo all’avanguardia, allora, ma non sfruttava appieno i potenti chip – prodotti da aziende come Nvidia – in grado di gestire più attività contemporaneamente. Il chip del vostro laptop aveva probabilmente quattro «core<span class="smallcaps">»</span> per gestire le istruzioni, ma i chip <span class="smallcaps">gpu</span> utilizzati nei server per processare i sistemi di <span class="smallcaps">ai</span> ne avevano migliaia. Ciò significava che un modello di <span class="smallcaps">ai</span> poteva «leggere<span class="smallcaps">»</span> molte parole in una frase contemporaneamente, non solo in sequenza. Non sfruttare questi chip era come spegnere una sega elettrica per tagliare il legno manualmente. Immaginate di scollegare una sega elettrica per trascinarne ripetutamente la lama su una tavola di legno. Sarebbe un lavoro lento e faticoso, un inutile spreco del potenziale della macchina. Lo stesso stava accadendo con i sistemi di <span class="smallcaps">ai</span> che elaboravano il linguaggio: non stavano usando tutto il potenziale dei chip che li alimentavano.</p>
			<p class="testo" id="p_0019">Ricercatori come Vaswani stavano studiando il concetto di «attenzione<span class="smallcaps">»</span> nell’<span class="smallcaps">ai</span>, ovvero la capacità di un computer di individuare le informazioni più importanti all’interno di un dataset. Tra un’insalata e un panino, i tre si domandarono se fosse possibile utilizzare quella stessa tecnica per tradurre le parole in modo più veloce e preciso.</p>
			<p class="testo" id="p_0020">Nei mesi successivi, i ricercatori iniziarono a sperimentare. Davanti al silenzioso scetticismo di chiunque li vedesse, Uszkoreit disegnava diagrammi della nuova architettura sulle lavagne dell’ufficio. Ciò su cui stava lavorando il suo team non aveva alcun senso, all’epoca. L’ipotesi era appunto quella di rimuovere l’elemento «ricorrente<span class="smallcaps">»</span> delle reti neurali, il che sembrava pura follia. Le diverse architetture che Vaswani stava costruendo, inoltre, non erano ancora significativamente migliori dello <em class="calibre3">status quo</em>. Ma quando si sparse la voce del loro progetto, altri ricercatori chiesero di farne parte.</p>
			<p class="testo" id="p_0021">Uno di questi era Noam Shazeer, già una leggenda in Google per aver contribuito a inventare un sistema che aiutava il programma AdSense a stabilire quali annunci mostrare su determinate pagine web. Sempre sorridente e con la voce tonante, era considerato eccentrico e chiacchierava con dirigenti come Sundar Pichai quasi fossero vecchi amici. Shazeer vantava una vasta esperienza con i modelli linguistici di grandi dimensioni, programmi informatici in grado di analizzare e generare testi simili a quelli prodotti dagli esseri umani dopo essere stati addestrati su miliardi di parole. Poco dopo essersi unito a quel gruppo di ricercatori, Shazeer escogitò alcuni trucchi che aiutarono il nuovo modello a gestire enormi quantità di dati.</p>
			<p class="testo" id="p_0022">«Una volta messe insieme tutte quelle cose, è successo qualcosa di magico», ricorda Uszkoreit. «È stato allora che una serie di altre idee ha preso il volo.»</p>
			<p class="testo" id="p_0023">Ben presto, otto ricercatori lavoravano al progetto, ancora senza nome, programmando in codice e affinando l’architettura di quello che avrebbero battezzato <em class="calibre3">Transformer</em>. Il nome faceva riferimento a un sistema capace di trasformare qualsiasi input in qualsiasi output e, benché gli scienziati si concentrassero sulla traduzione linguistica, il loro sistema avrebbe finito per fare molto di più.</p>
			<p class="testo" id="p_0024">Dopo un po’, cominciarono a notare alcuni miglioramenti. «Oh, wow, questa è un’altra cosa!» commentò Uszkoreit a un certo punto. Il sistema stava generando strutture frasali lunghe e complesse in tedesco; e, avendo trascorso molti anni in Germania, da bambino, si accorse che era migliore dei soliti contenuti prodotti da Google Translate. Era fluido, leggibile e, cosa più importante, corretto dal punto di vista fattuale. Polosukhin, che parlava francese, notò la stessa cosa.</p>
			<p class="testo" id="p_0025">Llion Jones, un programmatore gallese del team, rimase stupefatto nel constatare che il sistema stava procedendo con la cosiddetta risoluzione delle coreferenze (<em class="calibre3">coreference resolution</em>). Questa designa il compito di individuare tutte le espressioni che, all’interno di un testo, si riferiscono alla stessa entità, e aveva rappresentato un grosso ostacolo nel tentativo di far sì che i computer elaborassero il linguaggio in maniera corretta.</p>
			<p class="testo" id="p_0026">Prendiamo per esempio la frase «La gallina non attraversò la strada perché era troppo stanca»: per noi esseri umani è ovvio che «era» si riferisce alla gallina. Nella frase «La gallina non attraversò la strada perché era troppo larga», invece, è altrettanto ovvio che «era» si riferisce alla strada. Fino ad allora, era stato estremamente difficile far comprendere all’<span class="smallcaps">ai</span> questo tipo di cambiamento nel contesto, poiché ciò richiedeva un qualche elemento di conoscenza del senso comune, maturata nel tempo attraverso l’esperienza di come funziona il mondo e di come oggetti ed esseri viventi interagiscono.</p>
			<p class="testo" id="p_0027">«È un classico test d’intelligenza in cui l’<span class="smallcaps">ai</span> ha sempre fallito», dice Jones. «Non riuscivamo a inserire il senso comune in una rete neurale.» Ma quando alimentarono il trasformatore con le stesse frasi, i ricercatori videro accadere qualcosa d’insolito alla sua unità di attenzione (<em class="calibre3">attention head</em>). L’unità di attenzione era come un microrilevatore all’interno del modello che si concentrava su diverse parti dei dati in ingresso. Era la componente che sfruttava la potenza dei chip più avanzati e che permetteva al trasformatore di prestare attenzione a tutte le parole di una frase contemporaneamente, invece che in sequenza.</p>
			<p class="testo" id="p_0028">Quando i ricercatori sostituirono la parola «stanca» con «larga», notarono come l’unità di attenzione spostasse il riferimento dalla gallina alla strada.</p>
			<p class="testo" id="p_0029">«Credo che nessuno avesse mai visto una cosa del genere, prima», ricorda Jones. Gli venne quasi da chiedersi se non stava assistendo a uno sprazzo di intelligenza reale. «Il fatto che stesse estraendo il senso comune da un testo non strutturato era la prova che stava accadendo qualcosa di più interessante.»</p>
			<p class="testo" id="p_0030">Circa sei mesi dopo quelle prime conversazioni a pranzo, i ricercatori misero per iscritto i risultati ottenuti. Polosukhin aveva già lasciato Google, ma gli altri continuarono a lavorare al progetto, trattenendosi in ufficio fino a mezzanotte. Vaswani, che era l’autore principale, era solito dormire su un divano.</p>
			<p class="testo" id="p_0031">«Ci serve un titolo», disse a un certo punto.</p>
			<p class="testo" id="p_0032">Jones alzò lo sguardo dalla sua scrivania. «Non sono molto bravo con i titoli», rispose. «Ma che ne dici di <em class="calibre3">Attention Is All You Need</em>?» Era un pensiero che gli era venuto in mente così, a caso, e Vaswani non fece alcun commento. Anzi, si alzò e se ne andò, ricorda Jones.</p>
			<p class="testo" id="p_0033">Più tardi, però, quel titolo finì sulla prima pagina del loro articolo, sintesi perfetta di ciò che avevano scoperto. Quando si usava un trasformatore, il sistema di <span class="smallcaps">ai</span> poteva <em class="calibre3">prestare attenzione</em> a grandi quantità di dati contemporaneamente e utilizzarli in svariate maniere.</p>
			<p class="testo" id="p_0034">«Mi piace pensarli come motori di ragionamento», dice Vaswani.</p>
			<p class="testo" id="p_0035">Quei motori di ragionamento avevano il potenziale per rivoluzionare i sistemi di <span class="smallcaps">ai</span>, ma Google fu lenta a cogliere l’opportunità. Ci vollero diversi anni, per esempio, prima che l’azienda integrasse i trasformatori in servizi come Google Translate o <span class="smallcaps">bert</span>, un grande modello linguistico sviluppato per migliorare la capacità del motore di ricerca di comprendere le sfumature del linguaggio umano.</p>
			<p class="testo" id="p_0036">Gli inventori del trasformatore non potevano non sentirsi frustrati. Persino una piccola start-up in Germania aveva iniziato a usare il trasformatore per la traduzione automatica ben prima di Google, costringendo il colosso tecnologico alla rincorsa.</p>
			<p class="testo" id="p_0037">Alcuni di loro cercarono di mostrare a Google le possibilità più ampie offerte dal trasformatore. Poco dopo la pubblicazione dell’articolo, Shazeer iniziò a lavorare con un collega per applicare la tecnologia a un nuovo chatbot chiamato Meena. Lo addestrarono su circa quaranta miliardi di parole tratte da conversazioni sui social media e arrivarono a credere che avrebbe rivoluzionato il modo in cui le persone cercavano informazioni sul web e interagivano con i computer. Meena era talmente sofisticato da improvvisare giochi di parole o scambiare battute con un utente umano con la stessa naturalezza con cui poteva sostenere un dibattito filosofico.</p>
			<p class="testo" id="p_0038">Shazeer e il suo collega ne erano entusiasti e cercarono di inviare dettagli del bot a ricercatori esterni, nella speranza di lanciare una demo pubblica e migliorare il rudimentale Google Assistant presente nelle case in forma di altoparlante, per sostituirlo con qualcosa di molto più avanzato. Ma i dirigenti di Google stroncarono sul nascere ogni velleità. Temevano che il bot potesse fare affermazioni sconvenienti, danneggiando la reputazione dell’azienda o, più precisamente, il suo business pubblicitario digitale da 100 miliardi di dollari. Secondo un’inchiesta del «Wall Street Journal», frenarono ogni tentativo di Shazeer di rendere Meena accessibile al pubblico o di integrarlo nei prodotti dell’azienda.</p>
			<p class="testo" id="p_0039">«Google non si muove se non c’è di mezzo un business da un miliardo di dollari», dice Polosukhin. «E avviare un business da un miliardo di dollari è davvero complicato.» Ecco perché così tanti dipendenti di Google avevano lasciato l’azienda per fondare duemila start-up diverse, secondo un’intervista rilasciata da Pichai a Bloomberg nel 2023. Sebbene il dato possa far sembrare Google una fucina di innovazione, in realtà il colosso della ricerca somigliava più a un’enorme piovra che risucchiava ogni innovazione circostante. Molti di quegli imprenditori che avevano avviato nuove aziende finirono per rivenderle a Google o per accettarne gli investimenti. Quando Google non riesce a innovare, di solito compra.</p>
			<p class="testo" id="p_0040">Ci sono due modi di guardare all’approccio cauto di Google verso le nuove tecnologie. Pubblicamente, l’azienda si è sempre mossa con una certa prudenza. E molti ricercatori interni concordano sul fatto che i dirigenti vogliano davvero essere attenti nel distribuire l’<span class="smallcaps">ai</span> in maniera tale che non danneggi la società. Negli ultimi anni, Google ha stilato un elenco di principi guida per l’uso dell’<span class="smallcaps">ai</span>, copiando in gran parte regole già elaborate da DeepMind. Nel 2018, il capo del suo dipartimento legale, Kent Walker, annunciò che l’azienda avrebbe smesso di vendere tecnologie di riconoscimento facciale a causa del potenziale rischio di abusi. Più in generale, Google sottopone i suoi algoritmi a un rigoroso processo di revisione interna, che talvolta include anche l’intervento di esperti esterni per valutare eventuali problemi etici.</p>
			<p class="testo" id="p_0041">Tuttavia, l’azienda continuava a prendere decisioni eticamente discutibili. Nel maggio di quello stesso anno, Pichai presentò una nuova funzionalità dell’assistente vocale chiamata Duplex: un’<span class="smallcaps">ai</span> in grado di telefonare a un ristorante per prenotare un tavolo, utilizzando un intercalare infarcito di «ehm» e «uh» al punto da sembrare umana in modo inquietante. Pichai concluse la dimostrazione tra applausi scroscianti, ma il servizio non rivelava di essere una macchina. Google fu subito criticata per aver ingannato le persone all’altro capo del telefono.</p>
			<p class="testo" id="p_0042">L’approccio prudente di Google era in gran parte il risultato della sua lentezza burocratica. L’altra faccia della medaglia, per una delle aziende più grandi di sempre, con un monopolio quasi assoluto sul mercato della ricerca online, è che tutto si muove a passo di lumaca. Il timore costante di una reazione pubblica negativa o di controlli normativi paralizza l’innovazione. La principale priorità diventa sostenere la crescita e preservare la posizione dominante. Google è stata così ossessionata dal mantenere la sua presa sul mercato della ricerca che nel 2021 ha pagato oltre 26,3 miliardi di dollari ad Apple, Samsung e altri – più di <em class="calibre3">un terzo</em> del suo utile netto di quell’anno – solo per preinstallare il suo motore di ricerca sui loro dispositivi, com’è emerso in una recente causa antitrust intentata dal dipartimento di Giustizia degli Stati Uniti.</p>
			<p class="testo" id="p_0043">Le dimensioni colossali dell’azienda e la sua mania di crescita costringevano i ricercatori e gli ingegneri a superare numerosi livelli di gestione anche solo per far approvare delle minuzie. E con una concorrenza praticamente inesistente – dato che Google controllava circa il 90 per cento di tutte le ricerche online nel mondo – non c’era alcuna urgenza di innovare.</p>
			<p class="testo" id="p_0044">A un certo punto, mentre il gruppo dei trasformatori affinava le proprie ricerche, Shazeer si ritrovò a chiacchierare direttamente con Pichai accanto a una delle tante macchinette del caffè sparse per l’azienda. I lunghi trascorsi come esperto di <span class="smallcaps">ai</span> all’interno di Google gli avevano permesso di instaurare rapporti personali con alcuni dei massimi dirigenti. Secondo uno dei coautori dell’articolo sui trasformatori, Łukasz Kaiser, presente alla conversazione, Shazeer disse con la massima sicumera: «Questa cosa rimpiazzerà completamente Google».</p>
			<p class="testo" id="p_0045">«Aveva già questa convinzione, ovvero che avrebbe sostituito tutto», ricorda Kaiser. Shazeer diceva più o meno le stesse cose anche ai colleghi e aveva promosso il potenziale rivoluzionario del trasformatore in un promemoria interno indirizzato alla dirigenza di Google; dunque, non stava affatto scherzando. Il trasformatore permetteva ai computer di generare non solo testi, ma risposte a ogni tipo di domanda. Se i consumatori avessero iniziato a usare sempre più di frequente uno strumento simile, avrebbero potuto finire per trascurare progressivamente Google.</p>
			<p class="testo" id="p_0046">Pichai sembrò liquidare il commento, catalogando Shazeer come uno dei ricercatori più eccentrici di Google e limitandosi a dire: «Va bene, approfondite la questione». In preda alla frustrazione, Shazeer lasciò Google nel 2021 per proseguire autonomamente la sua ricerca sui modelli linguistici di grandi dimensioni, cofondando un’azienda di chatbot chiamata Character.ai. A quel punto, l’articolo <em class="calibre3">Attention Is All You Need</em> era diventato uno dei lavori di ricerca più influenti di tutti i tempi nel campo dell’<span class="smallcaps">ai</span>. Di norma, un articolo scientifico sull’<span class="smallcaps">ai</span> poteva ricevere qualche decina di citazioni, se i suoi autori erano fortunati. Ma il paper sui trasformatori ebbe un impatto tale, tra gli scienziati, da guadagnarsene più di ottantamila.</p>
			<p class="testo" id="p_0047">Non c’era nulla di insolito nel fatto che Google condividesse con il mondo alcune delle meccaniche di base di un’invenzione: era così che operavano spesso le aziende tecnologiche. Quando rendevano «open-source» nuove tecniche, ricevevano feedback dalla comunità scientifica, accrescendo così la loro reputazione tra i migliori ingegneri e facilitandone le assunzioni. Ma Google sottovalutò il costo che avrebbe pagato questa volta. Nessuno degli otto ricercatori che hanno inventato il trasformatore lavora più per l’azienda. La maggior parte di loro ha fondato una propria società di <span class="smallcaps">ai</span>, e al momento della stesura di questo testo il loro valore complessivo superava i 4 miliardi di dollari. Character.ai, da sola, valeva un miliardo di dollari ed era destinata a diventare uno dei siti di chatbot più popolari al mondo. Grazie a quell’innovazione che Google non ha saputo sfruttare in maniera adeguata, Shazeer è ora proiettato verso la stratosfera: «La ricerca online è una tecnologia da mille miliardi di dollari, ma mille miliardi non sono <em class="calibre3">cool</em>», dice oggi dal suo ufficio a Menlo Park, in California. «Sai cos’è <em class="calibre3">cool</em>? Un milione di miliardi di dollari. Questa è una tecnologia da un milione di miliardi di dollari, perché se la ricerca aveva lo scopo di rendere universalmente accessibili le informazioni, l’<span class="smallcaps">ai</span> ha lo scopo di rendere universalmente accessibile <em class="calibre3">l’intelligenza</em> e, in tal modo, rendere tutti enormemente più produttivi.»</p>
			<p class="testo" id="p_0048">Dopo la partenza di Shazeer, Google tenne per sé la sua ricerca su Meena, che in seguito ribattezzò Language Model for Dialogue Applications, o <span class="smallcaps">l</span>a<span class="smallcaps">mda</span>. I suoi scienziati continuarono a lavorare sul modello, addestrandolo e perfezionandolo con l’aiuto di collaboratori esterni fino a renderlo fluido e, con loro sorpresa, incredibilmente simile a un essere umano.</p>
			<p class="testo" id="p_0049">Per quanto questi progressi fossero entusiasmanti, Google voleva tenere tutto confinato nella sua bolla interna: <span class="smallcaps">l</span>a<span class="smallcaps">mda</span> era probabilmente il chatbot più avanzato al mondo, ma solo poche persone all’interno di Google potevano utilizzarlo. L’azienda era riluttante a rilasciare qualsiasi nuova tecnologia potesse finire per minacciare il successo della sua attività principale, la ricerca online. I dirigenti e il team di pubbliche relazioni la presentarono come una strategia improntata alla prudenza ma, più di ogni altra cosa, l’azienda era intenzionata a mantenere la propria reputazione e lo <em class="calibre3">status quo</em>.</p>
			<p class="testo" id="p_0050">Ben presto, Google avrebbe vissuto quello che Ashish Vaswani definisce un «momento biblico». Mentre il colosso di Mountain View continuava a stampare denaro grazie al business pubblicitario, Open<span class="smallcaps">ai</span> sembrava in procinto di compiere un passo monumentale verso l’<span class="smallcaps">agi</span> e, a differenza di Google, non stava mantenendo il segreto.</p>
		</section>
	</body>
</html>
