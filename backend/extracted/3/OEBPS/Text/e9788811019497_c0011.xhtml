<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:ibooks="http://apple.com/ibooks/html-extensions" lang="it-IT" xml:lang="it-IT" class="calibre">
  <head>
    <title>FILE 11 – Supremacy – Capitolo</title>
    <meta content="urn:uuid:35484d83-cd47-4b21-bd76-ea9d55abc8bb" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="../../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../../page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body epub:type="bodymatter" class="calibre2">
		<section epub:type="chapter" role="doc-chapter" class="calibre">
			<h2 class="capitolo" id="h2-0001"><span class="smallcaps1">8. è meraviglioso</span></h2>
			<p class="testo" id="p_0001">Per capire perché sia diventato così tremendamente difficile progettare sistemi di <span class="smallcaps">ai</span> etici in Google, o anche solo trasformare idee innovative in prodotti, bisogna fare un passo indietro ed esaminare un paio di dati. Al momento della stesura di questo libro, la società madre di Google, Alphabet Inc., aveva una capitalizzazione di mercato pari a 1.800 miliardi di dollari. Apple era stata la prima azienda statunitense quotata in Borsa a raggiungere una valutazione di 2.000 miliardi di dollari (nel 2020), mentre le valutazioni di mercato di Amazon e Microsoft si aggiravano rispettivamente intorno ai 1.700 e ai 3.000 miliardi di dollari nel 2024. Prima che Apple diventasse la prima società da 1.000 miliardi di dollari, nel 2018, nessuna azienda aveva mai raggiunto simili dimensioni. Eppure, c’è un elemento che accomuna quasi tutte le aziende con i numeri più alti al mondo: sono imprese tecnologiche. In effetti, le aziende che in genere consideriamo colossi hanno solo un quarto delle dimensioni delle controparti nella Silicon Valley. Il gigante petrolifero Exxon Mobil ha una valutazione di mercato di appena 450 miliardi di dollari, mentre Walmart vale 435 miliardi. Se combiniamo la capitalizzazione di mercato dei giganti tecnologici, superiamo il prodotto interno lordo della maggior parte delle nazioni del pianeta, a eccezione degli Stati Uniti e della Cina.</p>
			<p class="testo" id="p_0002">Guardando alla storia, le aziende che un tempo consideravamo giganti impallidiscono se confrontate a quelle di oggi. Al suo apice, prima che venisse smembrata nel 1984, <span class="smallcaps">at&amp;t</span> aveva una capitalizzazione di mercato di circa 60 miliardi di dollari dell’epoca, pari a circa 150 miliardi di dollari odierni. La capitalizzazione massima di General Electric, raggiunta nel 2000, era di circa 600 miliardi di dollari.</p>
			<p class="testo" id="p_0003">Nemmeno il dominio di mercato dei giganti tecnologici ha precedenti. Prima che i regolatori la smantellassero nel 1911, Standard Oil controllava circa il 90 per cento del settore petrolifero negli Stati Uniti. Oggi, Google controlla circa il 92 per cento del mercato globale dei motori di ricerca. Ogni giorno, circa un miliardo di persone in tutto il mondo effettua una ricerca su Google. Più di due miliardi accedono a Facebook. E circa un miliardo e mezzo di persone possiede un iPhone. Nessun governo o impero nella storia ha mai raggiunto un numero così grande di individui in un colpo solo.</p>
			<p class="testo" id="p_0004">Ci sono voluti poco più di due decenni perché queste aziende raggiungessero una portata simile, a partire dal boom e dal susseguente crollo delle dot-com. Come sono diventate così grandi? Hanno acquisito aziende come DeepMind, YouTube e Instagram, e hanno accumulato una quantità prodigiosa di dati sui consumatori, cosa che ha permesso ad alcune di loro di indirizzarci con pubblicità e suggerimenti in grado di influenzare il comportamento umano su larga scala. Mentre Google raccoglie dati tramite query di ricerca e interazioni su YouTube, Amazon traccia i nostri acquisti e le abitudini di navigazione. La quantità di dati raccolti è difficile da comprendere per le persone comuni: dettagli personali, cronologia di navigazione, dati sulla posizione e, in alcuni casi, persino registrazioni vocali. E queste informazioni non sono soltanto voluminose, ma anche eterogenee, tanto da consentire alle aziende tecnologiche di ottenere un quadro minuzioso del comportamento dei consumatori.</p>
			<p class="testo" id="p_0005">Realtà come Facebook e Google utilizzano questi dati per condurre pubblicità ipermirate, mostrando annunci che catturano l’interesse dei singoli e alimentano sofisticati algoritmi di raccomandazione. Questi software gestiscono i «feed» che gli utenti scorrono ogni giorno, assicurandosi che i contenuti mostrati siano quelli più adatti a mantenerli incollati allo schermo. Le aziende hanno tutto l’interesse a renderci il più possibile dipendenti dalle loro piattaforme, poiché questo genera più introiti pubblicitari. Ma gli effetti negativi sono numerosi. Gli americani sono così dipendenti da Facebook, Instagram e altre app di social media che nel 2023 hanno controllato i loro telefoni in media 144 volte al giorno, secondo uno studio.</p>
			<p class="testo" id="p_0006">Tutta questa «distribuzione personalizzata di contenuti» ha anche esacerbato le divisioni generazionali e politiche tra milioni di persone, poiché i contenuti più coinvolgenti sono spesso quelli che suscitano indignazione. Facebook, per esempio, ha spesso raccomandato i contenuti politici più provocatori nei feed degli utenti durante le elezioni presidenziali statunitensi del 2016, esponendo molte persone a notizie e opinioni che rinforzavano le loro convinzioni, in modo da creare vere e proprie bolle informative. Lo stesso fenomeno ha alimentato il crescente risentimento verso l’immigrazione in Gran Bretagna nei mesi precedenti al referendum britannico sulla Brexit, così come l’ostilità verso il popolo rohingya del Myanmar nel 2017. Secondo un rapporto di Amnesty International, gli algoritmi di Facebook hanno amplificato così tanto la diffusione di contenuti di odio contro i rohingya da favorire la campagna genocida dell’esercito del Myanmar che ha perpetrato uccisioni, torture, stupri e deportazioni di massa nei confronti degli appartenenti a questo gruppo etnico musulmano. Facebook ha poi ammesso a mezzo stampa di non aver fatto abbastanza per prevenire l’incitamento alla violenza contro i rohingya.</p>
			<p class="testo" id="p_0007">Nonostante le divisioni che aveva seminato, il modello di business di Facebook – trattare i suoi miliardi di utenti e i loro dati come prodotto e i suoi inserzionisti come i veri clienti – si è rivelato incredibilmente vincente. Più dati riusciva a raccogliere, più guadagnava dalla pubblicità. Benché questo modello basato sul coinvolgimento abbia avuto effetti tossici sulla società, ha incentivato Facebook a perseguire un unico obiettivo: crescere il più possibile.</p>
			<p class="testo" id="p_0008">L’altro fattore che ha determinato la crescita esponenziale di queste aziende è stato l’effetto di rete, un fenomeno apparentemente magico che ogni fondatore di start-up sogna di ottenere. L’idea alla base dell’effetto di rete è che quanto più elevato è il numero di utenti e clienti di un’azienda, migliori diventano i suoi algoritmi, cosa che le permette di consolidare ulteriormente la propria presa sul mercato a discapito dei competitor. Nel caso di Facebook, per esempio, le persone hanno iniziato a iscriversi perché tutti gli altri erano già su Facebook, e molti vi sono rimasti per anni – o, almeno, hanno resistito alla tentazione di cancellare il proprio account – per lo stesso motivo. Se siete fan di Apple, saprete quanto sia difficile passare a un altro produttore di dispositivi come Samsung, o far funzionare gli accessori di quest’ultimo con un iPhone. Tutti questi prodotti e servizi interconnessi ostacolano il cambiamento, rafforzando il dominio di Apple.</p>
			<p class="testo" id="p_0009">Non abbiamo un punto di riferimento storico per capire cosa accade quando le aziende diventano così grandi. I valori di capitalizzazione di mercato che Google, Amazon e Microsoft stanno raggiungendo non hanno precedenti. E se da un lato questa crescita porta più ricchezza agli azionisti, compresi i fondi pensionistici, ha anche centralizzato il potere a tal punto che la privacy, l’identità, il dibattito pubblico e, sempre più spesso, le prospettive occupazionali di miliardi di persone dipendono da un pugno di grandi aziende gestite da un numero ristretto di individui incredibilmente ricchi.</p>
			<p class="testo" id="p_0010">Non c’è da meravigliarsi, quindi, se per chi lavora all’interno di un gigante tecnologico e si rende conto che qualcosa non va, lanciare l’allarme può sembrare tanto inutile quanto cercare di far virare il <em class="calibre3">Titanic</em> subito prima dell’impatto con l’iceberg. Eppure, tutto ciò non ha impedito a una scienziata di <span class="smallcaps">ai</span>, Timnit Gebru, di provarci.</p>
			<p class="testo" id="p_0011">Nel dicembre 2015, alla conferenza Neur<span class="smallcaps">ips</span> in cui Sam Altman ed Elon Musk annunciarono lo sviluppo di un’<span class="smallcaps">ai</span> «a beneficio dell’umanità», Gebru si guardò intorno, tra migliaia di altri partecipanti, e rabbrividì. Quasi nessuno, lì dentro, le somigliava. Gebru aveva poco più di trent’anni, era di colore e aveva avuto un’infanzia tutt’altro che convenzionale e priva del sistema di supporto di cui molti dei suoi coetanei avevano beneficiato.</p>
			<p class="testo" id="p_0012">Suo padre, un ingegnere elettronico di origini eritree, era morto quando lei aveva solo cinque anni, e da adolescente era stata costretta a fuggire dall’Etiopia devastata dalla guerra. Non vedendo di buon occhio le sue ambizioni di nuova immigrata, gli insegnanti delle superiori, nel Massachusetts, l’avevano scoraggiata dal frequentare i corsi di Advanced Placement, dicendole che sarebbero stati troppo difficili per lei. Come avrebbe raccontato a «Wired», un insegnante le aveva detto: «Ho incontrato tante persone come te che pensano di poter venire qui da altri paesi e seguire i corsi più difficili». Ma Gebru li aveva frequentati comunque, fino a ottenere una borsa di studio per ingegneria elettronica alla Stanford University.</p>
			<p class="testo" id="p_0013">Alla fine, si era imbattuta nel campo dell’<span class="smallcaps">ai</span> e della <em class="calibre3">computer vision</em>, un software in grado di «vedere» e analizzare il mondo reale. La tecnologia era affascinante, ma Gebru aveva subito colto alcuni segnali di allarme. I sistemi di <span class="smallcaps">ai</span> stavano assumendo un ruolo sempre più centrale nella vita delle persone: assegnavano un punteggio di credito, concedevano o no un mutuo, segnalavano il volto di qualcuno alla polizia, sostenevano un giudice umano nell’emettere una sentenza penale. Benché potessero sembrare perfetti arbitri neutrali, spesso questi sistemi non lo erano. Se i dati su cui venivano addestrati erano distorti, l’esito logico era inevitabile. E Gebru era dolorosamente consapevole di cosa significassero i pregiudizi.</p>
			<p class="testo" id="p_0014">In un bar di San Francisco, da giovane, era stata attaccata insieme a un’altra donna di colore da alcuni uomini che avevano cercato di strangolarle entrambe. Avevano chiesto aiuto, ma la polizia aveva finito per accusarle di mentire e le aveva trattenute in una cella. Più avanti, mentre scriveva la sua tesi a Stanford, aveva scoperto che in quell’istituto solo un’altra persona di colore aveva conseguito un dottorato in informatica. E dei cinquemila presenti alla più grande conferenza internazionale sull’<span class="smallcaps">ai</span> del 2015, quella in cui, appunto, Altman e Musk stavano lanciando Open<span class="smallcaps">ai</span>, solo cinque erano di colore.</p>
			<p class="testo" id="p_0015">Gebru sapeva che non si trattava di casi isolati. Il pregiudizio era sistemico. Decenni dopo l’era dei diritti civili del <span class="smallcaps">xx</span> secolo, il razzismo era ancora culturalmente radicato nelle istituzioni e nella psiche del mondo. L’<span class="smallcaps">ai</span> rischiava di peggiorare ulteriormente la situazione. Tanto per cominciare, era tipicamente progettata da persone che non avevano sperimentato il razzismo, e questo era uno dei motivi per cui i dati utilizzati per addestrare i modelli di <span class="smallcaps">ai</span> spesso non rappresentavano in modo equo le persone appartenenti a gruppi minoritari e le donne.</p>
			<p class="testo" id="p_0016">Gebru ne vedeva le conseguenze nella sua ricerca accademica. Per esempio, si era imbattuta in un’indagine su un software utilizzato dal sistema di giustizia penale degli Stati Uniti chiamato <span class="smallcaps">compas</span> (Correctional Offender Management Profiling for Alternative Sanctions), che i giudici e gli ufficiali per la libertà condizionale utilizzavano per prendere decisioni su cauzioni, condanne e libertà vigilata.</p>
			<p class="testo" id="p_0017"><span class="smallcaps">compas</span> utilizzava l’apprendimento automatico per assegnare punteggi di rischio agli imputati. Più alto era il punteggio, maggiore era la probabilità che reiterassero il reato. Lo strumento dava punteggi elevati agli imputati neri molto più spesso di quanto non facesse con i bianchi, ma le sue previsioni erano spesso errate. <span class="smallcaps">compas</span> era due volte più propenso a sbagliare nelle sue previsioni sul comportamento criminale degli imputati neri rispetto a quanto avveniva con i caucasici, secondo un’indagine del 2016 di ProPublica che ha esaminato settemila punteggi di rischio assegnati a persone arrestate in Florida, per poi verificare se erano state accusate di nuovi crimini nei due anni successivi. Lo strumento era anche più incline a giudicare erroneamente gli imputati bianchi che poi commettevano altri crimini, classificandoli a basso rischio. Il sistema di giustizia penale americano era già sbilanciato nei confronti delle persone nere, e quel pregiudizio sembrava destinato a perpetrarsi con l’uso di strumenti di <span class="smallcaps">ai</span> poco trasparenti.</p>
			<p class="testo" id="p_0018">Nella sua tesi di dottorato a Stanford, Gebru aveva portato un altro esempio di come le autorità potessero usare l’<span class="smallcaps">ai</span> secondo modalità inquietanti. Addestrò un modello di visione artificiale per identificare ventidue milioni di automobili mostrate su Google Street View, e poi analizzò cosa potessero rivelare quelle auto sulla demografia di una zona. Quando correlò le automobili con i dati del censimento e della criminalità, scoprì che le zone con più Volkswagen e pick-up tendevano ad avere più residenti bianchi, mentre quelle con Oldsmobile e Buick avevano più residenti neri. E le zone con più furgoni avevano anche più segnalazioni di crimine. Simili correlazioni potevano essere sfruttate. E se la polizia avesse usato quei dati per cercare di prevedere dove potevano verificarsi dei crimini, come in <em class="calibre3">Minority Report</em>?</p>
			<p class="testo" id="p_0019">L’idea non era poi così bislacca. Per diversi anni, i distretti di polizia degli Stati Uniti avevano usato i computer per consigliare ai loro agenti quali aree pattugliare, una tecnologia conosciuta come «polizia predittiva». Ma il software veniva addestrato sui dati storici e spesso finiva per indirizzare gli agenti verso comunità di minoranze. Se i dati mostravano che una comunità era sottoposta a una sorveglianza eccessiva, il software continuava a indirizzare la polizia su quella comunità, amplificando un problema già esistente.</p>
			<p class="testo" id="p_0020">L’<span class="smallcaps">ai</span> stava diffondendo anche altri stereotipi online, in modo sottile ma insidioso. Sia Google Translate sia Bing Translate a volte rendevano certi mestieri maschili quando venivano tradotti in altre lingue. La frase <em class="calibre3">o bir mühendis</em> in turco, lingua che ha pronomi di genere neutro, diventava <em class="calibre3">he is an engineer</em> in inglese, mentre <em class="calibre3">o bir hemşire</em> diventava <em class="calibre3">she is a nurse</em>. Il software operava queste deduzioni grazie a una tecnica detta <em class="calibre3">word embedding</em>, che esaminava le parole che tendevano a trovarsi vicino ad altre parole, come <em class="calibre3">engineer</em>. Il modello poi determinava quale altra parola si adattasse meglio, come <em class="calibre3">he</em>. Google, Facebook, Netflix e Spotify alimentavano tutte le loro raccomandazioni online con la tecnica del <em class="calibre3">word embedding</em>, nonostante gli squilibri di genere che introducevano nei propri software.</p>
			<p class="testo" id="p_0021">Era evidente che l’<span class="smallcaps">ai</span> avesse problemi che nessuno si era curato di risolvere, perciò, quando Sam Altman annunciò Open<span class="smallcaps">ai,</span> nel 2015, Gebru andò su tutte le furie. Scrisse una lettera aperta denunciando lo spreco di risorse da parte di pochi miliardari egocentrici come Musk e Thiel, che investivano i loro soldi nel tentativo di realizzare un’<span class="smallcaps">ai</span> dai poteri divini, e si lamentò del fatto che le uniche preoccupazioni sollevate riguardo alla nuova organizzazione non profit fossero che i suoi ricercatori erano troppo concentrati sul <em class="calibre3">deep learning</em>.</p>
			<p class="testo" id="p_0022">«Un magnate della tecnologia bianco, nato e cresciuto in Sudafrica durante l’apartheid, insieme a un gruppo di investitori e ricercatori tutti bianchi e tutti uomini, sta cercando di impedire che l’<span class="smallcaps">ai</span> “assuma il controllo del mondo” e l’unico problema che emerge è che “tutti i ricercatori stanno lavorando sul <em class="calibre3">deep learning</em>?”» scrisse. «Google ha recentemente rilasciato un algoritmo di visione artificiale che classificava le persone nere come scimmie. <span class="smallcaps">come scimmie</span>. Alcuni cercano di giustificare questo errore affermando che l’algoritmo deve aver considerato il colore come discriminante essenziale nella classificazione degli esseri umani. Se ci fosse stata anche solo una persona nera nel team, o semplicemente qualcuno che riflettesse sulle questioni razziali, un prodotto che classificava le persone nere come scimmie non sarebbe mai uscito. […] Immaginate un algoritmo che classificasse regolarmente le persone bianche come non umane. Nessuna azienda americana lo definirebbe un sistema pronto per il rilevamento delle persone.»</p>
			<p class="testo" id="p_0023">Uno dei suoi colleghi, però, le consigliò di non pubblicare la lettera. Era troppo esplicita, e probabilmente sarebbe stata identificata. Gebru decise di posticiparne la pubblicazione di qualche anno, ma non poté fare a meno di chiedersi perché alcune delle persone più potenti della Silicon Valley fossero così preoccupate della possibilità di una catastrofe apocalittica legata all’<span class="smallcaps">ai</span> quando la stessa <span class="smallcaps">ai</span> stava già arrecando danni reali alle persone. Le risposte erano due. La prima era che pochi, se non nessuno, dei leader di Open<span class="smallcaps">ai</span> e DeepMind erano mai stati o rischiavano di diventare vittime di discriminazioni razziali o di genere. La seconda consisteva nel fatto che, paradossalmente, era nell’interesse delle loro aziende sbandierare i pericoli di una superintelligenza onnipotente. Poteva non sembrare sensato lanciare avvertimenti sui pericoli di qualcosa che stai cercando di vendere, ma si trattava di una strategia di marketing brillante. La gente tendeva a preoccuparsi di più per il qui e ora che per il futuro a lungo termine. Se l’<span class="smallcaps">ai</span> sembrava destinata ad annientarci in futuro, ciò conferiva anche un fascino seducente alle sue capacità nel presente.</p>
			<p class="testo" id="p_0024">La strategia era anche un modo astuto per distogliere l’attenzione del pubblico dai problemi urgenti e più immediati su cui le aziende avrebbero dovuto intervenire, con la conseguenza, però, di rallentare lo sviluppo e limitare le capacità dei loro modelli di <span class="smallcaps">ai</span>. Infatti, per dissuadere i modelli di <span class="smallcaps">ai</span> dal prendere decisioni distorte sarebbe stato necessario dedicare più tempo ad analizzare i dati su cui venivano addestrati. Oppure bisognava restringerne il campo di applicazione, compromettendo in tal modo l’obiettivo di dare ai sistemi di <span class="smallcaps">ai</span> il potere di generalizzare le loro conoscenze.</p>
			<p class="testo" id="p_0025">Non sarebbe stata la prima volta che grandi aziende distraevano il pubblico mentre i loro affari prosperavano. Nei primi anni Settanta, l’industria della plastica, sostenuta dalle compagnie petrolifere, aveva iniziato a promuovere l’idea del riciclo come soluzione al crescente problema dei rifiuti plastici. Keep America Beautiful, per esempio, un’organizzazione fondata nel 1953 e finanziata in parte da aziende produttrici di bevande e imballaggi, lanciava campagne di servizio pubblico per incoraggiare i consumatori a riciclare. Il suo celebre spot con il pellerossa in lacrime era andato in onda nel 1971 in occasione della Giornata della Terra e spronava la gente a riciclare bottiglie e giornali allo scopo di prevenire l’inquinamento. Chiunque non lo avesse fatto, avrebbe mostrato un disprezzo palese nei confronti dell’ambiente.</p>
			<p class="testo" id="p_0026">Il riciclo non è una cosa negativa di per sé. Tuttavia, promuovendo questa pratica, l’industria poteva sostenere che la plastica non fosse intrinsecamente dannosa, purché venisse riciclata correttamente: in tal modo, spostava la percezione della responsabilità dai produttori ai consumatori. Le aziende della plastica sapevano che riciclare su larga scala era costoso e spesso inefficiente. Un’inchiesta del 2020 condotta da <span class="smallcaps">npr</span> e dalla serie <span class="smallcaps">pbs</span> <em class="calibre3">Frontline</em> ha scoperto che meno del 10 per cento della plastica viene effettivamente riciclato, nonostante decenni di campagne di sensibilizzazione pubblica.</p>
			<p class="testo" id="p_0027">In ogni caso, queste campagne sono riuscite a impedire che l’attenzione pubblica mettesse in discussione la rapida espansione della produzione di plastica e il danno che stava causando all’ambiente. Il riciclo è diventato parte del discorso pubblico. Testate giornalistiche, consumatori e decisori politici a Washington trascorrevano più tempo a parlare di come riciclare di più che a regolamentare la produzione effettiva di plastica da parte delle aziende.</p>
			<p class="testo" id="p_0028">Proprio come l’industria petrolifera ha sempre distolto l’attenzione del mondo dal suo significativo impatto ambientale, i principali costruttori di <span class="smallcaps">ai</span> potevano sfruttare il clamore intorno a un futuro tipo Terminator o Skynet per distrarre dai problemi che gli algoritmi di apprendimento automatico stavano già causando. La responsabilità non ricadeva sui creatori o sull’industria perché agissero subito: era un problema astratto, da affrontare in un secondo momento.</p>
			<p class="testo" id="p_0029">Nel gennaio del 2017, pochi mesi prima che DeepMind cercasse di aiutare Google a rientrare in Cina con AlphaGo, Gebru presentò i risultati della sua tesi a un pubblico di venture capitalist e dirigenti della Silicon Valley. Mentre faceva scorrere le slide, spiegò che i sistemi di <span class="smallcaps">ai</span> potevano combinare la capacità di riconoscere le automobili con quella di fare previsioni sui modelli di voto, per esempio, o il reddito familiare.</p>
			<p class="testo" id="p_0030">Uno dei venture capitalist presenti, un investitore di Tesla e amico di Elon Musk di nome Steve Jurvetson, rimase sbalordito, ma non per le ragioni che sperava Gebru. Pensò a quanto potere conferisse questo tipo di dati a Google e alle informazioni che l’azienda avrebbe potuto ottenere riguardo a diversi quartieri o città. Ne rimase così colpito da pubblicare alcune foto della presentazione di Gebru su Facebook.</p>
			<p class="testo" id="p_0031">In quello che rappresentava un costante cortocircuito nel campo dell’<span class="smallcaps">ai</span>, alcuni dei presenti coglievano un’opportunità finanziaria mentre altri, come la stessa Gebru, vedevano un pericolo da contenere. Ogni volta che le capacità dell’<span class="smallcaps">ai</span> crescevano, emergeva una conseguenza imprevista che spesso causava danni a un gruppo minoritario. I sistemi di riconoscimento facciale erano quasi perfetti nell’individuare i volti degli uomini bianchi, ma cadevano spesso in errore con le donne nere. Uno studio fondamentale del 2018 della ricercatrice del <span class="smallcaps">mit</span> Joy Buolamwini scoprì che i sistemi di riconoscimento facciale di <span class="smallcaps">ibm</span>, Microsoft e della cinese Face++ tendevano a classificare erroneamente il genere delle persone dalla pelle più scura e delle donne, cosa di cui lei stessa aveva fatto esperienza. Molti di questi sistemi venivano addestrati su dataset fotografici dominati da uomini caucasici e su immagini raccolte dal web. I database li sovrarappresentavano perché internet rifletteva la demografia delle popolazioni occidentali che avevano maggiore accesso alla rete.</p>
			<p class="testo" id="p_0032">Ma Gebru non aveva intenzione di mollare. Aveva alcune soluzioni. Una di queste prevedeva che i creatori di sistemi di <span class="smallcaps">ai</span> seguissero standard più rigorosi nell’addestrare i loro modelli. Dopo essere entrata in Microsoft, elaborò un insieme di regole chiamato <em class="calibre3">Datasheets for Datasets</em>, secondo cui, quando un modello di <span class="smallcaps">ai</span> veniva addestrato, i programmatori avrebbero dovuto creare una «scheda tecnica» contenente tutti i dettagli su com’era stato creato, cosa conteneva, come sarebbe stato utilizzato, quali potessero essere i suoi limiti e qualsiasi altra considerazione di carattere etico. Per gli sviluppatori di <span class="smallcaps">ai</span> poteva rappresentare un passaggio burocratico in più, ma il procedimento aveva un suo scopo. Se dal modello fossero emersi dei bias, sarebbe stato molto più facile comprenderne il motivo.</p>
			<p class="testo" id="p_0033">Capire perché i sistemi di <span class="smallcaps">ai</span> commettono errori è più difficile di quanto si pensi, soprattutto man mano che diventano più sofisticati. Nel 2018, Amazon si rese conto che uno strumento di <span class="smallcaps">ai</span> interno, utilizzato per esaminare le candidature di lavoro, continuava a favorire più candidati uomini rispetto alle donne. Il motivo? I creatori del sistema lo avevano addestrato su curriculum inviati all’azienda nei dieci anni precedenti, la maggior parte dei quali proveniva, appunto, da uomini. Il modello aveva imparato che i curriculum con attributi maschili erano più desiderabili. Ma Amazon non riuscì a correggere lo strumento (o non volle farlo) e si limitò a disattivarlo.</p>
			<p class="testo" id="p_0034">Google adottò un approccio altrettanto drastico dopo che il suo strumento di foto aveva etichettato alcuni individui neri come «gorilla», decidendo che l’app non dovesse più riconoscere i gorilla, pur continuando a identificare altri animali. L’errore iniziale, doloroso e offensivo, era nato dal fatto che Google non aveva addestrato il suo strumento con un numero sufficiente di immagini di persone nere e dalla pelle scura e che, probabilmente, non lo aveva testato a sufficienza nemmeno sui propri dipendenti. Alla fine del 2023, l’azienda non era ancora abbastanza sicura di riuscire a correggere il modello di <span class="smallcaps">ai</span>; di conseguenza, decise semplicemente di disabilitare la funzionalità.</p>
			<p class="testo" id="p_0035">Alcuni ricercatori di <span class="smallcaps">ai</span> sostengono che sia troppo difficile correggere questi bias, perché i modelli di <span class="smallcaps">ai</span> moderni sarebbero talmente complessi che nemmeno i loro creatori comprendono appieno il motivo per cui prendono certe decisioni. I modelli di <em class="calibre3">deep learning</em>, come le reti neurali, sono costituiti da milioni o miliardi di parametri, noti anche come «pesi», che agiscono come regolatori all’interno di funzioni matematiche complesse tra livelli connessi. Immaginate i livelli di una rete neurale come se fossero una fabbrica con una catena di montaggio in cui ogni operaio sulla linea ha un compito specifico, per esempio verniciare una macchinina o aggiungere le ruote. Alla fine della catena, il risultato è una macchinina completa. Ogni livello in una rete neurale è come una stazione della catena di montaggio: ciascuno di essi apporta una piccola modifica ai dati. Il problema è che, con così tanti microcambiamenti che avvengono in sequenza, è difficile risalire esattamente al contributo di ogni singola stazione sulla catena (o livello nella rete neurale) nel processo di creazione della macchinina (e, allo stesso modo, nel giungere alla decisione di etichettare un imputato di colore come ad alto rischio di recidiva).</p>
			<p class="testo" id="p_0036">Dopo che Google era finita sotto i riflettori per l’incidente dei gorilla, un’altra scienziata informatica, Margaret Mitchell, entrò a far parte del colosso della ricerca per tentare di prevenire simili errori. Nata a Los Angeles e ben nota tra i ricercatori di <span class="smallcaps">ai</span> per il suo lavoro sull’equità nell’apprendimento automatico, Mitchell si unì a un piccolo ma crescente movimento che mirava a una maggior cautela circa l’impatto reale dei sistemi di apprendimento automatico. Al pari di Gebru, era preoccupata per gli strani errori che i sistemi di <span class="smallcaps">ai</span> continuavano a commettere. Aveva condotto gran parte delle sue ricerche postdottorato in linguistica computazionale e poi nell’ambito della generazione del linguaggio naturale, studiando tutti i modi in cui i computer potevano descrivere oggetti o analizzare emozioni nei testi. In Microsoft, lavorando su un’app per non vedenti, rimase turbata quando il sistema descrisse un individuo caucasico come «persona», mentre un individuo con la pelle scura veniva identificato come «persona nera».</p>
			<p class="testo" id="p_0037">Un’altra volta, mentre conduceva alcuni esperimenti su una rete neurale che descriveva immagini, Mitchell le fornì alcune foto dell’esplosione avvenuta in una fabbrica in Inghilterra. Una di queste era stata scattata dall’alto, da un appartamento vicino, e mostrava colonne di fumo e, in primo piano, una <span class="smallcaps">tv</span> sintonizzata su un notiziario che riportava l’incidente. Mitchell rimase sbalordita quando il sistema di <span class="smallcaps">ai</span> descrisse l’immagine come «magnifica», «bellissima» e con una «vista straordinaria».</p>
			<p class="testo" id="p_0038">«Il sistema aveva un problema del tipo “è meraviglioso”», racconta Mitchell, riferendosi alla celebre canzone di <em class="calibre3">The Lego Movie</em>, film ambientato in un mondo di mattoncini dove tutte le difficoltà della vita vengono allegramente spazzate sotto il tappeto. «Non aveva nessuna nozione della mortalità né del fatto che la morte sia qualcosa di brutto.»</p>
			<p class="testo" id="p_0039">Quello che il sistema aveva effettivamente imparato dalle foto di addestramento era che i tramonti erano belli e che una posizione sopraelevata offriva una vista spettacolare. Fu allora che Mitchell ebbe un’illuminazione: i dati erano tutto. Creando lacune nei dati per addestrare il proprio sistema, lo aveva inconsapevolmente caricato con una serie di bias, compresi quelli che minimizzavano la perdita di vite umane.</p>
			<p class="testo" id="p_0040">Mentre affrontava queste problematiche in Google, Mitchell rilevò un altro aspetto frustrante del lavoro presso una grande azienda tecnologica: si sentiva intrappolata in una burocrazia soffocante, tra riunioni fiume e dirigenti perennemente in ansia per la reputazione dell’impresa.</p>
			<p class="testo" id="p_0041">Nel 2018, Mitchell inviò un’e-mail a Gebru per chiederle di affiancarla in Google. Il settore dell’etica dell’<span class="smallcaps">ai</span> era ancora così ristretto che le due si conoscevano già. Gebru sarebbe stata disposta a cogestire il team di ricerca etica sull’<span class="smallcaps">ai</span> di Google?</p>
			<p class="testo" id="p_0042">La ricercatrice informatica esitò. Le erano giunte voci di corridoio che descrivevano Google come un ambiente di lavoro tossico, in particolare per le donne e le minoranze. In questo senso il caso di Andy Rubin, dirigente di Google, era esemplare. Rubin era considerato una star all’interno della azienda, avendo cofondato il popolare sistema operativo Android, ma nel 2014 aveva lasciato la società in sordina in seguito ad accuse di cattiva condotta sessuale. Pochi anni dopo, un’inchiesta del «New York Times» avrebbe rivelato che la dirigenza di Google aveva esaminato le accuse e le aveva ritenute fondate. Eppure, invece di cacciarlo, Google gli aveva riservato un congedo da eroe, con tanto di buonuscita da 90 milioni di dollari.</p>
			<p class="testo" id="p_0043">A ogni modo, non tutto era da buttare. Gebru rimase colpita da come i dipendenti si facevano sentire e reagivano quando l’azienda si comportava in maniera sbagliata. In migliaia avevano organizzato uno sciopero generale per protestare contro il trattamento riservato a Rubin e, pochi mesi prima che lei entrasse in azienda, più di tremila dipendenti avevano firmato una lettera aperta al <span class="smallcaps">ceo</span> Sundar Pichai, per chiedere che Google si ritirasse dal Project Maven (cosa che l’azienda fece). Per di più, quelle proteste erano state coordinate da un’esperta di etica dell’<span class="smallcaps">ai</span>, una donna di nome Meredith Whittaker, la cui capacità di esporre con chiarezza il problema aveva costretto Google a riconsiderare il programma. Dopotutto, in un posto del genere Gebru avrebbe forse potuto promuovere pratiche più responsabili, come gli standard dettati dai <em class="calibre3">Datasheets for Datasets</em>.</p>
			<p class="testo" id="p_0044">Tuttavia, esaminando le dimensioni del suo nuovo team di etica, emergeva chiaramente che le priorità di investimento di giganti tecnologici come Google nell’<span class="smallcaps">ai</span> erano le sue capacità. Nonostante l’importanza del lavoro del suo team, questo era composto da appena una manciata di informatici. Nel resto dell’azienda, invece, migliaia di ingegneri e ricercatori continuavano a lavorare per rendere i sistemi di <span class="smallcaps">ai</span> più veloci e potenti, creando nuovi standard di capacità che Gebru e Mitchell esaminavano nelle loro conseguenze non volute.</p>
			<p class="testo" id="p_0045">Mitchell si sentiva logorata da Google. Quando, durante le riunioni, metteva in guardia i dirigenti su alcuni dei potenziali problemi che i loro sistemi di <span class="smallcaps">ai</span> avrebbero potuto causare, riceveva e-mail dal dipartimento delle Risorse umane che le consigliavano di mostrarsi più collaborativa. Nella Silicon Valley, nel 2020, le donne occupavano appena un quarto dei posti di lavoro nel settore informatico in aziende come Google, Apple e Facebook e guadagnavano ancora solo 86 centesimi per ogni dollaro percepito dagli uomini. Le donne sperimentavano spesso trattamenti diseguali, molestie e discriminazioni nelle assunzioni e nelle promozioni, e la situazione era particolarmente complicata per le donne nere. Molte delle donne che partecipavano alle conferenze o agli eventi informali della Silicon Valley lavoravano nel marketing o nelle pubbliche relazioni, piuttosto che nel reparto ingegneristico o nella ricerca. Erano dunque le donne a occuparsi più spesso di etica dell’<span class="smallcaps">ai</span>, sapendo per esperienza diretta cosa significasse la discriminazione. Ma questo le rendeva anche più vulnerabili quando c’era da farsi sentire con autorevolezza nelle discussioni.</p>
			<p class="testo" id="p_0046">Eppure, Mitchell rimase prima sorpresa e poi ammirata da Gebru e dall’audacia con cui non esitava a sfidare l’autorità quando aveva bisogno di risorse o vedeva qualcosa che non andava. Un giorno, sedute nell’ufficio di Gebru nel Building 41 del campus di Google, si ritrovarono a parlare di un’e-mail inquietante inviata da un dirigente, un messaggio che rifletteva la discriminazione vissuta da entrambe nell’azienda. Mitchell era sul punto di scoppiare in lacrime. Gebru, invece, guardava le cose da una prospettiva diversa.</p>
			<p class="testo" id="p_0047">«Non essere depressa», le disse Gebru. «Arrabbiati.»</p>
			<p class="testo" id="p_0048">Poi tirò a sé il laptop e cominciò a stendere una risposta al manager, leggendo ad alta voce mentre smontava punto per punto, e con precisione chirurgica, ogni questione sollevata dal loro responsabile. In seguito, quando sia Mitchell sia Gebru sarebbero state licenziate da Google, quel dirigente avrebbe preso pubblicamente le loro difese per poi dimettersi poco dopo.</p>
			<p class="testo" id="p_0049">Gebru e Mitchell stavano finalmente per guadagnare la giusta attenzione alla loro causa, anche a costo di essere cacciate dopo aver scatenato quello che sarebbe diventato uno scandalo pubblico. Ma stavano remando contro la priorità centrale di Google. Nel frattempo, il team ben più numeroso di scienziati incaricati di rendere l’<span class="smallcaps">ai</span> di Google più intelligente stava per compiere uno dei balzi in avanti più prodigiosi nella storia dell’<span class="smallcaps">ai</span>. Un vero e proprio miracolo.</p>
		</section>
	</body>
</html>
