<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:ibooks="http://apple.com/ibooks/html-extensions" lang="it-IT" xml:lang="it-IT" class="calibre">
  <head>
    <title>FILE 8 – Supremacy – Capitolo</title>
    <meta content="urn:uuid:35484d83-cd47-4b21-bd76-ea9d55abc8bb" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="../../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../../page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body epub:type="bodymatter" class="calibre2">
		<section epub:type="chapter" role="doc-chapter" class="calibre">
			<h2 class="capitolo" id="h2-0001"><span class="smallcaps1">6. la missione</span></h2>
			<p class="testo" id="p_0001">Era il 2015, e Demis Hassabis costruiva ormai il suo team da cinque anni, raggiungendo sempre nuovi traguardi nel suo percorso lento ma costante verso l’<span class="smallcaps">agi</span>, in un contesto in cui non c’era praticamente nessun altro a tentare la stessa impresa. L’obiettivo di DeepMind era così radicale da permetterle di operare effettivamente come un monopolio. Nessun’altra azienda consolidata al mondo stava cercando di costruire un’<span class="smallcaps">ai</span> che potesse superare l’intelligenza umana, perciò Hassabis poteva condurre la ricerca al suo ritmo. Questo, tra l’altro, rafforzava l’idea, tra fondatori e dipendenti, che DeepMind fosse un vero e proprio laboratorio di ricerca orientato alla missione, piuttosto che una semplice azienda. Potevano così venire a patti con il fatto di appartenere a Google, continuando a «risolvere l’intelligenza» (per poi affrontare i più grandi problemi dell’umanità) perché non correvano sulla stessa ruota per criceti a cui la competizione costringeva altre aziende. La loro missione era unica. Ora, l’ipotesi di un rivale nella Silicon Valley stava per cambiare tutto. La marcia verso l’<span class="smallcaps">agi</span> era destinata a trasformarsi in una competizione.</p>
			<p class="testo" id="p_0002">Più cose Hassabis apprendeva sul conto di Open<span class="smallcaps">ai</span>, più la sua rabbia montava. Era stato il primo a intraprendere seriamente il cammino verso la realizzazione di un’<span class="smallcaps">agi</span> e, considerata la marginalità della materia cinque anni prima, per farlo aveva messo a rischio la sua reputazione nel mondo scientifico. Come se ciò non bastasse, questo nuovo competitor, probabilmente, stava sfruttando le sue idee. Sul sito di Open<span class="smallcaps">ai</span> figuravano sette cofondatori: cinque di loro avevano lavorato come consulenti e tirocinanti presso DeepMind per diversi mesi. E questo mandò Hassabis su tutte le furie, dal momento che era stato un libro aperto con il personale di DeepMind riguardo alle diverse strategie da perseguire per raggiungere l’<span class="smallcaps">agi</span>, al modo in cui costruire agenti autonomi o insegnare ai modelli di <span class="smallcaps">ai</span> a competere in giochi come scacchi e Go. Ora, cinque scienziati che erano a conoscenza di tutti quei dettagli stavano avviando un’organizzazione concorrente.</p>
			<p class="testo" id="p_0003">Da un punto di vista tecnico, Hassabis non avrebbe dovuto preoccuparsi troppo. C’erano molti altri ricercatori al di fuori di DeepMind che stavano svolgendo un lavoro simile con agenti autonomi, ambienti virtuali e giochi. Uno di quei cinque ex collaboratori era un rinomato scienziato, Ilya Sutskever, specializzato nel <em class="calibre3">deep learning</em> e non nella tecnica distintiva di DeepMind, ovvero l’apprendimento per rinforzo. Sutskever era il direttore scientifico di Open<span class="smallcaps">ai</span> e, come i suoi cofondatori, credeva fermamente nelle possibilità dell’<span class="smallcaps">agi</span>.</p>
			<p class="testo" id="p_0004">Tuttavia, Hassabis fremeva per l’audacia di Sam Altman nel reclutare personale che conosceva i segreti di DeepMind, e l’ansia non gli dava tregua. In genere, Hassabis rientrava a casa per cenare con la famiglia, prima di iniziare la seconda parte della sua giornata lavorativa, in cui leggeva articoli di ricerca e inviava e-mail fino alle tre o alle quattro del mattino. In alcune di quelle e-mail o riunioni notturne, Hassabis espresse le proprie preoccupazioni, sostenendo che Altman copiasse la strategia di DeepMind e cercasse di sottrargli i ricercatori.</p>
			<p class="testo" id="p_0005">Hassabis nutriva inoltre qualche perplessità circa la promessa di Open<span class="smallcaps">ai</span> di rendere pubblica la propria tecnologia. Reputava sconsiderato quell’approccio «open». «Pensavo fosse un po’ ingenuo credere che l’open-source fosse una panacea», dice oggi. «Man mano che le tecnologie a duplice uso diventano sempre più potenti, che dire dei malintenzionati che potrebbero accedervi per scopi nefasti? […] Non ci sarebbe modo di controllare il fenomeno.» DeepMind pubblicava alcune delle sue ricerche su riviste scientifiche conosciute, ma manteneva il massimo riserbo sui dettagli del suo codice e della sua tecnologia <span class="smallcaps">ai</span>. Per esempio, non aveva divulgato i modelli di <span class="smallcaps">ai</span> creati per padroneggiare <em class="calibre3">Breakout</em>.</p>
			<p class="testo" id="p_0006">Circostanza ancora più umiliante, i leader di DeepMind vennero a sapere che Musk parlava male di Hassabis ai suoi contatti nella Silicon Valley, secondo quanto riferito da persone che lavoravano a DeepMind e Open<span class="smallcaps">ai</span>. Rivolgendosi al nuovo personale di Open<span class="smallcaps">ai</span>, per esempio, il miliardario metteva tutti in guardia circa l’operato di DeepMind in Inghilterra, insinuando che Hassabis fosse un personaggio poco raccomandabile. Gettava ombre sul modo in cui Hassabis aveva progettato <em class="calibre3">Evil Genius</em>, un gioco in cui bisognava vestire i panni di un cattivo intento a costruire un’arma apocalittica per dominare il mondo. L’ovvia implicazione era che chiunque creasse giochi del genere, probabilmente, aveva una vena maniacale. Traendo spunto dalla battuta, il team di Open<span class="smallcaps">ai</span> iniziò a creare meme basati sugli screenshot di <em class="calibre3">Evil Genius</em> per farli girare su Slack, la piattaforma di messaggistica interna. A un certo punto, secondo un ex elemento dello staff di Open<span class="smallcaps">ai</span> che sentì il commento con le proprie orecchie, Musk arrivò a definire Hassabis l’«Hitler dell’<span class="smallcaps">ai</span>».</p>
			<p class="testo" id="p_0007">Qualunque fosse il motivo del suo voltafaccia nei confronti di DeepMind, Musk stava alimentando quella che sarebbe diventata una rivalità feroce. Aveva anche iniziato a sviluppare una visione sull’<span class="smallcaps">ai</span> più paranoica e pessimista, in linea con la sua tendenza a estremizzare. Avrebbe potuto, per esempio, battersi semplicemente contro le compagnie petrolifere per far fronte al cambiamento climatico, invece di affannarsi a fare degli esseri umani una specie interplanetaria. Avrebbe potuto acquistare una quota di Twitter, nel momento in cui gli era parsa troppo woke, invece di rilevarla per intero. Forse era l’inclinazione di Musk a prendere misure drastiche, la sua tendenza a esagerare o la convinzione di essere il salvatore dell’umanità, ma un paio d’anni dopo l’investimento in DeepMind, il tycoon stava sprofondando nel dogma dell’<em class="calibre3"><span class="smallcaps">ai</span> doom</em>, il timore che l’intelligenza artificiale potesse portare a una sorta di apocalisse.</p>
			<p class="testo" id="p_0008">Secondo quanto riportato dal «New York Times», sosteneva lunghe conversazioni notturne sull’argomento con la moglie, preoccupato dal fatto che il cofondatore di Google Larry Page fosse sulla strada giusta per creare sistemi di <span class="smallcaps">ai</span> molto più avanzati dopo aver acquisito DeepMind. Musk e Page erano amici intimi. Frequentavano le stesse cene e le stesse conferenze esclusive, oltre a condividere sogni visionari sul futuro. Stando alla sua biografia scritta dal giornalista di Bloomberg Ashlee Vance, se Musk si trovava a San Francisco e non aveva pensato a un posto per dormire, chiamava Page e gli chiedeva di usare il suo divano. I due giocavano ai videogame e discutevano di aerei futuristici o altre tecnologie. Musk riteneva che Page, sempre più introverso, fosse quasi fin troppo gentile. E questo iniziava a preoccuparlo. Il cofondatore di Google avrebbe potuto creare per errore qualcosa di maligno, come una «flotta di robot potenziati dall’<span class="smallcaps">ai</span> in grado di annientare l’umanità». Quella di Musk sembrava una battuta, ma era ciò che effettivamente pensava.</p>
			<p class="testo" id="p_0009">Mesi dopo che Page aveva acquistato DeepMind per 650 milioni di dollari, Musk pubblicò un post riguardante l’<span class="smallcaps">ai</span> su un forum online per poi cancellarlo di lì a poco. Nessuno si rendeva conto di quanto rapidamente si stava evolvendo l’<span class="smallcaps">ai</span>, diceva il messaggio. «A meno che non abbiate esposizione diretta a gruppi come DeepMind, non potete averne idea.» Musk si diceva inoltre scettico sul fatto che le «principali aziende di <span class="smallcaps">ai</span>» potessero impedire che superintelligenze digitali sfuggissero su internet seminando il caos.</p>
			<p class="testo" id="p_0010">Man mano che sprofondava nel tunnel del catastrofismo legato all’<span class="smallcaps">ai</span>, Musk cominciò a investire sempre più soldi e tempo nella questione. Donò 10 milioni di dollari al Future of Life Institute, un’organizzazione senza scopo di lucro che promuoveva la ricerca per impedire l’annientamento dell’umanità da parte dell’<span class="smallcaps">ai</span>. Poi, insieme a Larry Page, Hassabis e quanti altri erano seriamente impegnati a costruire l’<span class="smallcaps">agi</span>, prese parte a una conferenza organizzata dallo stesso ente a Porto Rico.</p>
			<p class="testo" id="p_0011">Nel corso della conferenza, dopo una cena, Musk e Page ebbero un acceso diverbio. Mentre la discussione si infiammava, sempre più partecipanti si avvicinarono per ascoltare: Page disse che Musk stava diventando fin troppo paranoico riguardo all’<span class="smallcaps">ai</span>. Doveva tenere a mente che l’umanità stava evolvendo verso un’utopia digitale in cui le nostre menti sarebbero state a un tempo biologiche e digitali. Continuando a fare tutto quel baccano sull’<span class="smallcaps">ai</span>, avrebbe finito per rallentare il percorso verso il futuro.</p>
			<p class="testo" id="p_0012">«Ma come puoi essere così sicuro che una superintelligenza non spazzerà via l’umanità?» chiese Musk.</p>
			<p class="testo" id="p_0013">«Stai facendo dello specismo», pare abba replicato Page, almeno stando al racconto del «New York Times», ergendosi apparentemente a difesa dei postumani del futuro. Concentrandosi sulla catastrofe, Musk stava ignorando le esigenze di tutte quelle future entità destinate a nascere dal silicio.</p>
			<p class="testo" id="p_0014">Da un lato, mentre teneva sotto controllo DeepMind e si calava in una comunità di facoltosi visionari futuristi, Musk diventava sempre più radicalizzato. Dall’altro lato, però, era in piena crisi da <span class="smallcaps">fomo</span> (da <em class="calibre3">Fear of missing out</em>), la paralizzante «paura di rimanere tagliati fuori» che è alla base di alcune delle decisioni d’investimento più importanti nella Silicon Valley. Man mano che l’<span class="smallcaps">ai</span> raggiungeva nuove pietre miliari, come la vittoria nella competizione ImageNet del 2012, le grandi aziende tecnologiche cominciavano a prestare maggiore attenzione al settore. Non solo Google aveva acquistato DeepMind, ma Mark Zuckerberg aveva creato una nuova divisione chiamata Facebook <span class="smallcaps">ai</span> Research (<span class="smallcaps">fair</span>), affidandola a uno dei principali esperti mondiali di <em class="calibre3">deep learning</em>, Yann LeCun. Probabilmente, fu proprio il desiderio di partecipare a questa nuova corsa all’oro della ricerca a spingere Musk verso un esito controintuitivo rispetto alle sue paure: creare ancora più <span class="smallcaps">ai</span>.</p>
			<p class="testo" id="p_0015">In seguito, Musk avrebbe ammesso su Twitter di aver fondato Open<span class="smallcaps">ai</span> per fare da «contrappeso a Google» e perché desiderava che l’<span class="smallcaps">ai</span> venisse sviluppata in modo più sicuro. Ma non c’era dubbio che l’<span class="smallcaps">ai</span> fosse cruciale per il successo finanziario delle sue aziende, che si trattasse delle capacità di guida autonoma delle auto Tesla, dei sistemi di navigazione dei razzi senza equipaggio di SpaceX o dei modelli alla base della sua futura azienda di interfacce cervello-computer, Neuralink.</p>
			<p class="testo" id="p_0016">Per quanto le visioni apocalittiche rinsaldassero in Musk la convinzione morale di dover raggiungere l’<span class="smallcaps">agi</span> prima di Demis Hassabis, costruire un’<span class="smallcaps">ai</span> tanto avanzata quanto quella di Google avrebbe certamente dato impulso ai suoi affari. Si trattava, insomma, di un’impresa redditizia. E solo questo può spiegare perché accettò di lavorare al progetto con uno degli imprenditori più influenti della Silicon Valley: quel Sam Altman che trasformava «milioni» in «miliardi» con le sue presentazioni, che aveva riempito Y Combinator di start-up futuristiche e che riguardo all’<span class="smallcaps">ai</span> coltivava ambizioni pari a quelle di Larry Page.</p>
			<p class="testo" id="p_0017">In un’e-mail a Musk del 25 maggio 2015, Altman affermava che qualcuno doveva precedere Google nel realizzare l’<span class="smallcaps">agi</span>, proponendo un progetto strutturato in modo tale che la tecnologia diventasse di dominio pubblico. «Probabilmente vale la pena di parlarne», aveva risposto Musk. Un mese dopo, Altman inviò un’altra e-mail, suggerendo la creazione di un laboratorio per sviluppare «la prima <span class="smallcaps">agi</span>», dando «massima priorità» alla sicurezza. L’<span class="smallcaps">ai</span> sarebbe stata di proprietà di un’organizzazione non profit e utilizzata «a beneficio del mondo». Musk rispose: «Sono d’accordo su tutto».</p>
			<p class="testo" id="p_0018">Per Altman, costruire un sistema di <span class="smallcaps">ai</span> universale era come prendere tutte le start-up tecnologiche che aveva seguito in Y Combinator e inserirle in un unico, grande coltellino svizzero. Quest’<span class="smallcaps">ai</span> avanzata avrebbe potuto avere capacità illimitate. Chi poteva sapere se sarebbero ancora state necessarie aziende o start-up, nel momento in cui una nuova superintelligenza avrebbe potuto generare abbastanza ricchezza da garantire il benessere economico a tutti gli abitanti del pianeta? Se per Hassabis l’<span class="smallcaps">agi</span> avrebbe svelato i misteri della scienza e del divino, Altman la vedeva come la strada verso l’abbondanza materiale a livello planetario. Lui e Musk discussero dell’idea di avviare un laboratorio di ricerca che potesse realizzare questo ideale e fungere da contrappeso a DeepMind e Google.</p>
			<p class="testo" id="p_0019">Musk e Altman decisero di differenziare ulteriormente la loro nuova organizzazione dalle grandi aziende tecnologiche. Nel suo impegno per costruire un’<span class="smallcaps">ai</span> che andasse a beneficio dell’umanità, essa avrebbe collaborato con altri istituti e reso pubblica la sua ricerca. Da qui il nome: Open<span class="smallcaps">ai</span>.</p>
			<p class="testo" id="p_0020">Altman si mise al lavoro per costituire il team di fondatori. Nell’estate del 2015, invitò a cena una dozzina di ricercatori in una sala privata del Rosewood, un hotel di lusso a pochi passi da alcune delle più potenti società di venture capital della Silicon Valley. Tra gli invitati c’erano Ilya Sutskever, lo scienziato che aveva trascorso diversi mesi tra le fila di DeepMind, e Greg Brockman, originario del North Dakota e laureato in matematica a Harvard, già direttore tecnico di Stripe e un vero talento nell’avviare aziende.</p>
			<p class="testo" id="p_0021">Durante la cena, Altman spiegò che l’obiettivo di questa nuova organizzazione sarebbe stato costruire l’<span class="smallcaps">agi</span> e poi distribuirne i benefici al mondo. I convitati trascorsero gran parte della serata a chiedersi se fosse davvero possibile non tanto distribuire all’umanità le ricchezze generate dall’<span class="smallcaps">ai</span>, quanto avviare un laboratorio di questo tipo, considerando che le grandi aziende tecnologiche avevano già reclutato la maggior parte dei migliori talenti nel campo. Non era troppo tardi per cercare di assumere i ricercatori più brillanti del settore?</p>
			<p class="testo" id="p_0022">«Sapevamo [inoltre] che le nostre risorse sarebbero state insignificanti rispetto a quelle delle [grandi] aziende tecnologiche», ricordò in seguito Brockman nel podcast di Lex Fridman. E poi, se avessero fondato davvero un’organizzazione simile, come l’avrebbero strutturata per garantire che la sua <span class="smallcaps">ai</span> andasse a beneficio del genere umano? «Era chiaro che una tale organizzazione dovesse essere [senza scopo di lucro], senza incentivi che potessero comprometterne la missione.»</p>
			<p class="testo" id="p_0023">A metà strada del viaggio di ritorno in macchina con Altman, Brockman dichiarò che, per quanto il progetto fosse irrealistico, lui ci stava. Dopotutto, quella era la Silicon Valley, il luogo in cui anche le idee più folli trovavano il modo di prosperare.</p>
			<p class="testo" id="p_0024">Essendo lui stesso uno stacanovista, Altman rimase colpito da come Brockman iniziò subito a pianificare tutta la logistica necessaria per avviare Open<span class="smallcaps">ai</span>. Con un tempo medio di risposta alle e-mail pari a cinque minuti, poteva essere altrettanto dedito alla causa quanto lo era lui. «Era dentro da capo a piedi», disse in seguito Altman. Al momento di costruire Open<span class="smallcaps">ai</span>, Brockman si incaricò dell’organizzazione generale.</p>
			<p class="testo" id="p_0025">Brockman si addossò anche il compito di sottrarre un primo gruppo di scienziati di talento ad aziende come Google e Facebook. A tale scopo contattò Yoshua Bengio, un docente della University of Montreal etichettato come uno dei «padrini» del movimento del <em class="calibre3">deep learning</em>, non per assumerlo ma perché gli indicasse i ricercatori più promettenti nel campo dell’<span class="smallcaps">ai</span>. Bengio stilò un elenco di nomi e glielo inviò.</p>
			<p class="testo" id="p_0026">Assumere queste persone non sarebbe stato affatto semplice. Alcuni di loro guadagnavano stipendi a sette cifre con aziende come Google e Facebook, numeri a cui Altman e Brockman non potevano nemmeno avvicinarsi. Ciò che avevano dalla loro parte, però, era una missione entusiasmante che puntava a cambiare il mondo e due nomi prestigiosi a capo del progetto. Elon Musk era ormai un tycoon di fama mondiale, e dirigere Y Combinator aveva elevato lo status di Altman nella Silicon Valley a un livello tale che tutti volevano conoscerlo. Per i ricercatori nel campo dell’<span class="smallcaps">ai</span>, anche una breve parentesi in questa nuova realtà non profit offriva opportunità prestigiose e un possibile salto di carriera che poteva valere il sacrificio economico.</p>
			<p class="testo" id="p_0027">Numerosi dei principali scienziati nella lista di Brockman accettarono di incontrarlo per discutere del lavoro. Oltre ai grandi nomi e alla visione generale, apprezzavano l’impostazione «open» di questa nuova organizzazione. Avrebbero avuto finalmente la possibilità di <em class="calibre3">pubblicare</em> le loro ricerche, invece di lavorare in segreto su qualche prodotto aziendale, e alcuni di loro apprezzavano anche l’idea di contrastare la molla del profitto che motivava gli sforzi di Google e DeepMind verso la realizzazione dell’<span class="smallcaps">ai</span>.</p>
			<p class="testo" id="p_0028">Per finalizzare l’accordo, Brockman radunò alcuni scienziati in una cantina. Sutskever sarebbe stato il colpo più grosso di tutti, se avesse accettato di unirsi al team. Il gruppo parlò ancora della creazione di un laboratorio di <span class="smallcaps">ai</span> completamente libero dalle pressioni aziendali, tanto da rendere «open-source» la propria ricerca, offrendone i frutti gratuitamente, e di come ciò avrebbe impedito alle grandi aziende tecnologiche come Google e Facebook di esercitare un dominio assoluto sull’<span class="smallcaps">ai,</span> man mano che questa diventava sempre più avanzata. Quasi tutti gli scienziati aderirono al progetto, compreso Sutskever, il talentuoso informatico che sorrideva solo di rado. Cresciuto in Russia e in Israele, aveva lavorato con il prestigioso pioniere del <em class="calibre3">deep learning</em> Geoffrey Hinton. E ora avrebbe lasciato Google Brain per entrare a far parte di Open<span class="smallcaps">ai</span>.</p>
			<p class="testo" id="p_0029">Con una decina di persone a bordo, nel dicembre del 2015 il team si diresse a Montreal, in Canada, per una conferenza annuale di <span class="smallcaps">ai</span> chiamata <span class="smallcaps">nips</span> (ora Neur<span class="smallcaps">ips</span>), allo scopo di annunciare il nuovo laboratorio di ricerca. Fuori dalla sede dell’evento si accumulava la neve, mentre i membri del team parlavano del laboratorio con gli altri partecipanti alla conferenza. L’annuncio vero e proprio arrivò online. Un sito web, OpenAI.com, fece la sua comparsa con un post di presentazione firmato da Brockman e Sutskever: «Il nostro obiettivo è far progredire l’intelligenza digitale nel modo che più probabilmente avvantaggerà l’umanità nel suo complesso, senza vincoli legati alla necessità di generare ritorni finanziari».</p>
			<p class="testo" id="p_0030">Musk e Altman avrebbero presieduto l’organizzazione, che poteva contare su un impegno di finanziamento imponente, pari a un miliardo di dollari, da parte di Musk, Thiel, Altman, Hoffman e Jessica Livingston, insieme a crediti per il cloud computing da Amazon. Musk stava pianificando di finanziare Open<span class="smallcaps">ai</span> con azioni di Tesla, proprio come si era offerto di fare con DeepMind qualche anno prima.</p>
			<p class="testo" id="p_0031">I numerosi accademici presenti a Neur<span class="smallcaps">ips</span> rimasero basiti nell’apprendere la notizia. Molti pensavano che costruire l’<span class="smallcaps">agi</span> fosse un sogno irrealizzabile, ma alcuni erano anche invidiosi. Negli ultimi dieci anni, le grandi aziende tecnologiche avevano drenato le università dei loro migliori talenti informatici, e si stava arrivando al punto in cui le menti più brillanti nel campo dell’<span class="smallcaps">ai</span> lavoravano per interessi aziendali. Di fatto, ora c’era una catena di montaggio che partiva dalle università di élite e finiva in Google, Facebook e Amazon. Un problema ormai vecchio di anni.</p>
			<p class="testo" id="p_0032">«Non esiste che qualcuno possa dire no a due o tre volte il proprio stipendio», dice Maja Pantic, docente di informatica presso l’Imperial College di Londra passata nel 2018 a dirigere le ricerche del centro di <span class="smallcaps">ai</span> di Samsung Electronics per poi trasferirsi in Meta. «È ciò che è successo a me e a tutti i miei colleghi.» Una strada seguita anche da altri luminari. Hinton lavorava ormai per Google; Fei-Fei Li aveva lasciato Stanford per Google; LeCun era entrato in Facebook. Ng aveva lasciato Stanford per Google e poi per Baidu, in Cina. Anche le migliori università, come Stanford, Oxford e il <span class="smallcaps">mit</span>, faticavano a trattenere gli accademici di punta: di fatto, là dove sarebbe dovuta crescere la nuova generazione di educatori, c’era un vuoto. La ricerca sull’<span class="smallcaps">ai</span> era sempre più segreta e orientata al guadagno. Ecco perché l’impegno di Musk e Altman nel rendere la loro ricerca accessibile al pubblico rappresentava una ventata d’aria fresca per i ricercatori. Qualcuno, finalmente, stava facendo qualcosa per contrastare la concentrazione dei saperi sull’<span class="smallcaps">ai</span> nelle grandi aziende.</p>
			<p class="testo" id="p_0033">La fuga di cervelli dalle università avveniva per due motivi. Il primo e il più ovvio era di carattere economico. Alla University of Toronto, dove il «padrino dell’<span class="smallcaps">ai</span>» Geoffrey Hinton aveva insegnato, i docenti di informatica potevano aspettarsi di guadagnare circa 100.000 dollari l’anno. Gli accademici con i salari più alti arrivavano a circa 550.000 dollari l’anno. Quello era il massimo. L’allievo più brillante di Hinton, Sutskever, non aveva nemmeno provato a intraprendere la carriera universitaria. Dopo una parentesi nella start-up di Hinton, era passato direttamente a Google Brain. Quando Open<span class="smallcaps">ai</span> offrì a Sutskever 2 milioni di dollari l’anno, Google Brain triplicò l’offerta, secondo quanto riportato nel già citato <em class="calibre3">Costruire l’intelligenza</em>.</p>
			<p class="testo" id="p_0034">Una seconda ragione era legata alla quantità di dati e alla potenza di calcolo necessarie per condurre esperimenti nella ricerca sull’<span class="smallcaps">ai</span>. Le università, in genere, dispongono di un numero limitato di <span class="smallcaps">gpu</span>, ovvero unità di elaborazione grafica, i potenti semiconduttori prodotti da Nvidia che oggi alimentano la maggior parte dei server utilizzati per addestrare i modelli di <span class="smallcaps">ai</span>. Quando lavorava nel mondo accademico, Pantic era riuscita ad acquistare solo sedici <span class="smallcaps">gpu</span> per l’intero gruppo di trenta ricercatori. Con così pochi chip, addestrare un modello di <span class="smallcaps">ai</span> richiedeva mesi. «Era ridicolo», dice. Non molto tempo dopo essere passata a Samsung, ottenne l’accesso a duemila <span class="smallcaps">gpu</span>. Tutta quella potenza di calcolo in più permetteva di addestrare un algoritmo in giorni, anziché in mesi, e la ricerca poteva così procedere molto più rapidamente.</p>
			<p class="testo" id="p_0035">Per quegli scienziati che restavano nel mondo accademico, diventava sempre più difficile sottrarsi alle Big Tech. Uno studio del 2022 ha rilevato che, nel decennio precedente, il numero di articoli accademici legati alle grandi aziende tecnologiche era più che triplicato, arrivando al 66 per cento. Secondo gli autori dello studio, condotto da ricercatori di diverse università, tra cui la Stanford e lo University College di Dublino, la loro crescente presenza «ricorda da vicino le strategie utilizzate dalle grandi aziende del tabacco». Questo, a sua volta, influenzava il modo in cui le università misuravano il successo della ricerca sull’<span class="smallcaps">ai</span>. Secondo Abeba Birhane, che ha guidato lo studio e ora è ricercatrice senior presso la Mozilla Foundation, invece di puntare su valori come il benessere delle persone, la giustizia e l’inclusione, gli accademici tendevano a concentrarsi sulle performance migliori.</p>
			<p class="testo" id="p_0036">Il benessere e l’inclusione non erano concetti vaghi, afferma la Birhane. Erano perfettamente misurabili. «Possono sembrare astratti, ma lo sono anche efficienza e prestazioni», aggiunge. «Le persone hanno trovato modi per misurare equità, privacy e altro ancora.» Ad aggravare ulteriormente la situazione c’era il fatto che, mentre i ricercatori, dalle università alle aziende tecnologiche, puntavano via via a rendere i loro modelli di <span class="smallcaps">ai</span> sempre più grandi e capaci, cresceva anche il rischio che questi modelli potessero talvolta produrre esiti razzisti o sessisti, spiega Birhane, citando un altro studio del 2023 di cui è coautrice. «Quello che abbiamo scoperto è che con un dataset più ampio crescono anche i contenuti d’odio.»</p>
			<p class="testo" id="p_0037">Eppure, la capacità computazionale era cruciale per il crescente potere che le grandi aziende tecnologiche stavano accumulando nel campo dell’<span class="smallcaps">ai</span>. Google e Meta, l’azienda precedentemente nota come Facebook, disponevano di migliaia di miliardi di dati utili per addestrare i modelli, e gestivano server farm che occupavano centinaia di migliaia di metri quadrati. Un singolo data center attualmente gestito da Google a Dalles, in Oregon, per esempio, è più grande di sei campi da football. La maggior parte delle università può offrire solo una frazione infinitesimale di tali risorse.</p>
			<p class="testo" id="p_0038">Quando l’obiettivo era affinare l’<span class="smallcaps">ai</span>, <em class="calibre3">più</em> significava anche <em class="calibre3">meglio</em>. Nell’avviare la ricerca presso Open<span class="smallcaps">ai</span>, Sutskever e il suo team si concentrarono sulla creazione di modelli di <span class="smallcaps">ai</span> il più possibile capaci, non necessariamente equi, giusti o rispettosi della privacy. In termini molto semplici, esisteva una formula per ottenere il risultato. Se addestravi un modello di <span class="smallcaps">ai</span> con un numero sempre maggiore di dati, aumentando i parametri a disposizione del modello e la potenza di calcolo impiegata per l’addestramento, il modello di <span class="smallcaps">ai</span> diventava più efficiente. Era la stessa straordinaria correlazione osservata dal professor Andrew Ng nei suoi esperimenti a Stanford. Non importava quale fosse l’obiettivo per cui era stato progettato il modello: incrementando tutti i parametri, sarebbe diventato più preciso nella traduzione delle lingue e avrebbe generato testi paragonabili a quelli di un essere umano.</p>
			<p class="testo" id="p_0039">«Se avete un dataset e una rete neurale molto ampi, il successo è garantito», disse Sutskever durante una conferenza sull’<span class="smallcaps">ai</span>. Le ultime quattro parole di quest’affermazione divennero il suo motto tra gli scienziati dell’<span class="smallcaps">ai</span>, tanto più dopo il grande lancio di Open<span class="smallcaps">ai</span>, quando il settore rinvigorì il proprio entusiasmo grazie a questa nuova organizzazione senza scopo di lucro, guidata da un brillante scienziato e da alcuni dei magnati più potenti della Silicon Valley.</p>
			<p class="testo" id="p_0040">Non passò molto tempo prima che iniziassero a sorgere alcuni problemi. Open<span class="smallcaps">ai</span> non ottenne subito il miliardo di dollari in finanziamenti annunciato a dicembre da Musk, Thiel e altri. In realtà, di lì a qualche anno, l’organizzazione senza scopo di lucro riuscì a raccogliere solo poco più di 130 milioni di dollari in donazioni effettive, secondo un’inchiesta del sito di notizie tecnologiche TechCrunch, che ha esaminato le dichiarazioni fiscali federali di Open<span class="smallcaps">ai</span>.</p>
			<p class="testo" id="p_0041">Open<span class="smallcaps">ai</span> mancava di fondi e aveva preso una direzione poco chiara. Il team fondatore, composto da trenta ricercatori, iniziò a lavorare nell’appartamento di Brockman nel quartiere Mission di San Francisco, riunendosi intorno al tavolo della cucina o sui divani con i portatili sulle ginocchia. Pochi mesi dopo il lancio, ricevettero la visita di un altro stimato ricercatore di Google Brain, Dario Amodei. Quest’ultimo iniziò a porre domande mirate. Cos’era questa storia di voler realizzare un’<span class="smallcaps">ai</span> «friendly» e rilasciarne pubblicamente il codice sorgente? Secondo quanto riportato nel già citato profilo di Altman sul «New Yorker», egli rispose che non avevano intenzione di rilasciare tutto il codice sorgente.</p>
			<p class="testo" id="p_0042">«Ma qual è l’obiettivo?» chiese Amodei.</p>
			<p class="testo" id="p_0043">«È un po’ vago», ammise Brockman. Il loro intento era garantire che l’<span class="smallcaps">agi</span> venisse sviluppata nella maniera corretta.</p>
			<p class="testo" id="p_0044">Amodei faceva parte del numero crescente di scienziati che condividevano i timori apocalittici di Musk e Yudkowsky. Lavorava in Google quando, meno di un anno prima, l’azienda era finita nell’occhio del ciclone dopo che si era scoperto come il sistema di riconoscimento visivo di Google Photos classificasse i soggetti di colore come gorilla. Dichiarandosi «inorridita», Google aveva rimosso completamente l’etichetta di gorilla dall’applicazione. «Avere sistemi che falliscono in maniera imprevedibile non è una buona cosa», avrebbe ammesso in un podcast, riferendosi all’incidente.</p>
			<p class="testo" id="p_0045">Ma le preoccupazioni di Amodei non si limitavano alle decisioni razziste e offensive degli algoritmi. Era anche in ansia per come l’apprendimento per rinforzo, la tecnica di <span class="smallcaps">ai</span> che DeepMind stava perfezionando, veniva utilizzato per controllare sistemi fisici quali robot, auto a guida autonoma e data center di Google. «Una volta che inizi a interfacciarti direttamente con il mondo e a controllare cose fisiche reali, ritengo che il rischio che qualcosa vada storto […] aumenti in maniera significativa», disse in un’intervista rilasciata nel 2016 al Future of Life Institute di Jaan Tallinn.</p>
			<p class="testo" id="p_0046">La ricerca sui potenziali danni dell’<span class="smallcaps">ai</span> lo portò a contemplare scenari sempre più catastrofici, tanto che nel 2023 avrebbe avvertito i media che c’era una probabilità del 25 per cento che un’<span class="smallcaps">ai</span> fuori controllo rappresentasse un rischio di estinzione per gli esseri umani. Google Brain non era il posto giusto per trovare soluzioni che riducessero quei rischi. Pochi mesi dopo quella conversazione negli uffici di Open<span class="smallcaps">ai</span>, Amodei si unì al team.</p>
			<p class="testo" id="p_0047">Per costruire l’<span class="smallcaps">agi</span>, il team fondatore di Open<span class="smallcaps">ai</span> aveva bisogno di attrarre più soldi e talenti; di conseguenza, cercò di concentrarsi su progetti che potessero generare resoconti positivi sui media. I primi ricercatori crearono un computer in grado di battere i migliori campioni umani a <em class="calibre3">Dota</em>, un videogioco strategico in 3D, e costruirono anche una mano robotica a cinque dita, alimentata da una rete neurale, capace di risolvere un cubo di Rubik. Questi progetti avevano lo scopo di tenere alto il morale di Elon Musk cercando di sopravanzare il lavoro svolto dall’altra parte dell’Atlantico nelle stanze segrete di DeepMind.</p>
			<p class="testo" id="p_0048">Musk non faceva mistero della sua sfiducia nei confronti di DeepMind. Nel 2017, il personale di Open<span class="smallcaps">ai</span> partecipò a un incontro presso la sede di SpaceX. Musk, che all’inizio visitava gli uffici di Open<span class="smallcaps">ai</span> ogni settimana e poi a intervalli di due settimane, accompagnò i visitatori in giro per la struttura e poi tenne una sessione di domande e risposte con circa quaranta dei suoi nuovi ricercatori nel campo dell’<span class="smallcaps">ai</span>. A un certo punto, iniziò a parlare del motivo per cui aveva finanziato Open<span class="smallcaps">ai</span>, e il motivo aveva un nome: Demis Hassabis.</p>
			<p class="testo" id="p_0049">«Ero uno degli investitori di DeepMind e mi preoccupava vedere che Larry [Page] era convinto che Demis lavorasse per lui. In realtà, Demis lavora solo per sé stesso», disse Musk. «E non mi fido di Demis.»</p>
			<p class="testo" id="p_0050">I ricercatori rimasero sbalorditi. A molti di loro sembrò che Musk avesse un conto in sospeso con Hassabis, più che una preoccupazione particolare riguardo alla direzione che stava prendendo l’<span class="smallcaps">ai</span>. Quando gli chiesero del suo antagonismo nei confronti di Hassabis, Musk citò i videogiochi che l’imprenditore britannico aveva progettato in passato, incentrati sulla conquista del mondo.</p>
			<p class="testo" id="p_0051">Durante la stessa sessione, Musk riferì di una conversazione avuta con un altro investitore di DeepMind, il quale, parlando di un incontro precedente con Hassabis, aveva detto: «Mi è sembrato di trovarmi in quel punto dei film in cui qualcuno deve alzarsi e sparare al tizio che ha davanti». In altre parole, secondo lui, qualcuno doveva fermare Hassabis prima che realizzasse un’<span class="smallcaps">agi</span> onnipotente.</p>
			<p class="testo" id="p_0052">Per quanto non sembrasse apprezzare Hassabis, Musk continuava a ricordare al personale di Open<span class="smallcaps">ai</span> che DeepMind era in vantaggio, presentando la ricerca dell’azienda britannica come punto di riferimento da raggiungere. Con il passare dei mesi, in Musk crebbe sempre di più la preoccupazione per il fatto che la tecnologia di Open<span class="smallcaps">ai</span> non fosse potente come quella di DeepMind.</p>
			<p class="testo" id="p_0053">Pur di mantenere a bordo il principale finanziatore, Altman e Brockman spinsero alcuni dei ricercatori a emulare il lavoro svolto da DeepMind. I ricercatori del progetto <em class="calibre3">Dota</em>, per esempio, non capivano perché dovessero lavorare su una simulazione di videogioco quando l’obiettivo finale era realizzare un’<span class="smallcaps">agi</span> che migliorasse la vita delle persone. Il motivo era che avevano bisogno dei soldi di Musk. «Se non lavoriamo su questo, tra qualche anno o persino l’anno prossimo Open<span class="smallcaps">ai</span> potrebbe cessare di esistere», spiegò loro Brockman.</p>
			<p class="testo" id="p_0054">Sebbene alla fine Open<span class="smallcaps">ai</span> abbia raggiunto una fama mondiale per il lavoro sui chatbot e i modelli di linguaggio di grandi dimensioni, nei primi anni si concentrò su simulazioni multi-agente e apprendimento per rinforzo, ambiti in cui DeepMind era già leader. Ma più inseguivano DeepMind in quei settori, più Altman e il suo team di dirigenti si rendevano conto che questi approcci all’<span class="smallcaps">ai</span> non promettevano chissà quale impatto sul mondo reale. Fu allora che Open<span class="smallcaps">ai</span> iniziò a evolvere in un tipo di organizzazione molto diversa da DeepMind. Se quest’ultima, infatti, aveva una cultura gerarchica e accademica che valorizzava gli elementi in possesso di un dottorato, la cultura di Open<span class="smallcaps">ai</span> era più orientata all’ingegneria. Molti dei suoi ricercatori di punta erano programmatori, hacker ed ex fondatori di start-up passati da Y Combinator. Erano più interessati a costruire cose e fare soldi che a raggiungere posizioni di prestigio in seno alla comunità scientifica in virtù di chissà quali scoperte.</p>
			<p class="testo" id="p_0055">Nel frattempo, Musk diventava sempre più impaziente. Si lamentò con Altman del fatto che, pur avendo reclutato una squadra incredibile di scienziati, Open<span class="smallcaps">ai</span> non aveva ancora mostrato qualcosa in cui potesse surclassare DeepMind. Ormai vicina al suo terzo anno di vita, l’organizzazione, secondo Musk, rimaneva ancora troppo indietro rispetto a Google e DeepMind. Così, propose una soluzione rapida: avrebbe preso il controllo di Open<span class="smallcaps">ai</span> e l’avrebbe fusa con Tesla. Open<span class="smallcaps">ai</span> non avrebbe mai colmato il gap con DeepMind senza un cambiamento radicale, spiegò Musk in un’e-mail del dicembre 2018 ad Altman e al suo team (e-mail poi resa pubblica da Open<span class="smallcaps">ai</span> e confermata da qualcuno che aveva letto la versione non censurata). «Purtroppo, il futuro dell’umanità è nelle mani di Demis», aggiunse. In altre parole, se lui non avesse preso in mano la situazione, Hassabis, il cattivo della storia, avrebbe avuto la meglio. Ma Altman e i suoi cofondatori volevano mantenere il controllo, pertanto rifiutarono la proposta di Musk.</p>
			<p class="testo" id="p_0056">Nel febbraio del 2018, in un annuncio pubblico sui nuovi donatori, Open<span class="smallcaps">ai</span> accennò brevemente all’uscita di Musk, presentandola però sotto una luce positiva. Musk, infatti, se ne andava per motivi etici: aveva un insanabile conflitto di interessi nel campo dell’<span class="smallcaps">ai</span>. «Elon Musk lascerà il consiglio di Open<span class="smallcaps">ai</span> ma continuerà a contribuire economicamente e a offrire consulenza all’organizzazione», si leggeva sul blog. «Dal momento che Tesla è sempre più focalizzata sull’<span class="smallcaps">ai</span>, questa decisione eliminerà un potenziale conflitto futuro per Elon.»</p>
			<p class="testo" id="p_0057">Molti elementi del team di Open<span class="smallcaps">ai</span> sapevano che era una sciocchezza. Sospettavano che, per quanto Musk sbandierasse il proprio interesse nella creazione di un’<span class="smallcaps">ai</span> il più possibile sicura, il suo obiettivo era anche quello di essere il primo a costruire l’<span class="smallcaps">ai</span> più avanzata. Era già l’uomo più ricco del pianeta e stava acquisendo un’influenza senza precedenti sulle infrastrutture americane: la <span class="smallcaps">nasa</span> mandava astronauti nello spazio grazie a SpaceX; Tesla guidava la rivoluzione sugli standard delle auto elettriche; e l’azienda di sua proprietà che si occupava di internet satellitare, Starlink, stava cercando di influenzare l’esito della guerra in Ucraina.</p>
			<p class="testo" id="p_0058">Era evidente che non si poteva fare troppo affidamento su Musk. Aveva promesso di donare un miliardo di dollari a Open<span class="smallcaps">ai</span> nel corso di diversi anni, ma alla fine aveva versato tra i 50 e i 100 milioni di dollari – una cifra irrisoria per l’uomo più ricco tra quanti avevano in qualche modo a cuore la questione dell’<span class="smallcaps">ai</span>. Donare quella somma sarebbe stato relativamente facile, soprattutto se avesse finanziato Open<span class="smallcaps">ai</span> con azioni Tesla. Tra il 2015 e il 2023 il valore delle azioni Tesla era aumentato di oltre il 18.000 per cento, perciò Open<span class="smallcaps">ai</span> avrebbe potuto raggiungere l’obiettivo del miliardo di dollari senza troppe difficoltà. A dispetto di tutti i timori professati per il futuro dell’umanità, Musk sembrava molto più interessato al proprio vantaggio sui competitor.</p>
			<p class="testo" id="p_0059">Insieme a Musk, Open<span class="smallcaps">ai</span> perse anche la sua principale fonte di finanziamento. E per Altman, che su quel progetto aveva puntato tutta la sua reputazione, fu un disastro. Alcuni dei migliori scienziati nel campo dell’<span class="smallcaps">ai</span> avevano accettato un compenso ridotto pur di lavorare con lui, e le sue pompose promesse di aiutare l’umanità cominciavano a suonare ridicole. Questa nuova era dello sviluppo dell’<span class="smallcaps">ai</span> presentava una pura e semplice verità: per avere successo servivano più risorse su ogni fronte, dal denaro con cui pagare i ricercatori ai dati per allenare i modelli, fino ai computer potenti per farli funzionare. Senza Musk, la possibilità di spuntare tutte queste caselle stava rapidamente svanendo.</p>
			<p class="testo" id="p_0060">Altman si trovò di fronte a un bivio cruciale. Lavorando dall’ufficio di Open<span class="smallcaps">ai</span> a San Francisco, rifletteva su come mantenere in vita la non profit con risorse estremamente limitate e sviluppare modelli di <span class="smallcaps">ai</span> che, probabilmente, sarebbero stati inferiori a quelli della concorrenza. In alternativa bisognava gettare la spugna e chiudere il progetto. Raccogliere fondi per una non profit era molto più difficile che per una start-up. Altman faticava a convincere persone facoltose a fare donazioni per la causa dell’<span class="smallcaps">agi</span> per pura generosità, senza la possibilità di un ritorno finanziario diretto. Aveva bisogno di decine di milioni di dollari, e Musk era stato il suo ultimo grande benefattore.</p>
			<p class="testo" id="p_0061">C’era un’altra opzione. Forse Open<span class="smallcaps">ai</span> avrebbe potuto offrire ai suoi sostenitori qualche tipo di beneficio economico diretto, oltre all’onore di contribuire alla nascita di un’utopia legata a un’<span class="smallcaps">ai</span> che andasse a vantaggio dell’umanità. Sarebbe stato un win-win. Più che una «donazione», i finanziatori avrebbero fatto un «investimento», secondo un linguaggio con cui Altman si sentiva più a suo agio. Tuttavia, vedeva solo pochi potenziali finanziatori a cui potersi rivolgere nella speranza di ottenere sia il denaro sia la potenza di calcolo necessaria per sviluppare l’<span class="smallcaps">agi</span>. E questi erano i colossi tecnologici come Google, Amazon, Facebook e Microsoft. Nessun altro disponeva di miliardi di dollari pronti all’uso o di supercomputer ospitati in edifici grandi quanto svariati campi da football.</p>
			<p class="testo" id="p_0062">Negli ultimi anni, sia Open<span class="smallcaps">ai</span> sia DeepMind avevano cercato di mettere in atto misure atte a impedire che un qualsiasi sistema di <span class="smallcaps">ai</span> ultrapotente da loro realizzato trovasse un utilizzo improprio. DeepMind stava tentando di modificare la propria struttura di governance per evitare che un monopolio motivato dal profitto, come Google, avesse carta bianca nel monetizzare l’<span class="smallcaps">agi</span>. Al contrario, un comitato di esperti avrebbe dovuto tenere sotto controllo la situazione. Altman e Musk avevano fondato Open<span class="smallcaps">ai</span> come una non profit, promettendo di condividere le ricerche e persino i brevetti con altre istituzioni, nel caso in cui fossero giunti alla soglia delle macchine superintelligenti. In quel modo, la priorità sarebbe rimasta il bene del genere umano.</p>
			<p class="testo" id="p_0063">Ora che si trovava costretto a lottare per la sopravvivenza, Altman avrebbe abbattuto alcuni di quei paletti. L’approccio cauto degli esordi si sarebbe fatto più spregiudicato, cambiando radicalmente il campo dell’<span class="smallcaps">ai</span> su cui lui e DeepMind stavano lavorando. E così, da impresa ponderata e per lo più accademica si sarebbe trasformata in una specie di Far West. Altman avrebbe sfruttato la sua capacità di imbastire una narrativa convincente per giustificare l’allontanamento dai principi fondatori di Open<span class="smallcaps">ai</span>. Dopotutto, era un imprenditore di start-up, e gli imprenditori di start-up, a volte, dovevano cambiare rotta con manovre drastiche. Così funzionava nella Silicon Valley. Avrebbe dovuto solo <em class="calibre3">ritoccare</em> alcuni di quei principi fondatori. Ma giusto un po’.</p>
		</section>
	</body>
</html>
