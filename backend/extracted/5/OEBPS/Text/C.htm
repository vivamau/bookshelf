<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xml:lang="en"
      lang="en"
      xmlns="http://www.w3.org/1999/xhtml"
      xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<title>TensorFlow in Action</title>
<link rel="stylesheet" type="text/css" href="../../override_v1.css"/>
<link rel="stylesheet" type="text/css" href="../../stylesheet.css"/><link rel="stylesheet" type="text/css" href="../../page_styles.css"/>
</head>
<body>
<div id="book-content">
<div id="sbo-rt-content" class="calibre"><h1 class="tochead"><a id="pgfId-1055492"></a>appendix C Natural language processing</h1>

  <h2 class="fm-head" id="sigil_toc_id_223"><a id="pgfId-1055493"></a>C.1 Touring around the zoo: Meeting other Transformer models</h2>

  <p class="body"><a class="calibre8" id="pgfId-1055497"></a>In chapter 13<a class="calibre8" id="marker-1055494"></a>, we<a class="calibre8" id="marker-1055495"></a><a class="calibre8" id="marker-1055496"></a> discussed a powerful Transformer-based model known as BERT (bidirectional encoder representations from Transformers<a class="calibre8" id="marker-1055498"></a>). But BERT was just the beginning of a wave of Transformer models. These models grew stronger and better, either by solving theoretical issues with BERT or re-engineering various aspects of the model to perform faster and better. Let’s understand some of these popular models to learn what sets them apart from BERT.</p>

  <h3 class="fm-head1" id="sigil_toc_id_224"><a id="pgfId-1055499"></a>C.1.1 Generative pre-training (GPT) model (2018)</h3>

  <p class="body"><a class="calibre8" id="pgfId-1055503"></a>The<a class="calibre8" id="marker-1055500"></a><a class="calibre8" id="marker-1055501"></a><a class="calibre8" id="marker-1055502"></a> story actually starts even before BERT. OpenAI introduced a model called GPT in the paper “Improving Language Understanding by Generative Pre-Training” by Radford et al. (<span class="fm-hyperlink"><a class="url" href="http://mng.bz/1oXV">http://mng.bz/1oXV</a></span>). It is trained in a similar fashion to BERT, pretraining on a large corpus of text followed by fine-tuning on a discriminative task. The GPT model is a <i class="fm-italics">Transformer decoder</i><a class="calibre8" id="marker-1055504"></a> compared to BERT, which is a <i class="fm-italics">Transformer encoder</i><a class="calibre8" id="marker-1055505"></a>. The difference is that the GPT model has left-to-right (or causal) attention, whereas BERT has bidirectional (i.e., left-to-right and right-to-left) attention used when computing self-attention outputs. In other words, the GPU model pays attention only to the words to the left of it as it computes the self-attention output of a given word. This is the same as the masked attention component we discussed in chapter 5. Due to this, GPT is also called an <i class="fm-italics">autoregressive model</i><a class="calibre8" id="marker-1055506"></a>, whereas BERT is called an <i class="fm-italics">autoencoder</i>. In addition, unlike BERT, adapting GPT to different tasks like sequence classification, token classification, or question answering requires slight architectural changes, which is cumbersome. GPT has three versions (GPT-1, GPT-2, and GPT-3); each model became bigger while introducing slight changes to improve<a class="calibre8" id="marker-1055508"></a><a class="calibre8" id="marker-1055509"></a><a class="calibre8" id="marker-1055510"></a> performance.</p>

  <p class="fm-callout"><a id="pgfId-1055512"></a><span class="fm-callout-head">NOTE</span> OpenAI, TensorFlow: <span class="fm-hyperlink"><a class="url" href="https://github.com/openai/gpt-2">https://github.com/openai/gpt-2</a></span>.</p>

  <h3 class="fm-head1" id="sigil_toc_id_225"><a id="pgfId-1055513"></a>C.1.2 DistilBERT (2019)</h3>

  <p class="body"><a class="calibre8" id="pgfId-1055518"></a>Following<a class="calibre8" id="marker-1057809"></a><a class="calibre8" id="marker-1057810"></a><a class="calibre8" id="marker-1057811"></a> BERT, DistilBERT is a model introduced by Hugging Face in the paper “DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter” by Sanh et al. (<span class="fm-hyperlink"><a class="url" href="https://arxiv.org/pdf/1910.01108v4.pdf">https://arxiv.org/pdf/1910.01108v4.pdf</a></span>) in 2019. The primary focus of DistilBERT is to compress BERT while keeping the performance similar. It is trained using a transfer learning technique known as <i class="fm-italics">knowledge distillation</i><a class="calibre8" id="marker-1057815"></a> (<span class="fm-hyperlink"><a class="url" href="http://mng.bz/qYV2">http://mng.bz/qYV2</a></span>). The idea is to have a teacher model (i.e., BERT), and a smaller model (i.e., DistilBERT) that tries to mimic the teacher’s output. The DistilBERT model is smaller compared to BERT and only has 6 layers, as opposed to BERT, which has 12 layers. The DistilBERT model is initialized with the initialization of every other layer of BERT (because DistilBERT has exactly half the layers of BERT). Another key difference of DistilBERT is that it is only trained on the masked language modeling task and not on the next-sentence prediction<a class="calibre8" id="marker-1057817"></a><a class="calibre8" id="marker-1057818"></a><a class="calibre8" id="marker-1057819"></a> task.</p>

  <p class="fm-callout"><a id="pgfId-1055526"></a><span class="fm-callout-head">NOTE</span> Hugging Face’s Transformers: <span class="fm-hyperlink"><a class="url" href="https://huggingface.co/transformers/model_doc/distilbert.html">https://huggingface.co/transformers/model_doc/distilbert.html</a></span>.</p>

  <h3 class="fm-head1" id="sigil_toc_id_226"><a id="pgfId-1055527"></a>C.1.3 RoBERT/ToBERT (2019)</h3>

  <p class="body"><a class="calibre8" id="pgfId-1055532"></a>RoBERT<a class="calibre8" id="marker-1055528"></a><a class="calibre8" id="marker-1055529"></a><a class="calibre8" id="marker-1055530"></a><a class="calibre8" id="marker-1055531"></a> (recurrence over BERT) and ToBERT (Transformer over BERT) are two models introduced in the paper “Hierarchical Transformer Models for Long Document Classification” by Pappagari et al. (<span class="fm-hyperlink"><a class="url" href="https://arxiv.org/pdf/1910.10781.pdf">https://arxiv.org/pdf/1910.10781.pdf</a></span>). The main problem addressed by this paper is the inability or the performance degradation experienced for long text sequences (e.g., call transcripts) in BERT. This is because the self-attention layer has a computational complexity of O(n<sup class="fm-superscript">2</sup>) for a sequence of length n. The solution proposed in these models is to factorize long sequences to smaller segments of length k (with overlap) and feed each segment to BERT to generate the pooled output (i.e., the output of the <span class="fm-code-in-text">[CLS]</span> token) or the posterior probabilities (from a task-specific classification layer). Next, stack the outputs returned by BERT for each segment and pass them on to a recurrent model like LSTM (RoBERT) or a smaller Transformer<a class="calibre8" id="marker-1055534"></a><a class="calibre8" id="marker-1055535"></a><a class="calibre8" id="marker-1055536"></a><a class="calibre8" id="marker-1055537"></a> (ToBERT).</p>

  <p class="fm-callout"><a id="pgfId-1055539"></a><span class="fm-callout-head">NOTE</span> Hugging Face’s Transformers: <span class="fm-hyperlink"><a class="url" href="https://huggingface.co/transformers/model_doc/roberta.html">https://huggingface.co/transformers/model_doc/roberta.html</a></span>.</p>

  <h3 class="fm-head1" id="sigil_toc_id_227"><a id="pgfId-1055540"></a>C.1.4 BART (2019)</h3>

  <p class="body"><a class="calibre8" id="pgfId-1055545"></a>BART<a class="calibre8" id="marker-1055541"></a><a class="calibre8" id="marker-1055542"></a><a class="calibre8" id="marker-1055543"></a><a class="calibre8" id="marker-1055544"></a> (bidirectional and auto-regressive Transformers), proposed in “BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension” by Lewis et al. (<span class="fm-hyperlink"><a class="url" href="https://arxiv.org/pdf/1910.13461.pdf">https://arxiv.org/pdf/1910.13461.pdf</a></span>) is a sequence-to-sequence model. We discussed sequence-to-sequence models in chapters 11 and 12, and BART draws on the same concepts. BART has an encoder and a decoder. If you remember from chapter 5, the Transformer model also has an encoder and a decoder and can be thought of as a sequence-to-sequence model. The encoder of a Transformer has bidirectional attention, whereas the decoder of a Transformer has left-to-right attention (i.e., is autoregressive).</p>

  <p class="body"><a class="calibre8" id="pgfId-1055548"></a>Unlike the vanilla Transformer model, BART uses several innovative pre-training techniques (document reconstruction) to pretrain the model. Particularly, BART is trained as a denoising autoencoder, where a noisy input is provided and the model needs to reconstruct the true input. In this case, the input is a document (a collection of sentences). The documents are corrupted using the methods listed in table C.1.</p>

  <p class="fm-table-caption"><a id="pgfId-1056977"></a>Table C.1 Various methods employed in corrupting documents. The true document is “I was hungry. I went to the café.” The “_” character denotes the mask token.</p>

  <table border="1" class="contenttable" width="100%">
    <colgroup class="calibre12">
      <col class="calibre13" span="1" width="20%"/>
      <col class="calibre13" span="1" width="60%"/>
      <col class="calibre13" span="1" width="20%"/>
    </colgroup>

    <tr class="calibre14">
      <th class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1056983"></a><b class="fm-bold">Method</b></p>
      </th>

      <th class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1056985"></a><b class="fm-bold">Description</b></p>
      </th>

      <th class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1056987"></a><b class="fm-bold">Example</b></p>
      </th>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1056989"></a>Token masking</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1056991"></a>Tokens in the sentences are randomly masked.</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1056993"></a>I was _ . I _ to the cafe</p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1056995"></a>Token deletion</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1056997"></a>Tokens are randomly deleted.</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1056999"></a>I hungry . I went to café</p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1057001"></a>Sentence permutation</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1057003"></a>Change the order of the sentences.</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1057005"></a>I went to the café . I was hungry</p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1057007"></a>Document rotation</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1057009"></a>Rotate the document so that the starting and ending of the document changes.</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1057011"></a>Café . I was hungry . I went to the</p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1057013"></a>Text infilling</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1057015"></a>Instead of a single token, mask spans tokens with a single mask token. A 0 length span would insert the mask token.</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1057017"></a>I was _ hungry . I _ the café</p>
      </td>
    </tr>
  </table>

  <p class="body"><a class="calibre8" id="pgfId-1055589"></a>With the corruption logic, we generate inputs to BART, and the target will be the true document without corruptions. Initially, the corrupted document is input to the encoder, and then the decoder is asked to recursively predict the true sequence while using the previously predicted output(s) as the next input. This is similar to how we predicted translations using a machine translation model in chapter 11.</p>

  <p class="body"><a class="calibre8" id="pgfId-1055590"></a>Once the model is pretrained, you can use BART for any of the NLP tasks that Transformer models are typically used for. For example, BART can be used for sequence classification tasks (e.g., sentiment analysis) in the following way:</p>

  <ol class="calibre11">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1055591"></a>Input the token sequence (e.g., movie review) to both the encoder and the decoder.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1055592"></a>Add a special token (e.g., <span class="fm-code-in-text">[CLS]</span>) to the end of the sequence when feeding the decoder input. We added the special token to the beginning of the sequence when working with BERT.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1055593"></a>Get the hidden representation output for the special token by the decoder and feed that to a downstream classifier that will predict the final output (e.g., positive/negative prediction).</p>
    </li>
  </ol>

  <p class="body"><a class="calibre8" id="pgfId-1055598"></a>To use BART for sequence-to-sequence problems (e.g., machine translation),<a class="calibre8" id="marker-1055594"></a><a class="calibre8" id="marker-1055595"></a><a class="calibre8" id="marker-1055596"></a><a class="calibre8" id="marker-1055597"></a> follow these steps:</p>

  <ol class="calibre11">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1055599"></a>Input the source sequence to the encoder.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1055600"></a>Add a starting (e.g., <span class="fm-code-in-text">[SOS]</span>) and ending (e.g., <span class="fm-code-in-text">[EOS]</span>) special token to the start and end of the target sequence, respectively.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1055601"></a>During training, train the decoder with all tokens in the target sequence except the last as the input and all tokens but the first as the target (i.e., teacher forcing).</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1055602"></a>During inference, provide the starting token as the first input to the decoder and recursively predict the next output while using the previous output(s) as the input (i.e., autoregressive),</p>
    </li>
  </ol>

  <p class="fm-callout"><a id="pgfId-1055603"></a><span class="fm-callout-head">NOTE</span> Hugging Face’s Transformers: <span class="fm-hyperlink"><a class="url" href="http://mng.bz/7ygy">http://mng.bz/7ygy</a></span>.</p>

  <h3 class="fm-head1" id="sigil_toc_id_228"><a id="pgfId-1055604"></a>C.1.5 XLNet (2020)</h3>

  <p class="body"><a class="calibre8" id="pgfId-1055608"></a>XLNet<a class="calibre8" id="marker-1055605"></a><a class="calibre8" id="marker-1055606"></a><a class="calibre8" id="marker-1055607"></a> was introduced in the paper “XLNet: Generalized Autoregressive Pretraining for Language Understanding” by Yang et al. (<span class="fm-hyperlink"><a class="url" href="https://arxiv.org/pdf/1906.08237.pdf">https://arxiv.org/pdf/1906.08237.pdf</a></span>) in early 2020. Its primary focus was to capture the best of both worlds in autoencoder-based models (e.g., BERT) and autoregressive models (e.g., GPT). For this discourse, it is important to understand the advantages and drawbacks of the two approaches.</p>

  <p class="body"><a class="calibre8" id="pgfId-1055611"></a>A key advantage that BERT has as an autoencoder model is that the task-specific classification head has hidden representations of tokens that are enriched by bidirectional context, as it can pay attention to both sides of a given token. And as you can imagine, knowing what comes before as well as after for a given token yield results in better downstream tasks. Conversely, GPT pays attention to only the left side of a given word to generate the representation. Therefore, GPT’s token representations are suboptimal in the sense that they only pay unidirectional attention (left side) to the token.</p>

  <p class="body"><a class="calibre8" id="pgfId-1055613"></a>On the other hand, the pretraining methodology of BERT involves introducing the special token <span class="fm-code-in-text">[MASK]</span>. Though this token appears in the pretraining context, it never appears in the fine-tuning context, creating a pretraining fine-tuning discrepancy.</p>

  <p class="body"><a class="calibre8" id="pgfId-1055614"></a>There’s a more critical issue that is lurking in BERT. BERT formulates the language modeling under the assumption that masked tokens are separately constructed (i.e., independence assumption). In other words, if you have the sentence “I love <span class="fm-code-in-text">[MASK]</span><sub class="fm-subscript">1</sub> <span class="fm-code-in-text">[MASK]</span><sub class="fm-subscript">2</sub> city,” the second mask token is generated with no attention to what was chosen for the <span class="fm-code-in-text">[MASK]</span><sub class="fm-subscript">1</sub> token. This is wrong because to generate a valid city name, you must know the value of <span class="fm-code-in-text">[MASK]</span><sub class="fm-subscript">1</sub> before generating <span class="fm-code-in-text">[MASK]</span><sub class="fm-subscript">2</sub>. However, the autoregressive nature of GPT allows the model to first predict the value for <span class="fm-code-in-text">[MASK]</span><sub class="fm-subscript">1</sub> and then use that along with other words to its left to generate the value for <span class="fm-code-in-text">[MASK]</span><sub class="fm-subscript">2</sub> about the first word in the city before generating the second word (i.e., context aware).</p>

  <p class="body"><a class="calibre8" id="pgfId-1055615"></a>XLNet blends these two language modeling approaches into one so that you have the bidirectional context coming from the approach used in BERT and the context awareness from GPT’s approach. The new approach is called <i class="fm-italics">permutation language modeling</i><a class="calibre8" id="marker-1055616"></a>. The idea is as follows. Consider a T elements-long sequence of words. There are T! permutations for that sequence. For example, the sentence “Bob loves cats” will have 3! = 3 × 2 × 1 = 6 permutations:</p>
  <pre class="programlisting">Bob loves cats
Bob cats loves
loves Bob cats
loves cats Bob
cats Bob loves
cats loves Bob</pre>

  <p class="body"><a class="calibre8" id="pgfId-1055625"></a>If the parameters of the language model used to learn this are shared across all the permutations, not only can we use an autoregressive approach to learn it, but we can also capture information from both sides of the text for a given word. This is the main idea explored in the<a class="calibre8" id="marker-1055626"></a><a class="calibre8" id="marker-1055627"></a><a class="calibre8" id="marker-1055628"></a> paper.</p>

  <p class="fm-callout"><a id="pgfId-1055629"></a><span class="fm-callout-head">NOTE</span> Hugging Face’s Transformers: <span class="fm-hyperlink"><a class="url" href="http://mng.bz/mOl2">http://mng.bz/mOl2</a></span>.</p>

  <h3 class="fm-head1" id="sigil_toc_id_229"><a id="pgfId-1055630"></a>C.1.6 Albert (2020)</h3>

  <p class="body"><a class="calibre8" id="pgfId-1055634"></a>Albert<a class="calibre8" id="marker-1055631"></a><a class="calibre8" id="marker-1055632"></a><a class="calibre8" id="marker-1055633"></a> is a variant BERT model that delivers competitive performance to BERT with fewer parameters. Albert makes two important contributions: reducing the model size and introducing a new self-supervised loss that helps the model capture language better.</p>

  <p class="fm-head2"><a id="pgfId-1055635"></a>Factorization of the embedding layer</p>

  <p class="body"><a class="calibre8" id="pgfId-1055640"></a>First<a class="calibre8" id="marker-1055637"></a><a class="calibre8" id="marker-1055638"></a><a class="calibre8" id="marker-1055639"></a>, Albert factorizes the embedding matrix used in BERT. In BERT the embeddings are metrics of size V × H, where V is the vocabulary size and H is the hidden state size. In other words, there is a tight coupling between the embedding size (i.e., length of an embedding vector) and the final hidden representation size. However, the embeddings (e.g., WordPiece embeddings in BERT) are not designed to capture context, whereas hidden states are computed taking both the token and its context into account. Therefore, it makes sense to have a large hidden state size H, as the hidden state captures a more informative representation of a token than embeddings. But doing so increases the size of the embedding matrix due to the tight coupling present. Therefore, Albert suggests factorizing the embedding matrix to two matrices, V × E and E × H, decoupling the embedding size and the hidden state size. With this design, one can increase the hidden state size while keeping the embedding size<a class="calibre8" id="marker-1055643"></a><a class="calibre8" id="marker-1055644"></a><a class="calibre8" id="marker-1055645"></a> small.</p>

  <p class="fm-head2"><a id="pgfId-1055646"></a>Cross-layer parameter sharing</p>

  <p class="body"><a class="calibre8" id="pgfId-1055650"></a>Cross-layer<a class="calibre8" id="marker-1055649"></a> parameter sharing is another technique introduced in Albert to reduce the parameter space. Since all layers in BERT (as well the Transformer model in general) have uniform layers from top to bottom, parameter sharing is trivial. Parameter sharing can happen in one of the following three ways:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1055651"></a>Across all self-attention sublayers</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1055652"></a>Across all the fully connected sublayers</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1055653"></a>Across self-attention and fully connected sublayers (separately)</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1055654"></a>Albert uses sharing all parameters across layers as the default strategy. By using this strategy, Albert achieves a 71%-86% parameter reduction without compromising the performance of the model<a class="calibre8" id="marker-1055657"></a> significantly.</p>

  <p class="fm-head2"><a id="pgfId-1055658"></a>Sentence-order prediction instead of next sentence prediction</p>

  <p class="body"><a class="calibre8" id="pgfId-1055663"></a>Finally<a class="calibre8" id="marker-1055660"></a><a class="calibre8" id="marker-1055661"></a><a class="calibre8" id="marker-1055662"></a>, the authors of the paper argue that the value added by the next-sentence prediction pretraining task in BERT is doubtful, which is supported by several previous studies. Therefore, they introduce a new, more challenging model that focuses primarily on language coherence: sentence-order prediction. In this, the model is trained with a binary classification head to predict whether a given pair of sentences are in the correct order. The data can be easily generated, where positive samples are taken as sentences next to each other in that order, and negative samples are generated by swapping two adjacent sentences. The authors argue that this is more challenging than next-sentence prediction, leading to a more informed model<a class="calibre8" id="marker-1055665"></a><a class="calibre8" id="marker-1055666"></a><a class="calibre8" id="marker-1055667"></a> than<a class="calibre8" id="marker-1055668"></a><a class="calibre8" id="marker-1055669"></a><a class="calibre8" id="marker-1055670"></a> BERT.</p>

  <p class="fm-callout"><a id="pgfId-1055671"></a><span class="fm-callout-head">NOTE</span> TFHub: (<span class="fm-hyperlink"><a class="url" href="https://tfhub.dev/google/albert_base/3">https://tfhub.dev/google/albert_base/3</a></span>). Hugging Face’s Transformers: (<span class="fm-hyperlink"><a class="url" href="http://mng.bz/5QM1">http://mng.bz/5QM1</a></span>).</p>

  <h3 class="fm-head1" id="sigil_toc_id_230"><a id="pgfId-1055673"></a>C.1.7 Reformer (2020)</h3>

  <p class="body"><a class="calibre8" id="pgfId-1055677"></a>The<a class="calibre8" id="marker-1057756"></a><a class="calibre8" id="marker-1057757"></a><a class="calibre8" id="marker-1057758"></a> Reformer was one of the latest to join the family of Transformers. The main idea behind the Reformer is its ability to scale to sequences that are several tens of thousands of tokens long. The Reformer was introduced in the paper “Reformer: The Efficient Transformer” by Kitaev et al. (<span class="fm-hyperlink"><a class="url" href="https://arxiv.org/pdf/2001.04451.pdf">https://arxiv.org/pdf/2001.04451.pdf</a></span>) in early 2020<a class="calibre8" id="marker-1059035"></a>.</p>

  <p class="body"><a class="calibre8" id="pgfId-1055678"></a>The main limitation of the vanilla Transformers that prevents them from being used for long sequences is the computational complexity of the self-attention layer. It needs to look at every other word for every word to generate the final representation, which has a O(L<sup class="fm-superscript">2</sup>) complexity for a sequence that has L tokens. The reformer uses locality-sensitive hashing<a class="calibre8" id="marker-1055679"></a> (LSH) to reduce this complexity to O(L logL). The idea of LSH is to assign every input a hash; the inputs having the same hash are considered similar and assigned to the same bucket. With that, similar inputs are placed in one bucket. To do that, we have to introduce several modifications to the self-attention sublayer.</p>

  <p class="fm-head2"><a id="pgfId-1055680"></a>Locality-sensitive hashing in the self-attention layer</p>

  <p class="body"><a class="calibre8" id="pgfId-1055683"></a>First<a class="calibre8" id="marker-1055681"></a><a class="calibre8" id="marker-1055682"></a>, we need to make sure the Q and K matrices are identical. This is a necessity as the idea is to compute similarity between queries and keys. This is easily done by sharing the weights between Q and K weight matrices. Next, a hashing function needs to be developed, which can generate a hash for a given query/key so that similar queries/keys (shared-qk) get similar hashes. Also remember that this must be done in a differentiable way to ensure end-to-end training of the model. The following hashing function is used</p>
  <pre class="programlisting">h(x) = argmax([xR; - xR])</pre>

  <p class="body"><a class="calibre8" id="pgfId-1055685"></a>where <span class="fm-code-in-text">R</span> is a random matrix of size <span class="fm-code-in-text">[d_model, b/2]</span> for a user-defined <span class="fm-code-in-text">b</span> (i.e., number of buckets) and <span class="fm-code-in-text">x</span> is the input of shape <span class="fm-code-in-text">[b, L, d_model]</span>. By using this hashing function, you get a bucket ID for each input token in a batch in a given position. To learn more about this technique, refer to the original paper “Practical and Optimal LSH for Angular Distance” by Andoni et al. (<span class="fm-hyperlink"><a class="url" href="https://arxiv.org/pdf/1509.02897.pdf">https://arxiv.org/pdf/1509.02897.pdf</a></span>). According to the bucket ID, the shared-qk items are sorted.</p>

  <p class="body"><a class="calibre8" id="pgfId-1055686"></a>Then the sorted shared-qk items are chunked using a fixed chunk size. A larger chunk size means more computations (i.e., more words are considered for a given token), whereas a smaller chunk size can mean underperformance (i.e., not enough tokens to look at).</p>

  <p class="body"><a class="calibre8" id="pgfId-1055687"></a>Finally, the self-attention is computed as follows. For a given token, look in the same chunk that it’s in as well as the previous chunk and attend to the words with the same bucket ID in both of those chunks. This will produce the self-attention output for all the tokens provided in the input. This way, the model does not have to look at every other word for every token and can focus on a subset of words or tokens for a given token. This makes the model scalable to sequences several tens of<a class="calibre8" id="marker-1055688"></a><a class="calibre8" id="marker-1055689"></a> thousands<a class="calibre8" id="marker-1055690"></a><a class="calibre8" id="marker-1055691"></a><a class="calibre8" id="marker-1055692"></a> of<a class="calibre8" id="marker-1055693"></a><a class="calibre8" id="marker-1055694"></a> tokens<a class="calibre8" id="marker-1055695"></a> long.</p>

  <p class="fm-callout"><a id="pgfId-1055696"></a><span class="fm-callout-head">NOTE</span> Hugging Face’s Transformers: <span class="fm-hyperlink"><a class="url" href="http://mng.bz/6XaD">http://mng.bz/6XaD</a></span>.</p>
</div>
</div>
</body>
</html>