<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xml:lang="en"
      lang="en"
      xmlns="http://www.w3.org/1999/xhtml"
      xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<title>TensorFlow in Action</title>
<link rel="stylesheet" type="text/css" href="../../override_v1.css"/>
<link rel="stylesheet" type="text/css" href="../../stylesheet.css"/><link rel="stylesheet" type="text/css" href="../../page_styles.css"/>
</head>
<body>
<div id="book-content">
<div id="sbo-rt-content" class="calibre"><h1 class="tochead"><a id="pgfId-1123194"></a>9 Natural language processing with TensorFlow: Sentiment analysis</h1>

  <p class="co-summary-head"><a id="pgfId-1123197"></a>This chapter<a id="marker-1125515"></a><a id="marker-1125516"></a> covers</p>

  <ul class="calibre9">
    <li class="co-summary-bullet"><a class="calibre8" id="pgfId-1123198"></a>Preprocessing text with Python</li>

    <li class="co-summary-bullet"><a class="calibre8" id="pgfId-1123199"></a>Analyzing text-specific attributes important for the model</li>

    <li class="co-summary-bullet"><a class="calibre8" id="pgfId-1123200"></a>Creating a data pipeline to handle text sequences with TensorFlow</li>

    <li class="co-summary-bullet"><a class="calibre8" id="pgfId-1123202"></a><a class="calibre8" id="aHlk60153502"></a>Analyzing sentiments with a recurrent deep learning model (LSTM)</li>

    <li class="co-summary-bullet"><a class="calibre8" id="pgfId-1123203"></a>Training the model on imbalanced product reviews</li>

    <li class="co-summary-bullet"><a class="calibre8" id="pgfId-1123204"></a>Implementing word embeddings to improve model performance</li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1123205"></a>In the previous chapters, we looked at two compute-vision-related applications: image classification and image segmentation. Image classification focuses on recognizing if an object belonging to a certain class is present in an image. Image segmentation tasks look not only at recognizing the objects present in the image, but also which pixels in the image belong to a certain object. We also anchored our discussions around learning the backbone of complex convolutional neural networks such as Inception net (image classification) and DeepLab v3 (image segmentation) models. If we look beyond images, text data is also a prominent modality of data. For example, the world wide web is teeming with text data. We can safely assume that it is the most common modality of data available in the web. Therefore, natural language processing (NLP) has been and will be a deeply rooted topic, enabling us to harness the power of the freely available text (e.g., through language modeling) and build machine learning products that can leverage textual data to produce meaningful outcomes (e.g., sentiment analysis).</p>

  <p class="body"><a class="calibre8" id="pgfId-1123206"></a><i class="fm-italics">NLP</i> is a term that we give to the overarching notion that houses a plethora of tasks having to do with text. Everything from simple tasks, such as changing the case of text (e.g., converting uppercase to lowercase), to complex tasks, such as translating languages and word sense disambiguation (inferring the meaning of a word with the same spelling depending on the context) falls under the umbrella of NLP. Following are some of the notable tasks that you will experience if you enter the realm of NLP:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1123208"></a><i class="fm-italics">Stop word removal</i><a class="calibre8" id="marker-1123207"></a>—Stop words are uninformative words that frequent text corpora (e.g., “and,” “it,” “the,” “am,” etc.). Typically, these words add very little or nothing to the semantics (or meaning) of text. To reduce the feature space, many tasks remove stop words in early stages before feeding text to the model.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123210"></a><i class="fm-italics">Lemmatization</i><a class="calibre8" id="marker-1123209"></a>—This is another technique to reduce the feature space the model has to deal with. Lemmatization will convert a given word to its base form (e.g., buses to bus, walked to walk, went to go, etc.), which reduces the size of the vocabulary and, in turn, the dimensionality of data the model needs to learn from.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123213"></a><i class="fm-italics">Part of speech (PoS) tagging</i><a class="calibre8" id="marker-1123211"></a><a class="calibre8" id="marker-1123212"></a>—PoS tagging does exactly what it says: it tags every word in a given text with a part of speech (e.g., noun, verb, adjective, etc.). The Penn Treebank project provides one of the most popular and comprehensive list of PoS tags available. To see the full list, go to <span class="fm-hyperlink"><a class="url" href="http://mng.bz/mO1W">http://mng.bz/mO1W</a></span>.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123215"></a><i class="fm-italics">Named entity recognition (NER</i><a class="calibre8" id="marker-1123214"></a><i class="fm-italics">)</i>—NER is responsible for extracting various entities (e.g., Names of people/companies, geo locations, etc.) from text.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123217"></a><i class="fm-italics">Language modeling</i><a class="calibre8" id="marker-1123216"></a>—Language modeling is the task of predicting the <i class="fm-italics">n</i><sup class="fm-superscript">th</sup> word given 1, . . . , <i class="fm-italics">w</i> -1<sup class="fm-superscript">th</sup> word. Language modeling can be used to generate songs, movie scripts, and stories by training the model on relevant data. Due to the highly accessible nature of the data needed for language modeling (i.e., it does not require any labeled data), it commonly serves as a pretraining method to inject language understanding for decision-support NLP models.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123218"></a><i class="fm-italics">Sentiment analysis</i>—Sentiment analysis is the task of identifying the sentiment given a piece of text. For example, a sentiment analyzer can analyze product/ movie reviews and automatically produce a score to indicate how good the product is.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1123220"></a><i class="fm-italics">Machine translation</i><a class="calibre8" id="marker-1123219"></a>—Machine translation is the task of converting a phrase/ sentence in a source language to a phrase/sentence in a target language. These models are trained using bilingual parallel text corpora.</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1123221"></a>It is rare that you will not come across an NLP task as a data scientist or a ML researcher. To solve NLP tasks quickly and successfully, it is important to understand processing data, standard models used, and so on.</p>

  <p class="body"><a class="calibre8" id="pgfId-1123222"></a>In this chapter, you will learn how to develop a sentiment analyzer for video game review classification. You will start by exploring the data and learn about some common NLP preprocessing steps. You will also note that the data set is imbalanced (i.e., does not have a roughly equal number of samples for all the classes) and learn what can be done about that. We will then develop a TensorFlow data pipeline with which we will pipe data to our model to train. Here, you’ll encounter a new machine learning model known as a <i class="fm-italics">long short-term memory</i> (LSTM) model that has made its mark in the NLP domain. LSTM models can process sequences (e.g., a sentence—a sequence of words in a particular order) by going through each element iteratively to produce some outcome at the end. In this task, the model will output a binary value (0 for negative reviews and 1 for positive reviews). While traversing the sequence, an LSTM model maintains the memory of what it has seen so far. This makes LSTM models very powerful and able to process long sequences and learn patterns in them. After the model is trained, we will evaluate it on some test data to make sure it performs well and then save it. The high-level steps we will follow to develop this model include the following:</p>

  <ol class="calibre11">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1123223"></a>Download the data. We will use a video game review corpus from Amazon.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123224"></a>Explore and clean the data. Incorporate some text cleaning steps (e.g., lemmatization) to clean the data and prepare the corpus for the modeling task.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123225"></a>Create a data pipeline to convert raw text to a numerical format understood by machine learning models.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123226"></a>Train the model on the data produced by the data pipeline.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123227"></a>Evaluate the model on validation, and test data to ensure model’s generalizability.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1123228"></a>Save the trained model, and write the performance results.</p>
    </li>
  </ol>

  <h2 class="fm-head" id="sigil_toc_id_118"><a id="pgfId-1123229"></a>9.1 What the text? Exploring and processing text</h2>

  <p class="body"><a class="calibre8" id="pgfId-1123233"></a>You<a class="calibre8" id="marker-1123231"></a><a class="calibre8" id="marker-1123232"></a> are building a sentiment analyzer for a popular online video game store. They want a bit more than the number of stars, as the number of stars might not reflect the sentiment accurately due to the subjectivity of what a star means. The executives believe the text is more valuable than the number of stars. You’ve been asked to develop a sentiment analyzer that can determine how positive or negative a review is, given the text.</p>

  <p class="body"><a class="calibre8" id="pgfId-1123234"></a>You have decided to use an Amazon video game review data set for this. It contains various reviews posted by users along with the number of stars. Text can be very noisy due to the complexity of language, spelling mistakes, and so on. Therefore, some type of preprocessing will act as the gatekeeper for producing clean data. In this section, we will examine the data and some basic statistics. Then we will perform several preprocessing steps: comprising, lowering the case (e.g., convert “John” to “john”), removing punctuation/numbers, removing stop words (i.e., uninformative words like “to,” “the,” “a,” etc.) and lemmatization (converting words to their base form; e.g., “walking” to “walk”).</p>

  <p class="body"><a class="calibre8" id="pgfId-1123235"></a>As the very first step, let’s download the data set in the next listing.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1123237"></a>Listing 9.1 Downloading the Amazon review data set</p>
  <pre class="programlisting">import os
import requests
import gzip
import shutil
 
# Retrieve the data
if not os.path.exists(os.path.join('data','Video_Games_5.json.gz')):      <span class="fm-combinumeral">❶</span>
    url = 
<span class="fm-code-continuation-arrow">➥</span> "http:/ /deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Video_Games_
<span class="fm-code-continuation-arrow">➥</span> 5.json.gz"
    # Get the file from web
    r = requests.get(url)
 
    if not os.path.exists('data'):
        os.mkdir('data')
    
    # Write to a file
    with open(os.path.join('data','Video_Games_5.json.gz'), 'wb') as f:
        f.write(r.content)
else:                                                                     <span class="fm-combinumeral">❷</span>
    print("The tar file already exists.")
    
if not os.path.exists(os.path.join('data', 'Video_Games_5.json')):        <span class="fm-combinumeral">❸</span>
    with gzip.open(os.path.join('data','Video_Games_5.json.gz'), 'rb') as f_in:
        with open(os.path.join('data','Video_Games_5.json'), 'wb') as f_out:
            shutil.copyfileobj(f_in, f_out)
else:
    print("The extracted data already exists")</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1141302"></a><span class="fm-combinumeral">❶</span> If the gzip file has not been downloaded, download it and save it to the disk.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1141330"></a><span class="fm-combinumeral">❷</span> If the gzip file is located in the local disk, don’t download it.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1141347"></a><span class="fm-combinumeral">❸</span> If the gzip file exists but has not been extracted, extract it.</p>

  <p class="body"><a class="calibre8" id="pgfId-1123267"></a>This code will download the data to a local folder if it doesn’t already exist and extract the content. It will have a JSON file that will contain the data. JSON is a format for representing data and is predominately used to transfer data in web requests. It allows us to define data as key-value pairs. If you look at the JSON file, you will see that it has one record per line, where each record is a set of key-value pairs, and key is the column name and value is the value of that column for that record. You can see a few records extracted from the data:</p>
  <pre class="programlisting">{"overall": 5.0, "verified": true, "reviewTime": "10 17, 2015", 
<span class="fm-code-continuation-arrow">➥</span> "reviewerID": "xxx", "asin": "0700026657", "reviewerName": "xxx", 
<span class="fm-code-continuation-arrow">➥</span> "reviewText": "This game is a bit hard to get the hang of, but when you 
<span class="fm-code-continuation-arrow">➥</span> do it's great.", "summary": "but when you do it's great.", 
<span class="fm-code-continuation-arrow">➥</span> "unixReviewTime": 1445040000}
{"overall": 4.0, "verified": false, "reviewTime": "07 27, 2015", 
<span class="fm-code-continuation-arrow">➥</span> "reviewerID": "xxx", "asin": "0700026657", "reviewerName": "xxx", 
<span class="fm-code-continuation-arrow">➥</span> "reviewText": "I played it a while but it was alright. The steam was a 
<span class="fm-code-continuation-arrow">➥</span> bit of trouble. The more they move ... looking forward to anno 2205 I 
<span class="fm-code-continuation-arrow">➥</span> really want to play my way to the moon.", "summary": "But in spite of 
<span class="fm-code-continuation-arrow">➥</span> that it was fun, I liked it", "unixReviewTime": 1437955200}
{"overall": 3.0, "verified": true, "reviewTime": "02 23, 2015", 
<span class="fm-code-continuation-arrow">➥</span> "reviewerID": "xxx", "asin": "0700026657", "reviewerName": "xxx", 
<span class="fm-code-continuation-arrow">➥</span> "reviewText": "ok game.", "summary": "Three Stars", "unixReviewTime": 
<span class="fm-code-continuation-arrow">➥</span> 1424649600}</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123271"></a>Next, we will further explore the data we have:</p>
  <pre class="programlisting">import pandas as pd
 
# Read the JSON file
review_df = pd.read_json(
    os.path.join('data', 'Video_Games_5.json'), lines=True, orient='records'
)
# Select on the columns we're interested in 
review_df = review_df[["overall", "verified", "reviewTime", "reviewText"]]
review_df.head()</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123282"></a>The data is in JSON format. pandas provides a <span class="fm-code-in-text">pd.read_json()</span> function<a class="calibre8" id="marker-1123281"></a> to read JSON data easily. When reading JSON data, you have to make sure that you set the orient argument correctly. This is because the orient argument enables pandas to understand the structure of JSON data. JSON data is unstructured compared to CSV files, which have a more consistent structure. Setting <span class="fm-code-in-text">orient='records'</span> will enable pandas to read data structured in this way (one record per line) correctly into a pandas DataFrame. Running the previous code snippet will produce the output shown in table 9.1.</p>

  <p class="fm-table-caption"><a id="pgfId-1128408"></a>Table 9.1 Sample data from the Amazon review data set</p>

  <table border="1" class="contenttable" width="100%">
    <colgroup class="calibre12">
      <col class="calibre13" span="1" width="15%"/>
      <col class="calibre13" span="1" width="15%"/>
      <col class="calibre13" span="1" width="15%"/>
      <col class="calibre13" span="1" width="15%"/>
      <col class="calibre13" span="1" width="40%"/>
    </colgroup>

    <tr class="calibre14">
      <th class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1128418"></a> </p>
      </th>

      <th class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1128420"></a><b class="fm-bold">overall</b></p>
      </th>

      <th class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1128422"></a><b class="fm-bold">verified</b></p>
      </th>

      <th class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1128424"></a><b class="fm-bold">reviewTime</b></p>
      </th>

      <th class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1128426"></a><b class="fm-bold">reviewText</b></p>
      </th>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1128428"></a>0</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1128430"></a>5</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1128432"></a>True</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1128434"></a>10 17, 2015</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1128436"></a>This game is a bit hard to get the hang of, bu...</p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1128438"></a>1</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1128440"></a>4</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1128442"></a>False</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1128444"></a>07 27, 2015</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1128446"></a>I played it a while but it was alright. The st...</p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1128448"></a>2</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1128450"></a>3</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1128452"></a>True</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1128454"></a>02 23, 2015</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1128456"></a>ok game.</p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1128458"></a>3</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1128460"></a>2</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1128462"></a>True</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1128464"></a>02 20, 2015</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1128466"></a>found the game a bit too complicated, not what...</p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1128468"></a>4</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1128470"></a>5</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1128472"></a>True</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1128474"></a>12 25, 2014</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1128476"></a>great game, I love it and have played it since...</p>
      </td>
    </tr>
  </table>

  <p class="body"><a class="calibre8" id="pgfId-1123289"></a>We will now remove any records that have an empty or <span class="fm-code-in-text">null</span> value in the <span class="fm-code-in-text">reviewText</span> column:</p>
  <pre class="programlisting">review_df = review_df[~review_df["reviewText"].isna()]
review_df = review_df[review_df["reviewText"].str.strip().str.len()&gt;0]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123292"></a>As you may have already noticed, there’s a column that says whether the review is from a verified buyer. To preserve the integrity of our data, let’s only consider the reviews from verified buyers. But before that, we have to make sure that we have enough data after filtering unverified reviews. To do that, let’s see how many records there are for different values (i.e., <span class="fm-code-in-text">True</span> and <span class="fm-code-in-text">False</span>) of the verified column. For that, we will use panda’s built-in <span class="fm-code-in-text">value_counts()</span> function<a class="calibre8" id="marker-1123293"></a> as follows:</p>
  <pre class="programlisting">review_df["verified"].value_counts()</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123295"></a>This will return</p>
  <pre class="programlisting">True     332504
False    164915
Name: verified, dtype: int64</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123299"></a>That’s great news. It seems we have more data from verified buyers than unverified users. Let’s create a new DataFrame called <span class="fm-code-in-text">verified_df</span> that only contains verified reviews:</p>
  <pre class="programlisting">verified_df = review_df.loc[review_df["verified"], :]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123302"></a>Next, out of the verified reviews, we will evaluate the number of reviews for each different rating in the overall column:</p>
  <pre class="programlisting">verified_df["overall"].value_counts()</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123304"></a>This will give</p>
  <pre class="programlisting">5    222335
4     54878
3     27973
1     15200
2     12118
Name: overall, dtype: int64</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123312"></a>This is an interesting finding. Typically, we want to have equal amounts of data for each different rating. But that’s never the case in the real world. For example, here, we have four times more 5-star reviews than 4-star reviews. This is known as <i class="fm-italics">class imbalance</i><a class="calibre8" id="marker-1123313"></a>. Real-world data is often noisy, imbalanced, and dirty. We will see these characteristics as we look further into the data. We will circle back to the issue of class imbalance in the data when we are developing our model.</p>

  <p class="body"><a class="calibre8" id="pgfId-1123314"></a>Sentiment analysis is designed as a classification problem. Given the review (as a sequence of words, for example), the model predicts a class out of a set of discrete classes. We are going to focus on two classes: positive or negative. We will make the assumption that 5 or 4 stars indicate a positive sentiment, whereas 3, 2, or 1 star mean a negative sentiment. Astute problem formulation, such as reducing the number of classes, can make the classification task easier. To do this, we can use the convenient built-in pandas function <span class="fm-code-in-text">map()</span>. <span class="fm-code-in-text">map()</span> takes a dictionary, where the key indicates the current value, and the value indicates the value the current value needs to be mapped to:</p>
  <pre class="programlisting">verified_df["label"]=verified_df["overall"].map({5:1, 4:1, 3:0, 2:0, 1:0})</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123317"></a>Now let’s check the number of instances for each class after the transformation</p>
  <pre class="programlisting">verified_df["label"].value_counts()</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123319"></a>which will return</p>
  <pre class="programlisting">1    277213
0     55291
Name: label, dtype: int64</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123323"></a>There’s around 83% positive samples and 17% negative samples. That’s a significant discrepancy in terms of the number of samples. The final step of our simple data exploration is to make sure there’s no order in the data. To shuffle the data, we will use panda’s <span class="fm-code-in-text">sample()</span> function. <span class="fm-code-in-text">sample()</span>is technically used to sample a small fraction of data from a large data set. But by setting <span class="fm-code-in-text">frac=1.0</span> and a fixed random seed, we can get the full data set shuffled in a random manner:</p>
  <pre class="programlisting">verified_df = verified_df.sample(frac=1.0, random_state=random_seed)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123327"></a>Finally, we will separate the inputs and labels into two separate variables, as this will make processing easier for the next steps:</p>
  <pre class="programlisting">inputs, labels = verified_df["reviewText"], verified_df["label"]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123330"></a>Next, we will focus on an imperative task, which will ultimately improve the quality of the data that is going into the model: cleaning and preprocessing the text. Here we will focus on performing the following subtasks. You will learn more details about every subtask in the coming discussion:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1123331"></a>Lower the case of words.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123332"></a>Treat shortened forms of words (e.g., “aren’t,” “you’ll,” etc.).</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123334"></a>Tokenize text into words (known as <i class="fm-italics">tokenization</i><a class="calibre8" id="marker-1123333"></a>).</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123335"></a>Remove uninformative text, such as numbers, punctuation, and stop words. Stop words are words that are frequent in text corpora but do not justify the value of their presence enough for most NLP tasks (e.g., “and,” “the,” “am,” “are,” “it,” “he,” “she,” etc.).</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1123336"></a>Lemmatize words. Lemmatization is the process of converting words to their base-form (e.g., plural nouns to singular nouns and past-tense verbs to present-tense verbs).</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1123337"></a>To do most of these tasks, we will rely on a famous and well-known Python library for text processing known as NLTK (Natural Language Toolkit<a class="calibre8" id="marker-1123338"></a>). If you have set up the development environment, you should have the NLTK library installed. But our job is not done yet. To perform some of the subtasks, we need to download several external resources provided by NLTK:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1123340"></a><span class="fm-code-in-text">averaged_perceptron_tagger</span><a class="calibre8" id="marker-1123339"></a>—Identifies part of speech</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123343"></a><span class="fm-code-in-text">wordnet</span><a class="calibre8" id="marker-1123341"></a> <i class="fm-italics">and</i> <span class="fm-code-in-text">omw-1.4</span>-_W<a class="calibre8" id="marker-1123342"></a>ill be used to lemmatize (i.e., convert words to their base form)</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123345"></a><span class="fm-code-in-text">stopwords</span><a class="calibre8" id="marker-1123344"></a>—Provides the list of stop words for various languages</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1123347"></a><span class="fm-code-in-text">punkt</span>—Used to tokenize text to smaller components (e.g., words, sentences, etc.)</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1123348"></a>Let’s first do that:</p>
  <pre class="programlisting">import nltk
 
nltk.download('averaged_perceptron_tagger', download_dir='nltk')
nltk.download('wordnet', download_dir='nltk')
nltk.download('omw-1.4', download_dir='nltk')
nltk.download('stopwords', download_dir='nltk')
nltk.download('punkt', download_dir='nltk')
nltk.data.path.append(os.path.abspath('nltk'))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123357"></a>We can now continue with our project. To understand the various preprocessing steps that’ll be laid out here, we will zoom in on a single review, which is simply a Python string (i.e., a sequence of characters). Let’s call this single review <span class="fm-code-in-text">doc</span>.</p>

  <p class="body"><a class="calibre8" id="pgfId-1123359"></a>First we can convert a string to lowercase simply by calling the function <span class="fm-code-in-text">lower</span><a class="calibre8" id="marker-1123358"></a><span class="fm-code-in-text">()</span> on a string. <span class="fm-code-in-text">lower</span><a class="calibre8" id="marker-1123360"></a><span class="fm-code-in-text">()</span> is a Python built-in function available for strings that will convert characters in a given string to lowercase characters:</p>
  <pre class="programlisting">    doc = doc.lower()</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123364"></a>Next, we will expand the <span class="fm-code-in-text">"n't"</span> to <span class="fm-code-in-text">"not"</span> if it’s present:</p>
  <pre class="programlisting">import re
    
doc = re.sub(pattern=r"\w+n\'t ", repl="not ", string=doc)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123369"></a>To do so, we will use regular expressions. Regular expressions give us a way to match arbitrary patterns and manipulate them in various ways. Python has a built-in library to handle regular expressions, known as <span class="fm-code-in-text">re</span>. Here, <span class="fm-code-in-text">re.sub()</span> will substitute words that match a certain <span class="fm-code-in-text">pattern</span> (i.e., any sequence of alphabetical characters followed by <span class="fm-code-in-text">"n't;</span> e.g., “don’t,” “can’t”) and replace them with a string passed as <span class="fm-code-in-text">repl</span> (i.e., “not “) in the <span class="fm-code-in-text">string doc</span>. For example, “won’t” will be replaced with “not.” We do not care about the prefix “will,” as it will be removed anyway during the stop word removal we will perform later. If you’re interested, you can read more about regular expression syntax at <span class="fm-hyperlink"><a class="url" href="https://www.rexegg.com/regex-quickstart.html">https://www.rexegg.com/regex-quickstart.html</a></span>.</p>

  <p class="body"><a class="calibre8" id="pgfId-1123371"></a>We will remove shortened forms such as ’ll, ’re, ’d, and ’ve. You might notice that this will result in uncomprehensive phrases like “wo” (i.e., “won’t” becomes “wo” + “not”); we can safely ignore them. Notice that we are treating the shortened form of “not” quite differently from other shortened forms. This is because, unlike the other shortened forms, if present, “not” can have a significant impact on what a review actually conveys. We will talk about this again in just a little while:</p>
  <pre class="programlisting">doc = re.sub(pattern=r"(?:\'ll |\'re |\'d |\'ve )", repl=" ", string=doc)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123375"></a>Here, to replace the shortened forms of ’ll, ’re, ’d, and ’ve, we are again using regular expressions. Here, <span class="fm-code-in-text">r"(?:\'ll|\'re|\'d|\'ve)"</span> is a regular expression in Python that essentially identifies any occurrence of <span class="fm-code-in-text">'ll/'re/'d/'ve</span> in <span class="fm-code-in-text">doc</span>. Then we will remove any digits in doc using the <span class="fm-code-in-text">re.sub()</span> function<a class="calibre8" id="marker-1123376"></a> like before:</p>
  <pre class="programlisting">    doc = re.sub(pattern=r"/d+", repl="", string=doc)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123380"></a>We will remove stop words and any punctuation as our next step. As mentioned earlier, stop words are words that appear in text but add very little value to the meaning of the text. In other words, even if the stop words are missing from the text, you’ll still be able to infer the meaning of what’s being said. The library NLTK provides a list of stop words, so we don’t have to come up with them:</p>
  <pre class="programlisting">from nltk.corpus import stopwords
from nltk import word_tokenize
import string
 
EN_STOPWORDS = set(stopwords.words('english')) - {'not', 'no'}
 
(doc) if w not in EN_STOPWORDS and w not in string.punctuation]  </pre>

  <p class="body"><a class="calibre8" id="pgfId-1123390"></a>Here, to access the stop words, all you have to do is call <span class="fm-code-in-text">from nltk.corpus import stopwords</span> and then call <span class="fm-code-in-text">stopwords.words('english')</span>. This will return a list. If you look at the words present in the list of stop words, you’ll observe almost all the common words (e.g., “I,” “you,” “a,” “the,” “am,” “are,” etc.) you’d encounter while reading a text. But as we stressed earlier, the word “not” is a special word, especially in the context of sentiment analysis. The presence of words like “no” and “not” can completely flip the meaning of a text in our case.</p>

  <p class="body"><a class="calibre8" id="pgfId-1123391"></a>Also note the use of the function <span class="fm-code-in-text">word_tokenize()</span><a id="marker-1123388"></a>. This is a special processing step known as <i class="fm-italics">tokenization</i>. Here, passing a string to <span class="fm-code-in-text">word_tokenize()</span> returns a list, with each word being an element. Word tokenization might look very trivial for a language like English, where words are delimited by a space character or a period. But this can be a complex task in other languages (e.g., Japanese) where separation between tokens is not explicit.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1123392"></a>Do “not” let stop words fool you!</p>

    <p class="fm-sidebar-text"><a id="pgfId-1131698"></a>If you look at most stop word lists, you will see that the words “no” and “not” are considered stop words as they are common words to see in a text corpus. However, for our task of sentiment analysis, these words play a significant role in changing the meaning (and possibly the label) of a review. The review “this is a great video game” means the opposite of “this is not a great video game” or “this game is no good.” For this reason, we specifically remove the words “no” and “not” from the list of stop words.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1123395"></a>Next, we have another treatment known as <i class="fm-italics">lemmatization</i><a class="calibre8" id="marker-1123394"></a>. Lemmatization truncates/ stems a given word to a base form, for example, converting plural nouns to singular nouns or past tense verbs to present tense, and so on. This can be done easily using a lemmatizer object shipped with the NLTK package:</p>
  <pre class="programlisting">lemmatizer = WordNetLemmatizer()</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123400"></a>Here we are downloading the <span class="fm-code-in-text">WordNetLemmatizer</span>. <span class="fm-code-in-text">WordNetLemmatizer</span><a class="calibre8" id="marker-1123399"></a> is a lemmatizer built on the well-renowned WordNet database. If you haven’t heard of WordNet, it is a famous lexical database (in the form of a network/graph) that you can utilize for tasks such as information retrieval, machine translation, text summarization, and so on. WordNet comes in many sizes and flavors (e.g., Multilingual WordNet, Non-English WordNet, etc.). You can explore more about WordNet and browse the database online at <span class="fm-hyperlink"><a class="url" href="https://wordnet.princeton.edu/">https://wordnet.princeton.edu/</a></span>:</p>
  <pre class="programlisting">pos_tags = nltk.pos_tag(tokens)
    clean_text = [
        lemmatizer.lemmatize(w, pos=p[0].lower()) \
        if p[0]=='N' or p[0]=='V' else w \
        for (w, p) in pos_tags
    ]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123411"></a>By calling the function <span class="fm-code-in-text">lemmatizer.lemmatize()</span>, you can convert any given word to its base form (if it is not already in base form). But when calling the function, you need to pass in an important argument called <span class="fm-code-in-text">pos</span>. <span class="fm-code-in-text">pos</span> refers to the PoS tag (part-of-speech tag) of that word. PoS tagging is a special NLP task, where the task is to classify a given word to a PoS tag from a given set of discrete PoS tags. Here are a few examples of PoS tags:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1123412"></a><i class="fm-italics">DT</i>—Determiner (e.g., a, the)</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123413"></a><i class="fm-italics">JJ</i>—Adjective (e.g., beautiful, delicious)</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123414"></a><i class="fm-italics">NN</i>—Noun, singular or mass (e.g., person, dog)</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123415"></a><i class="fm-italics">NNS</i>—Noun, plural (e.g., people, dogs)</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123416"></a><i class="fm-italics">NNP</i>—Proper noun, singular (e.g., I, he, she)</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123417"></a><i class="fm-italics">NNPS</i>—Proper noun, plural (e.g., we, they)</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123418"></a><i class="fm-italics">VB</i>—Verb, base form (e.g., go, eat, walk)</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123419"></a><i class="fm-italics">VBD</i>—Verb, past tense (e.g., went, ate, walked)</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123420"></a><i class="fm-italics">VBG</i>—Verb, gerund or present participle (e.g., going, eating, walking)</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123421"></a><i class="fm-italics">VBN</i>—Verb, past participle (e.g., gone, eaten, walked)</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123422"></a><i class="fm-italics">VBP</i>—Verb, non-third-person singular present</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1123423"></a><i class="fm-italics">VBZ</i>—Verb, third-person singular present</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1123424"></a>You can find the full list of PoS tags at <span class="fm-hyperlink"><a class="url" href="http://mng.bz/mO1W">http://mng.bz/mO1W</a></span>. A note-worthy observation is how the tags are organized. You can see that if you consider only the first two characters of the tags, you get a broader set of classes (e.g., NN, VB), where all the nouns will be classified with NN and verbs will be classified with VB, and so on. We will use this property to make our lives easier.</p>

  <p class="body"><a class="calibre8" id="pgfId-1123425"></a>Back to our code: let’s assimilate how we are using PoS tags to lemmatize words. When lemmatizing words, you have to pass in the PoS tag for the word you are lemmatizing. This is important as the lemmatization logic is different for different types of words. We will first get a list that has (&lt;word&gt;, &lt;pos&gt;) elements for the words in tokens (returned by the tokenization process). Then we iterate through the <span class="fm-code-in-text">pos_tags</span> list and call the <span class="fm-code-in-text">lemmatizer.lemmatize()</span> function with the word and the PoS tag. We will only lemmatize verbs and nouns to save computational time.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1123427"></a>More about WordNet</p>

    <p class="fm-sidebar-text"><a id="pgfId-1123428"></a>Word net is a lexical database (sometimes called a lexical ontology) that is in the form of an interconnected network. These connections are based on how similar two words are. For example, the words “car” and “automobile” have a smaller distance, whereas “dog” and “volcano” are far apart.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1123429"></a>The words in WordNet are grouped into <i class="fm-italics">synsets</i> (short for synonym sets). A synset captures the words that share a common meaning (e.g., dog, cat, hamster). Each word can belong to one or multiple synsets. Each synset has <i class="fm-italics">lemmas</i>, which are the words in that synset.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1123430"></a>Going a level up, there are relationships between synsets. There are four different relationships:</p>

    <ul class="calibre9">
      <li class="fm-sidebar-number">
        <p class="list1"><a id="pgfId-1123432"></a><i class="fm-italics">Hypernyms</i><a id="marker-1129750"></a>—A hypernym synset is the synset that is more general than a given synset. For example, “animal” is a hypernym synset of “pet.”</p>
      </li>

      <li class="fm-sidebar-number">
        <p class="list1"><a id="pgfId-1123434"></a><i class="fm-italics">Hyponyms</i><a id="marker-1129752"></a>—A hyponym synset is a more specific synset than a given synset. For example, “car” is a hyponym synset of the “vehicle” synset.</p>
      </li>

      <li class="fm-sidebar-number">
        <p class="list1"><a id="pgfId-1123436"></a><i class="fm-italics">Holonyms</i><a id="marker-1129754"></a>—A holonym synset is a synset that a given synset is a part of (is-part-of relationship). For example, “engine” is a holonym synset of the “car” synset.</p>
      </li>

      <li class="fm-sidebar-number">
        <p class="list1"><a id="pgfId-1123438"></a><i class="fm-italics">Meronyms</i><a id="marker-1129756"></a><a id="marker-1141556"></a>—A meronym synset is a synset that a given synset is made of (is-made-of relationship). For example, “leaf” is a meronym synset of the “plant” synset.</p>
      </li>
    </ul>

    <p class="fm-sidebar-text"><a id="pgfId-1123439"></a>Due to this organization of interconnected synsets, using WordNet you can measure the distance between two words as well. Similar words will have a smaller distance, whereas disparate words will have a larger distance.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1123440"></a>You can experiment with these ideas in NLTK by importing WordNet from <span class="fm-code-in-text1">nltk.corpus import wordnet</span>. For more information refer to <span class="fm-hyperlink"><a class="url" href="https://www.nltk.org/howto/wordnet.html">https://www.nltk.org/howto/wordnet.html</a></span>.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1123443"></a>This concludes the series of steps we are incorporating to build the preprocessing workflow for our text. We will encapsulate these steps in a function called <span class="fm-code-in-text">clean_ text()</span>, as in the following listing.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1123445"></a>Listing 9.2 Preprocessing logic for reviews in the dataset</p>
  <pre class="programlisting">def clean_text(doc):
    """ A function that cleans a given document (i.e. a text string)"""
    
    doc = doc.lower()                                        <span class="fm-combinumeral">❶</span>
    doc = doc.replace("n\'t ", ' not ')                      <span class="fm-combinumeral">❷</span>
    doc = re.sub(r"(?:\'ll |\'re |\'d |\'ve )", " ", doc)    <span class="fm-combinumeral">❸</span>
    doc = re.sub(r"/d+","", doc)                             <span class="fm-combinumeral">❹</span>
     
    tokens = [
        w for w in word_tokenize(doc) if w not in EN_STOPWORDS and w not in 
<span class="fm-code-continuation-arrow">➥</span> string.punctuation
    ]                                                        <span class="fm-combinumeral">❺</span>
    
    pos_tags = nltk.pos_tag(tokens)                          <span class="fm-combinumeral">❻</span>
    clean_text = [
        lemmatizer.lemmatize(w, pos=p[0].lower()) \          <span class="fm-combinumeral">❼</span>
        if p[0]=='N' or p[0]=='V' else w \                   <span class="fm-combinumeral">❼</span>
        for (w, p) in pos_tags                               <span class="fm-combinumeral">❼</span>
    ]
 
    return clean_text</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1140770"></a><span class="fm-combinumeral">❶</span> Turn to lower case.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1140791"></a><span class="fm-combinumeral">❷</span> Expand the shortened form n’t to “not.”</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1140808"></a><span class="fm-combinumeral">❸</span> Remove shortened forms like ’ll, ’re, ’d, ’ve, as they don’t add much value to this task.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1140825"></a><span class="fm-combinumeral">❹</span> Remove digits.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1140842"></a><span class="fm-combinumeral">❺</span> Break the text into tokens (or words); while doing that, ignore stop words from the result.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1140859"></a><span class="fm-combinumeral">❻</span> Get the PoS tags for the tokens in the string.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1140876"></a><span class="fm-combinumeral">❼</span> To lemmatize, get the PoS tag of each token; if it is N (noun) or V (verb) lemmatize, else keep the original form.</p>

  <p class="body"><a class="calibre8" id="pgfId-1123475"></a>You can check the processing done in the function by calling it on a sample text</p>
  <pre class="programlisting">sample_doc = 'She sells seashells by the seashore.'
print("Before clean: {}".format(sample_doc))
print("After clean: {}".format(clean_text(sample_doc)))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123480"></a>which returns</p>
  <pre class="programlisting">Before clean: She sells seashells by the seashore.
After clean: [“sell”, “seashell”, “seashore”]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123484"></a>We will leverage this function along with panda’s <span class="fm-code-in-text">apply</span> function<a class="calibre8" id="marker-1123483"></a> to apply this processing pipeline on each row of text that we have in our <span class="fm-code-in-text">data</span> DataFrame:</p>
  <pre class="programlisting">inputs = inputs.apply(lambda x: clean_text(x))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123487"></a>You might want to leave your computer for a while to grab a coffee or to check on your friends. It might take close to an hour to run this one-liner. The final result looks like table 9.2.</p>

  <p class="fm-table-caption"><a id="pgfId-1129950"></a>Table 9.2 Original text versus preprocessed text</p>

  <table border="1" class="contenttable" width="100%">
    <colgroup class="calibre12">
      <col class="calibre13" span="1" width="50%"/>
      <col class="calibre13" span="1" width="50%"/>
    </colgroup>

    <tr class="calibre14">
      <th class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1129954"></a><b class="fm-bold">Original text</b></p>
      </th>

      <th class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1129956"></a><b class="fm-bold">Clean text (tokenized)</b></p>
      </th>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1129958"></a>Worked perfectly on Wii and GameCube.No issues with compatibility or loss of memory.</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1129960"></a><span class="fm-code-in-figurecaption">['work', 'perfectly', 'wii', 'gamecube', ‘no’, 'issue', 'compatibility', 'loss', 'memory']</span></p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1129962"></a>Loved the game, and the other collectibles that came with it are well made. The mask is big, and it almost fits my face, so that was impressive.</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1129964"></a><span class="fm-code-in-figurecaption">['loved', 'game', 'collectible', 'come', 'well', 'make', 'mask', 'big', 'almost', 'fit', 'face', 'impressive']</span></p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1129966"></a>It’s an okay game. To be honest, I am very bad at these types of games and to me it’s very difficult! I am always dying, which depresses me. Maybe if I had more skill I would enjoy this game more!</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1129968"></a><span class="fm-code-in-figurecaption">["'s", 'okay', 'game', 'honest', 'bad', 'type', 'game', '--', "'s", 'difficult', 'always', 'die', 'depresses', 'maybe', 'skill', 'would', 'enjoy', 'game']</span></p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1129970"></a>Excellent product as described.</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1129972"></a><span class="fm-code-in-figurecaption">['excellent', 'product', 'describe']</span></p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1129974"></a>The level of detail is great; you can feel the love for cars in this game.</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1129976"></a><span class="fm-code-in-figurecaption">['level', 'detail', 'great', 'feel', 'love', 'car', 'game']</span></p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1129978"></a>I can’t play this game.</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1129980"></a><span class="fm-code-in-figurecaption">['not', 'play', 'game']</span></p>
      </td>
    </tr>
  </table>

  <p class="body"><a class="calibre8" id="pgfId-1123521"></a>Finally, to avoid overdosing on coffee or pestering your friends by running this too many times, we will save the data to the disk:</p>
  <pre class="programlisting">inputs.to_pickle(os.path.join('data','sentiment_inputs.pkl'))
labels.to_pickle(os.path.join('data','sentiment_labels.pkl'))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123525"></a>Now, we are going to define a data pipeline to transform the data to a format that is understood by the model and can be used to train and evaluate the model.</p>

  <p class="fm-head2"><a id="pgfId-1123526"></a>Exercise 1</p>

  <p class="body"><a class="calibre8" id="pgfId-1123527"></a>Given the string <span class="fm-code-in-text">s</span>, “i-purchased-this-game-for-99-i-want-a-refund,” you’d like to replace the dash “-” with a space and then lemmatize only the verbs in the text. How would you do<a class="calibre8" id="marker-1123529"></a> that?</p>

  <h2 class="fm-head" id="sigil_toc_id_119"><a id="pgfId-1123530"></a>9.2 Getting text ready for the model</h2>

  <p class="body"><a class="calibre8" id="pgfId-1123533"></a>You<a class="calibre8" id="marker-1123531"></a><a class="calibre8" id="marker-1123532"></a> have a clean data set with text stripped of any unnecessary or unwarranted linguistic complexities for the problem we’re solving. Additionally, the binary labels have been generated from the number of stars given for each review. Before pushing ahead with the model training and evaluation, we have to do some further processing of our data set. Specifically, we will create three subsets of data—training, validation and testing—which will be used to train and evaluate the model. Next, we will look at two important characteristics of our data sets: the vocabulary size and the distribution of the sequence length (i.e., the number of words) in the examples we have. Finally, you will convert the words to numbers (or numerical IDs), as machine learning models do not understand strings but numbers.</p>

  <p class="body"><a class="calibre8" id="pgfId-1123534"></a>In this section, we will further prepare the data to be consumed by the model. Right now, we have a very nice layout of processing steps to go from a noisy, inconsistent review to a simple, consistent text string that preserves the semantics of the review. But we haven’t addressed the elephant in the room! That is, machine learning models understand numerical data, not textual data. The string “not a great game” does not mean anything to a model if you present it as is. We have to further refine our data so that we end up with a number sequence instead of a word sequence. In our journey to get the data ready for the model, we will perform the following subtasks:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1123535"></a>Check the size of the vocabulary/word frequency after preprocessing. This will later be used as a hyperparameter of the model.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123536"></a>Check the summary statistics of the sequence length (mean, median, and standard deviation). This will later be used as a hyperparameter of the model.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1123537"></a>Create a dictionary that will map each unique word to a unique ID (we will call this a tokenizer).</p>
    </li>
  </ul>

  <h3 class="fm-head1" id="sigil_toc_id_120"><a id="pgfId-1123538"></a>9.2.1 Splitting training/validation and testing data</h3>

  <p class="body"><a class="calibre8" id="pgfId-1123542"></a>A<a class="calibre8" id="marker-1123540"></a><a class="calibre8" id="marker-1123541"></a> word of caution! When performing these tasks, you might inadvertently create oozing <a id="marker-1129819"></a>data leakages in our model. We have to make sure we perform these tasks <i class="fm-italics">using only the training data set</i> and keep the validation and testing data separate. Therefore, our first goal should be separating training/validation/test data.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1123543"></a>The lurking data leakages in NLP</p>

    <p class="fm-sidebar-text"><a id="pgfId-1123544"></a>You might be thinking, “Great! All I have to do is load the processed text corpus and perform the analysis or tasks on that.” Not so fast! That is the incorrect way to do it. Before doing any data-specific processing/analysis, such as computing the vocabulary size or developing a tokenizer, you have to separate the data into training/validation and test sets, and then perform this processing/analysis on the training data.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1123545"></a>The purpose of the validation data is to act as a compass for choosing hyperparameters and to determine when to stop training. The test data set is your benchmark on how well the model is going to perform in the real world. Given the nature of the purpose fulfilled by the validation/test data, they should not be part of your analysis, but only be used to evaluate performance. Using validation/test data in your analysis to develop a model gives you an unfair advantage and causes what’s known as <i class="fm-italics">data leakage</i><a id="marker-1141639"></a>. Data leakage refers to directly or indirectly providing access to the examples you evaluate the model on. If the validation/test data is used for any analysis we do, we are providing access to those data sets before the evaluation phase. Models with data leakages can lead to poor performance in the real world.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1123548"></a>We know we have an imbalanced data set. Despite having an imbalanced data set, we have to make sure our model is good at identifying both positive and negative reviews. This means the data sets we’ll be evaluating on need to be balanced. To achieve this, here’s what we will do:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1123549"></a>Create balanced (i.e., equal count of positive and negative samples) validation and test sets</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1123550"></a>Assign the remaining datapoints to the training set</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1123551"></a>Figure 9.1 depicts this process.</p>

  <p class="fm-figure"><img alt="09-01" class="calibre10" src="../../OEBPS/Images/09-01.png" width="1092" height="653"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1142147"></a>Figure 9.1 The process for splitting training/valid/test data</p>

  <p class="body"><a class="calibre8" id="pgfId-1123558"></a>We will now see how we can do this in Python. First, we start by identifying the indices that correspond to positive labels and negative labels separately:</p>
  <pre class="programlisting">neg_indices = pd.Series(labels.loc[(labels==0)].index)
pos_indices = pd.Series(labels.loc[(labels==1)].index)</pre>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1123562"></a>Stratified sampling: An alternative to imbalanced data sets</p>

    <p class="fm-sidebar-text"><a id="pgfId-1123563"></a>Your design of validation and test sets will dictate how you will define performance metrics to evaluate the trained model. If you create equally balanced validation/test sets, then you can safely use accuracy as a metric to evaluate the trained model. That is what we will do here: create balanced validation/test data sets, and then use accuracy as a metric to evaluate the model. But you might not be so lucky all the time. There can be scenarios where the minority class is so scary you can’t afford to create balanced data sets.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1123565"></a>In such instances, you can use <i class="fm-italics">stratified sampling</i><a id="marker-1132242"></a><a id="marker-1141683"></a>. Stratified sampling creates individual data sets, roughly maintaining the original class ratios in the full data set. When this is the case, you have to carefully choose your metric, because standard accuracy can no longer be trusted. For example, if you care about identifying positive samples with high accuracy at the cost of a few false positives, then you should use recall (or F1 score with higher weight given to recall) as the performance metric.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1123567"></a>Next, we will define the size of our validation/test set as a function of <span class="fm-code-in-text">train_fraction</span> (a user-defined argument that determines how much data to leave for the training set). We will use a default value of 0.8 for the <span class="fm-code-in-text">train_fraction</span>:</p>
  <pre class="programlisting">n_valid = int(
    min([len(neg_indices), len(pos_indices)]) * ((1-train_fraction)/2.0)
)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123571"></a>It might look like a complex computation, but it is, in fact, a simple one. We will use the valid fraction as half of the fraction of data left for training data (the other half is used for the testing set). And finally, to convert the fractional value to the actual number of samples, we multiply the fraction by the smallest of counts of positive and negative samples. This way, we make sure the underrepresented class stays as the focal point during the data split. We keep the validation set and the test set equal. Therefore</p>
  <pre class="programlisting">n_test = n_valid</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123573"></a>Next, we define the three sets of indices (for train/validation/test datasets) for each label type (positive and negative). We will create a funneling process to assign data points to different data sets. First, we do the following:</p>

  <ol class="calibre11">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1123574"></a>Randomly sample <span class="fm-code-in-text">n_test</span> number of indices from the negative indices (<span class="fm-code-in-text">neg_ test_indices</span>).</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123575"></a>Then randomly sample <span class="fm-code-in-text">n_valid</span> indices from the remaining indices (<span class="fm-code-in-text">neg_ valid_inds</span>).</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1123576"></a>The remaining indices are kept as the training instances (<span class="fm-code-in-text">neg_train_inds</span>).</p>
    </li>
  </ol>

  <p class="body"><a class="calibre8" id="pgfId-1123577"></a>The same process is then repeated for positive indices to create three index sets for training/validation/test data sets:</p>
  <pre class="programlisting">neg_test_inds = neg_indices.sample(n=n_test, random_state=random_seed)
neg_valid_inds = neg_indices.loc[
    ~neg_indices.isin(neg_test_inds)
].sample(n=n_test, random_state=random_seed)
neg_train_inds = neg_indices.loc[
    ~neg_indices.isin(neg_test_inds.tolist()+neg_valid_inds.tolist())
]
    
pos_test_inds = pos_indices.sample(n=n_test, random_state=random_seed
)
pos_valid_inds = pos_indices.loc[
    ~pos_indices.isin(pos_test_inds)
].sample(n=n_test, random_state=random_seed)
pos_train_inds = pos_indices.loc[        
    ~pos_indices.isin(pos_test_inds.tolist()+pos_valid_inds.tolist())
]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123594"></a>With the negative and positive indices to slice the inputs and labels, now it’s time to create actual data sets:</p>
  <pre class="programlisting">tr_x = inputs.loc[
    neg_train_inds.tolist() + pos_train_inds.tolist()
].sample(frac=1.0, random_state=random_seed)
tr_y = labels.loc[
    neg_train_inds.tolist() + pos_train_inds.tolist()
].sample(frac=1.0, random_state=random_seed)
 
v_x = inputs.loc[
    neg_valid_inds.tolist() + pos_valid_inds.tolist()
].sample(frac=1.0, random_state=random_seed)
v_y = labels.loc[
    neg_valid_inds.tolist() + pos_valid_inds.tolist()
].sample(frac=1.0, random_state=random_seed)
 
ts_x = inputs.loc[
    neg_test_inds.tolist() + pos_test_inds.tolist()
].sample(frac=1.0, random_state=random_seed)
ts_y = labels.loc[
    neg_test_inds.tolist() + pos_test_inds.tolist()
].sample(frac=1.0, random_state=random_seed)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123615"></a>Here, <span class="fm-code-in-text">(tr_x, tr_y)</span>, <span class="fm-code-in-text">(v_x, v_y)</span>, and <span class="fm-code-in-text">(ts_x, ts_y)</span> represent the training, validation, and testing data sets, respectively. Here, the data sets suffixed with <span class="fm-code-in-text">_x</span> come from the <span class="fm-code-in-text">inputs</span>, and the data sets suffixed with <span class="fm-code-in-text">_y</span> come from the <span class="fm-code-in-text">labels</span>. Finally, we can wrap the logic we discussed in a single function as in the following listing.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1123617"></a>Listing 9.3 Splitting training/validation/testing data sets</p>
  <pre class="programlisting">def train_valid_test_split(inputs, labels, train_fraction=0.8):
    """ Splits a given dataset into three sets; training, validation and test """
    
    neg_indices = pd.Series(labels.loc[(labels==0)].index)                   <span class="fm-combinumeral">❶</span>
    pos_indices = pd.Series(labels.loc[(labels==1)].index)                   <span class="fm-combinumeral">❶</span>
    
    n_valid = int(min([len(neg_indices), len(pos_indices)]) 
       * ((1-train_fraction)/2.0))                                           <span class="fm-combinumeral">❷</span>
    n_test = n_valid                                                         <span class="fm-combinumeral">❷</span>
    
    neg_test_inds = neg_indices.sample(n=n_test, random_state=random_seed)   <span class="fm-combinumeral">❸</span>
    neg_valid_inds = neg_indices.loc[~neg_indices.isin(
       neg_test_inds)].sample(n=n_test, random_state=random_seed)            <span class="fm-combinumeral">❹</span>
    neg_train_inds = neg_indices.loc[~neg_indices.isin(
        neg_test_inds.tolist()+neg_valid_inds.tolist())]                     <span class="fm-combinumeral">❺</span>
    
    pos_test_inds = pos_indices.sample(n=n_test)                             <span class="fm-combinumeral">❻</span>
    pos_valid_inds = pos_indices.loc[
        ~pos_indices.isin(pos_test_inds)].sample(n=n_test)                   <span class="fm-combinumeral">❻</span>
    pos_train_inds = pos_indices.loc[
        ~pos_indices.isin(pos_test_inds.tolist()+pos_valid_inds.tolist())    <span class="fm-combinumeral">❻</span>
    ]
    
    tr_x = inputs.loc[neg_train_inds.tolist() + 
<span class="fm-code-continuation-arrow">➥</span> pos_train_inds.tolist()].sample(frac=1.0, random_state=random_seed)       <span class="fm-combinumeral">❼</span>
    tr_y = labels.loc[neg_train_inds.tolist() + 
<span class="fm-code-continuation-arrow">➥</span> pos_train_inds.tolist()].sample(frac=1.0, random_state=random_seed)       <span class="fm-combinumeral">❼</span>
    v_x = inputs.loc[neg_valid_inds.tolist() + 
<span class="fm-code-continuation-arrow">➥</span> pos_valid_inds.tolist()].sample(frac=1.0, random_state=random_seed)       <span class="fm-combinumeral">❼</span>
    v_y = labels.loc[neg_valid_inds.tolist() + 
<span class="fm-code-continuation-arrow">➥</span> pos_valid_inds.tolist()].sample(frac=1.0, random_state=random_seed)       <span class="fm-combinumeral">❼</span>
    ts_x = inputs.loc[neg_test_inds.tolist() + 
<span class="fm-code-continuation-arrow">➥</span> pos_test_inds.tolist()].sample(frac=1.0, random_state=random_seed)        <span class="fm-combinumeral">❼</span>
    ts_y = labels.loc[neg_test_inds.tolist() + 
<span class="fm-code-continuation-arrow">➥</span> pos_test_inds.tolist()].sample(frac=1.0, random_state=random_seed)        <span class="fm-combinumeral">❼</span>
    
    print('Training data: {}'.format(len(tr_x)))
    print('Validation data: {}'.format(len(v_x)))
    print('Test data: {}'.format(len(ts_x)))
    
    return (tr_x, tr_y), (v_x, v_y), (ts_x, ts_y)</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1140145"></a><span class="fm-combinumeral">❶</span> Separate indices of negative and positive data points.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1140173"></a><span class="fm-combinumeral">❷</span> Compute the valid and test data set sizes (for minority class).</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1140190"></a><span class="fm-combinumeral">❸</span> Get the indices of the minority class that goes to the test set.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1140207"></a><span class="fm-combinumeral">❹</span> Get the indices of the minority class that goes to the validation set.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1140224"></a><span class="fm-combinumeral">❺</span> The rest of the indices in the minority class belong to the training set.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1140241"></a><span class="fm-combinumeral">❻</span> Compute the majority class indices for the test/validation/train sets</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1140260"></a><span class="fm-combinumeral">❼</span> Get the training/valid/test data sets using the indices created.</p>

  <p class="body"><a class="calibre8" id="pgfId-1123660"></a>Then simply call the function to generate training/validation/testing data:</p>
  <pre class="programlisting">(tr_x, tr_y), (v_x, v_y), (ts_x, ts_y) = train_valid_test_split(data, labels)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123663"></a>Next, we’re going to examine the corpus a bit more to explore the vocabulary size and sequence length with respect to the reviews we have in the training set. These will be used as hyperparameters to the model later<a class="calibre8" id="marker-1123665"></a><a class="calibre8" id="marker-1123666"></a> on.</p>

  <h3 class="fm-head1" id="sigil_toc_id_121"><a id="pgfId-1123667"></a>9.2.2 Analyze the vocabulary</h3>

  <p class="body"><a class="calibre8" id="pgfId-1123668"></a>Vocabulary size is an important hyperparameter for the model. Therefore, we have to find the optimal vocabulary<a class="calibre8" id="marker-1123669"></a><a class="calibre8" id="marker-1123670"></a><a class="calibre8" id="marker-1123671"></a> size that will allow us to capture enough information to solve the task accurately. To do that, we will first create a long list, where each element is a word:</p>
  <pre class="programlisting">data_list = [w for doc in tr_x for w in doc]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123674"></a>This line goes through each <span class="fm-code-in-text">doc</span> in <span class="fm-code-in-text">tr_x</span> and then through each word (<span class="fm-code-in-text">w</span>) in that doc and creates a flattened sequence of words that are present in all the documents. Because we have a Python list, where each element is a word, we can utilize Python’s built-in <span class="fm-code-in-text">Counter</span> objects to get a dictionary, where each word is mapped to a key and the value represents the frequency of that word in the corpus. Note how we are using the training data set only for this analysis in order to avoid data leakage:</p>
  <pre class="programlisting">from collections import Counter
cnt = Counter(data_list)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123679"></a>With our word frequency dictionary out of the way, let’s look at some of the most common words in our corpus:</p>
  <pre class="programlisting">freq_df = pd.Series(
    list(cnt.values()), 
    index=list(cnt.keys())
).sort_values(ascending=False)
 
print(freq_df.head(n=10))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123687"></a>This will return the following result, where you can see the top words that appear in the text. Looking at the results, it makes sense. It’s no surprise that words like “game,” “like,” and “play” get priority in terms of frequency over the other words:</p>
  <pre class="programlisting">game     407818
not      248244
play     128235
's       127844
get      108819
like     100279
great     97041
one       89948
good      77212
time      63450
dtype: int64</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123701"></a>Going a step forward, let’s compute the summary statistics on the text corpus. By doing this, we can see the average frequency of words, standard deviation, minimum, maximum, and so on:</p>
  <pre class="programlisting">print(freq_df.describe())</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123704"></a>This will give some important basic statistics about the frequency of words. For example, from this we can say the average frequency of words is ~76 with a standard deviation of ~1754:</p>
  <pre class="programlisting">count    133714.000000
mean         75.768207
std        1754.508881
min           1.000000
25%           1.000000
50%           1.000000
75%           4.000000
max      408819.000000
dtype: float64</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123714"></a>We will then create a variable called <span class="fm-code-in-text">n_vocab</span> that will hold the size of the vocabulary containing the words appearing at least 25 times in the corpus. You should get a value close to 11,800 for<a class="calibre8" id="marker-1123715"></a><a class="calibre8" id="marker-1123716"></a><a class="calibre8" id="marker-1123717"></a> <span class="fm-code-in-text">n_vocab</span>:</p>
  <pre class="programlisting">n_vocab = (freq_df &gt;= 25).sum()</pre>

  <h3 class="fm-head1" id="sigil_toc_id_122"><a id="pgfId-1123719"></a>9.2.3 Analyzing the sequence length</h3>

  <p class="body"><a class="calibre8" id="pgfId-1123723"></a>Remember<a class="calibre8" id="marker-1123720"></a><a class="calibre8" id="marker-1123721"></a><a class="calibre8" id="marker-1123722"></a> that <span class="fm-code-in-text">tr_x</span> is a pandas Series object, where each row contains a review and each review is a list of words. When the data is in this format, we can use the <span class="fm-code-in-text">pd.Series.str.len</span><a class="calibre8" id="marker-1123724"></a><span class="fm-code-in-text">()</span> function to get the length of each row (or the number of words in each review):</p>
  <pre class="programlisting">seq_length_ser = tr_x.str.len()</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123727"></a>When computing the basic statistics, we will do things a bit differently. Our goal here is to find three bins of sequence lengths so we can bin them to short, medium, and long sequences. We will use these bucket boundaries when defining our TensorFlow data pipeline. To do that, we will first identify the cut-off points (or quantiles) to remove the top and bottom 10% of data. This is because top and bottom slices are full of outliers, and, as you know, they will skew the statistics like mean. In pandas, you can get the quantiles with the <span class="fm-code-in-text">quantile()</span> function<a class="calibre8" id="marker-1123728"></a>, where you pass a fractional value to indicate which quantile you’re interested in:</p>
  <pre class="programlisting">p_10 = seq_length_ser.quantile(0.1)
p_90 = seq_length_ser.quantile(0.9)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123731"></a>Then you simply filter the data between those quantiles. Next, we use the <span class="fm-code-in-text">describe</span> function<a class="calibre8" id="marker-1123732"></a> with the 33% percentile and 66% percentile, as we want to bin to three different categories:</p>
  <pre class="programlisting">seq_length_ser[(seq_length_ser &gt;= p_10) &amp; (seq_length_ser &lt; p_90)].describe(percentiles=[0.33, 0.66])</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123734"></a>If you run this code, you’ll get the following output:</p>
  <pre class="programlisting">count    278675.000000
mean         15.422596
std          16.258732
min           1.000000
33%           5.000000
50%          10.000000
66%          16.000000
max          74.000000
Name: reviewText, dtype: float64</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123744"></a>Following the results, we will use 5 and 15 as our bucket boundaries. In other words, reviews are classified according to the following logic:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1123745"></a>Review length in [0, 5) are short reviews.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123746"></a>Review length in [5, 15) are medium reviews.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1123747"></a>Review length in [15, inf) are long reviews.</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1123748"></a>The last two subsections conclude our analysis to find the vocabulary size and sequence length. The outputs presented here provided all the information to pick our hyperparameters with a principled<a class="calibre8" id="marker-1123749"></a><a class="calibre8" id="marker-1123750"></a><a class="calibre8" id="marker-1123751"></a> mind-set.</p>

  <h3 class="fm-head1" id="sigil_toc_id_123"><a id="pgfId-1123752"></a>9.2.4 Text to words and then to numbers with Keras</h3>

  <p class="body"><a class="calibre8" id="pgfId-1123756"></a>We<a class="calibre8" id="marker-1123754"></a><a class="calibre8" id="marker-1123755"></a> have a clean, processed corpus of text as well as the vocabulary size and sequence length parameters we’ll use later. Our next task is to convert text to numbers. There are two standard steps in converting text to numbers:</p>

  <ol class="calibre11">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1123757"></a>Split text to tokens (e.g., characters/words/sentences).</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1123758"></a>Create a dictionary that maps each unique token to a unique ID.</p>
    </li>
  </ol>

  <p class="body"><a class="calibre8" id="pgfId-1123759"></a>For example, if you have the sentence</p>
  <pre class="programlisting">the cat sat on the mat</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123761"></a>we will first tokenize this into words, resulting in</p>
  <pre class="programlisting">[the, cat, sat, on, the, mat]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123764"></a>and have the dictionary</p>
  <pre class="programlisting">{the: 1, cat: 2, sat: 3, on: 4, mat: 5}</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123767"></a>Then you can create the following sequence to represent the original text:</p>
  <pre class="programlisting">[1,2,3,4,1,5]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123770"></a>The Keras <span class="fm-code-in-text">Tokenizer</span> object<a class="calibre8" id="marker-1123769"></a> supports exactly this functionality. It takes a corpus of text, tokenizes it with some user-defined parameters, builds the dictionary automatically, and saves it as a state. This way, you can use the <span class="fm-code-in-text">Tokenizer</span> to convert any arbitrary text as many times as you like to numbers. Let’s look at how we can do this using the Keras <span class="fm-code-in-text">Tokenizer</span>:</p>
  <pre class="programlisting">from tensorflow.keras.preprocessing.text import Tokenizer
 
tokenizer = Tokenizer(
    num_words=n_vocab, 
    oov_token='unk', 
    lower=False, 
    filters='!"#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n',
    split=' ', 
    char_level=False
)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123781"></a>You can see there are several arguments passed to the <span class="fm-code-in-text">Tokenizer</span>. Let’s look at these arguments in a bit more detail:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1123782"></a><span class="fm-code-in-text">num_words</span>—This defines the vocabulary size to limit the size of the dictionary. If <span class="fm-code-in-text">num_words</span> is set to 1,000, it will consider the most common 1,000 words in the corpus and assign them unique IDs.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123783"></a><span class="fm-code-in-text">oov_token</span>—This argument treats the words that fall outside the defined vocabulary size. The words that appear in the corpus but are not captured within the most common <span class="fm-code-in-text">num_words</span> words will be replaced with this token.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123784"></a><span class="fm-code-in-text">lower</span>—This determines whether to perform case lowering on the text. Since we have already done that, we will set it to <span class="fm-code-in-text">False</span>.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123785"></a><span class="fm-code-in-text">filter</span>—This defines any character(s) you want removed from the text before tokenizing.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123786"></a><span class="fm-code-in-text">split</span>—This is the separator character that will be used to tokenize your text. We want individual words to be tokens; therefore, we will use space, as words are usually separated by a space.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1123787"></a><span class="fm-code-in-text">char_level</span>—This indicates whether to perform character-level tokenization (i.e., each character is a token).</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1123788"></a>Before we move forward, let’s remind ourselves what our data looks like in the current state. Remember that we have</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1123789"></a>Cleaned data</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123790"></a>Preprocessed data</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1123791"></a>Split each review into individual words</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1123792"></a>By the end of this process, we have data as shown. First, we have the input, which is a <span class="fm-code-in-text">pd.Series</span> object<a class="calibre8" id="marker-1123793"></a> that contains the list of clean words. The number in front of the text is the index of that record in the <span class="fm-code-in-text">pd.Series</span> object<a class="calibre8" id="marker-1123794"></a>:</p>
  <pre class="programlisting">122143    [work, perfectly, wii, gamecube, issue, compat...
444818    [loved, game, collectible, come, well, make, m...
79331     ['s, okay, game, honest, bad, type, game, --, ...
97250                        [excellent, product, describe]
324411        [level, detail, great, feel, love, car, game]
...
34481     [not, actually, believe, write, review, produc...
258474    [good, game, us, like, movie, franchise, hard,...
466203    [fun, first, person, shooter, nice, combinatio...
414288                       [love, amiibo, classic, color]
162670    [fan, halo, series, start, enjoy, game, overal...
Name: reviewText, dtype: object</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123807"></a>Next, we have the labels, where each label is a binary label to indicate whether the review is a positive review or a negative review:</p>
  <pre class="programlisting">122143    1
444818    1
79331     0
97250     1
324411    1
...
34481     1
258474    1
466203    1
414288    1
162670    0
Name: label, dtype: int64</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123821"></a>In a way, that first step of tokenizing the text has already happened. The Keras <span class="fm-code-in-text">Tokenizer</span> is smart enough to skip that step if it has already happened. To build the dictionary of the <span class="fm-code-in-text">Tokenizer</span>, you can call the <span class="fm-code-in-text">tf.keras.preprocessing.text.Tokenizer.fit_on_texts()</span> function<a class="calibre8" id="marker-1135839"></a>, as shown:</p>
  <pre class="programlisting">tokenizer.fit_on_texts(tr_x.tolist())</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123825"></a>The <span class="fm-code-in-text">fit_on_texts()</span> function<a class="calibre8" id="marker-1123824"></a> accepts a list of strings, where each string is a single entity of what you’re processing (e.g., a sentence, a review, a paragraph, etc.) or a list of lists of tokens, where a token can be a word, a character, or even a sentence. As you fit the <span class="fm-code-in-text">Tokenizer</span> on some text, you can inspect some of the internal state variables. You can check the word to ID mapping using</p>
  <pre class="programlisting">tokenizer.word_index[“game”]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123827"></a>which will return</p>
  <pre class="programlisting">2</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123829"></a>You also can check the ID to word mapping (i.e., the reverse operation of mapping a word to an ID) using</p>
  <pre class="programlisting">tokenizer.index_word[4]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123831"></a>which will return</p>
  <pre class="programlisting">“play”</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123833"></a>To convert a text corpus to a sequences of indices, you can use the <span class="fm-code-in-text">texts_to_sequences()</span> function<a class="calibre8" id="marker-1123834"></a>. It takes a list of lists of tokens and returns a list of lists of IDs:</p>
  <pre class="programlisting">tr_x = tokenizer.texts_to_sequences(tr_x.tolist())
v_x = tokenizer.texts_to_sequences(v_x.tolist())
ts_x = tokenizer.texts_to_sequences(ts_x.tolist())</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123840"></a>Let’s see some of the results of the <span class="fm-code-in-text">text_to_sequences()</span> function<a class="calibre8" id="marker-1123839"></a> that converted some samples:</p>
  <pre class="programlisting">Text: ['work', 'perfectly', 'wii', 'gamecube', 'issue', 'compatibility', 
<span class="fm-code-continuation-arrow">➥</span> 'loss', 'memory']
Sequence: [14, 295, 83, 572, 121, 1974, 2223, 345]
Text: ['loved', 'game', 'collectible', 'come', 'well', 'make', 'mask', 
<span class="fm-code-continuation-arrow">➥</span> 'big', 'almost', 'fit', 'face', 'impressive']
Sequence: [1592, 2, 2031, 32, 23, 16, 2345, 153, 200, 155, 599, 1133]
 
Text: ["'s", 'okay', 'game', 'honest', 'bad', 'type', 'game', '--', "'s", 
<span class="fm-code-continuation-arrow">➥</span> 'difficult', 'always', 'die', 'depresses', 'maybe', 'skill', 'would', 
<span class="fm-code-continuation-arrow">➥</span> 'enjoy', 'game']
Sequence: [5, 574, 2, 1264, 105, 197, 2, 112, 5, 274, 150, 354, 1, 290, 
<span class="fm-code-continuation-arrow">➥</span> 400, 19, 67, 2]
 
Text: ['excellent', 'product', 'describe']
Sequence: [109, 55, 501]
 
Text: ['level', 'detail', 'great', 'feel', 'love', 'car', 'game']
Sequence: [60, 419, 8, 42, 13, 265, 2]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123855"></a>Great! We can see that text is converted to ID sequences perfectly. We will now proceed to defining the TensorFlow pipeline using the data returned by the Keras <span class="fm-code-in-text">Tokenizer</span>.</p>

  <p class="fm-head2"><a id="pgfId-1123856"></a>Exercise 2</p>

  <p class="body"><a class="calibre8" id="pgfId-1123857"></a>Given the string <span class="fm-code-in-text">s</span>, <span class="fm-code-in-text">"a_b_B_c_d_a_D_b_d_d"</span>, can you define a tokenizer, <span class="fm-code-in-text">tok</span>, that lowers the text, splits by the underscore character “_”, has a vocabulary size of 3, and fits the <span class="fm-code-in-text">Tokenizer</span> on s. If the <span class="fm-code-in-text">Tokenizer</span> ignores the out-of-vocabulary index words starting from 1, what would be the output if you<a class="calibre8" id="marker-1123859"></a><a class="calibre8" id="marker-1123860"></a> call<a class="calibre8" id="marker-1123861"></a><a class="calibre8" id="marker-1123862"></a><a class="calibre8" id="marker-1123863"></a> <span class="fm-code-in-text">tok.texts_to_sequences([s])</span>?</p>

  <h2 class="fm-head" id="sigil_toc_id_124"><a id="pgfId-1123864"></a>9.3 Defining an end-to-end NLP pipeline with TensorFlow</h2>

  <p class="body"><a class="calibre8" id="pgfId-1123867"></a>You<a class="calibre8" id="marker-1123865"></a><a class="calibre8" id="marker-1123866"></a> have defined a clean data set that is in the numerical format the model expects it to be in. Here, we will define a TensorFlow data set pipeline to produce batches of data from the data we have defined. In the data pipeline, you will generate a batch of data, where the batch consists of a tuple <span class="fm-code-in-text">(x,y)</span>. <span class="fm-code-in-text">x</span> represents a batch of text sequences, where each text sequence is an arbitrarily long sequence of token IDs. <span class="fm-code-in-text">y</span> is a batch of labels corresponding to the text sequences in the batch. When generating a batch of examples, first the text sequences are assigned to buckets depending on the sequence length. Each bucket has a predefined allowed sequence length interval. Examples in a batch consist only of examples in the same bucket.</p>

  <p class="body"><a class="calibre8" id="pgfId-1123868"></a>We are now in a great position. We have done quite a lot of preprocessing on the data and have converted text to machine readable numbers. In the next step, we will build a <span class="fm-code-in-text">tf.data</span> pipeline<a class="calibre8" id="marker-1123869"></a> to convert the output of the <span class="fm-code-in-text">Tokenizer</span> to a model-friendly output.</p>

  <p class="body"><a class="calibre8" id="pgfId-1123870"></a>As the first step, we are going to concatenate the target label (having a value of 0/1) to the input. This way, we can shuffle the data in any way we want and still preserve the relationship between inputs and the target label:</p>
  <pre class="programlisting">    data_seq = [[b]+a for a,b in zip(text_seq, labels) ]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123873"></a>Next, we will create a special type of <span class="fm-code-in-text">tf.Tensor</span> object known as a <i class="fm-italics">ragged tensor</i> (i.e., <span class="fm-code-in-text">tf.RaggedTensor</span><a class="calibre8" id="marker-1123874"></a>). In a standard tensor, you have fixed dimensions. For example, if you define a 3 × 4-sized tensor, every single row needs to have four columns (i.e., four values). Ragged tensors are a special type of tensor that supports variable-sized tensors. For example, it is perfectly fine to have data like this as a ragged tensor:</p>
  <pre class="programlisting">[
  [1,2],
  [3,2,5,9,10],
  [3,2,3]
]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123882"></a>This tensor has three rows, where the first row has two values, the second five values, and the final row three values. In other words, it has a variable second dimension. This is a perfect data structure for our problem because each review has a different number of words, leading to variable-sized ID sequences corresponding to each review:</p>
  <pre class="programlisting">max_length = 50    
tf_data = tf.ragged.constant(data_seq)[:,:max_length]</pre>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1123887"></a>Primer on tf.RaggedTensor</p>

    <p class="fm-sidebar-text"><a id="pgfId-1123889"></a><span class="fm-code-in-text1">tf.RaggedTensor</span> objects<a id="marker-1132765"></a> are a special type of tensor that can have variable-sized dimensions. You can read more about ragged tensors at <span class="fm-hyperlink"><a class="url" href="http://mng.bz/5QZ8">http://mng.bz/5QZ8</a></span>. There are many ways to define a ragged tensor.</p>

    <p class="fm-sidebar-text"><a id="pgfId-1123890"></a>We can define a ragged tensor by passing a nested list containing values to the <span class="fm-code-in-text1">tf.ragged.constant()</span> function<a id="marker-1132770"></a>:</p>
    <pre class="programlisting">a = tf.ragged.constant([[1, 2, 3], [1,2], [1]])</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1123894"></a>You can also define a flat sequence of values and define where to split the rows:</p>
    <pre class="programlisting">b = tf.RaggedTensor.from_row_splits([1,2,3,4,5,6,7], row_splits=[0, 3, 3, 6, 7])</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1123896"></a>Here, each value in the <span class="fm-code-in-text1">row_splits</span> argument defines where subsequent rows in the resulting tensor end. For example, the first row will contain elements from index 0 to 3 (i.e., 0, 1, 2). This will output</p>
    <pre class="programlisting">&lt;tf.RaggedTensor [[1, 2, 3], [], [4, 5, 6], [7]]&gt;</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1123899"></a>You can get the shape of the tensor using <span class="fm-code-in-text1">b.shape</span>, which will return</p>
    <pre class="programlisting">[4, None]</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1123902"></a>You can even have multidimensional ragged tensors, where you have more than one variable-sized dimension as follows:</p>
    <pre class="programlisting">c = tf.RaggedTensor.from_nested_row_splits(
    flat_values=[1,2,3,4,5,6,7,8,9], 
    nested_row_splits=([0,2,3],[0,4,6,9]))</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1123907"></a>Here, the <span class="fm-code-in-text1">nested_row_splits</span> is a list of 1D tensors, where the <i class="fm-italics">i</i> <sup class="fm-superscript1">th</sup> tensor represents the row split for the <i class="fm-italics">i</i> <sup class="fm-superscript1">th</sup> dimension. <span class="fm-code-in-text1">c</span> will look as follows:</p>
    <pre class="programlisting">&lt;tf.RaggedTensor [[[1, 2, 3, 4], [5, 6]], [[7, 8, 9]]]&gt;</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1123910"></a>You can perform slicing and indexing on ragged tensors, similar to how you do on normal tensors:</p>
    <pre class="programlisting">print(c[:1, :, :])</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1123913"></a>This will return</p>
    <pre class="programlisting">&lt;tf.RaggedTensor [[[1, 2, 3, 4], [5, 6]]]&gt;</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1123917"></a>where</p>
    <pre class="programlisting">print(c[:,:1,:])</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1123921"></a>This will return</p>
    <pre class="programlisting">&lt;tf.RaggedTensor [[[1, 2, 3, 4]], [[7, 8, 9]]]&gt;</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1123924"></a>Finally, with</p>
    <pre class="programlisting">print(c[:, :, :2])</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1123927"></a>you will get</p>
    <pre class="programlisting">&lt;tf.RaggedTensor [[[1, 2], [5, 6]], [[7, 8]]]&gt;</pre>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1123930"></a>We will limit the maximum length of the reviews to <span class="fm-code-in-text">max_length</span>. This is done under the assumption that <span class="fm-code-in-text">max_length</span> words are adequate to capture the sentiment in a given review. This way, we can avoid final data being excessively long because of one or two extremely long comments present in the data. The higher the <span class="fm-code-in-text">max_length</span>, the better, in terms of capturing the information in the review. But a higher <span class="fm-code-in-text">max_length</span> value comes with a hefty price tag in terms of required computational power:</p>
  <pre class="programlisting">    text_ds = tf.data.Dataset.from_tensor_slices(tf_data)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123933"></a>We will create a data set using the <span class="fm-code-in-text">tf.data.Dataset.from_tensor_slices()</span> function<a class="calibre8" id="marker-1123934"></a>. This function on the ragged tensor, which we just created, will extract one row (i.e., a single review) at a time. It’s important to remember that each row will have a different size. We will filter any reviews that are empty. You could do this using the <span class="fm-code-in-text">tf.data.Dataset.filter()</span> function<a class="calibre8" id="marker-1123935"></a>:</p>
  <pre class="programlisting">text_ds = text_ds.filter(lambda x: tf.size(x)&gt;1)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123938"></a>Essentially, we are saying here that any review that has a size smaller than or equal to 1 will be discarded. Remember that each record will have at least a single element (which is the label). This is an important step because having empty reviews can cause problems in the model down the track.</p>

  <p class="body"><a class="calibre8" id="pgfId-1123939"></a>Next, we will address an extremely important step and the highlight of our impressive data pipeline. In sequence processing, you might have heard the term <i class="fm-italics">bucketing</i><a class="calibre8" id="marker-1123940"></a> (or <i class="fm-italics">binning</i><a class="calibre8" id="marker-1123941"></a>). Bucketing refers to, when batching data, using similar-sized inputs. In other words, a single batch of data includes similar-sized reviews and will not have reviews with drastically different lengths in the same batch. The following sidebar explains the process of bucketing in more detail.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1123942"></a>Bucketing: Similar length sequences stick together!</p>

    <p class="fm-sidebar-text"><a id="pgfId-1123943"></a>Let’s look at an example. Assume that you have a list of reviews, [r1(5), r2(11), r3(6), r4(4), r5(15), r6(18), r7(25), r8(29), r9(30)], where the code <span class="fm-code-in-text1">rx</span> represents the review ID and the number within brackets represents the number of words in the review. If you select a batch size of 3, it makes sense to batch data the following way:</p>
    <pre class="programlisting">[r1, r3, r4]
[r2, r5, r6]
[r7, r8, r9]</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1123948"></a>You can see that the similar-length reviews are batched together. This is implemented practically by a process known as bucketing. First, we create several buckets with predefined boundaries. For instance, in our example, there can be three buckets with the following intervals:</p>
    <pre class="programlisting">[[0,11), [11, 21), [21, inf)) </pre>

    <p class="fm-sidebar-text"><a id="pgfId-1123952"></a>Then, depending on the length of the review, each review is assigned to a bucket. Finally, when getting batches of data, a batch (randomly sampled) from a single bucket at random is selected.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1123953"></a>After identifying the buckets, we have to batch the data so that we end up with a fixed-sequence length. This is achieved by padding zeros to the end until we have all sequences in that batch with equal lengths. Let’s assume the reviews r1, r3, and r4 have the following word ID sequences:</p>
    <pre class="programlisting">[10, 12, 48, 21,  5]
[ 1, 93, 28,  8, 20, 10]
[32, 20,  1,  2]</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1123959"></a>To batch these sequences, we will pad zeros to the end of short sequences, resulting in</p>
    <pre class="programlisting">[10, 12, 48, 21,  5,  0]
[ 1, 93, 28,  8, 20, 10]
[32, 20,  1,  2,  0,  0]</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1123965"></a>You can see that now we have a batch of data with fixed-sequence lengths that can be converted to a <span class="fm-code-in-text1">tf.Tensor</span>.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1123967"></a>Fortunately, all we need to worry about in order to use bucketing is understanding the syntax of a convenient TensorFlow function that is already provided: <span class="fm-code-in-text">tf.data .experimental.bucket_by_sequence_length</span><a class="calibre8" id="marker-1123968"></a><span class="fm-code-in-text">()</span>. The experimental namespace is a special namespace allocated for TensorFlow functionality that has not been fully tested. In other words, there might be edge cases where these functions might fail. Once the functionality is well tested, these cases will move out of the experimental namespace into a stable one. Note that this function returns another function that performs the bucketing on a data set. Therefore, you have to use this function in conjunction with <span class="fm-code-in-text">tf.data.Dataset.apply()</span> in order to execute the returned function. The syntax can be slightly cryptic at first glance. But things will be clearer when we take a deeper look at the arguments. You can see that we’re using the bucket boundaries we identified earlier when analyzing the sequence lengths of the reviews:</p>
  <pre class="programlisting">bucket_boundaries=[5,15]
batch_size = 64
bucket_fn = tf.data.experimental.bucket_by_sequence_length(
        element_length_func = lambda x: tf.cast(tf.shape(x)[0],'int32'), 
        bucket_boundaries=bucket_boundaries, 
        bucket_batch_sizes=[batch_size,batch_size,batch_size], 
        padding_values=0, 
        pad_to_bucket_boundary=False
    )</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123978"></a>Let’s examine the arguments provided to this function:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1123979"></a><span class="fm-code-in-text">elment_length_func</span>—This is at the heart of the bucketing function as it tells the function how to compute the length of a single record or instance coming in. Without the length of the record, bucketing is impossible.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123980"></a><span class="fm-code-in-text">bucket_boundaries</span>—Defines the upper bound of bucket boundaries. This argument accepts a list of values in increasing order. If you provided <span class="fm-code-in-text">bucket_bounderies</span> [x, y, z], where x &lt; y &lt; z, then the bucket intervals would be [0, x), [x, y), [y, z), [z, inf).</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123981"></a><span class="fm-code-in-text">bucket_batch_sizes</span>—Batch size for each bucket. You can see that we have defined the same batch size for all the buckets. But you can also use other strategies, such as higher batch size for shorter sequences.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1123982"></a><span class="fm-code-in-text">padded_values</span>—This defines, when bringing sequences to the same length, what to pad the short sequences with. Padding with zero is a very common method. We will stick with that.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1123983"></a><span class="fm-code-in-text">pad_to_bucket_boundary</span>—This is a special Boolean argument that will decide the final size of the variable dimension of each batch. For example, assume you have a bucket with the interval [0, 11) and a batch of sequences with lengths [4, 8, 5]. If <span class="fm-code-in-text">pad_to_bucket_boundary=True</span>, the final batch will have the variable dimension of 10, which means every sequence is padded to the maximum limit. If <span class="fm-code-in-text">pad_to_bucket_boundary=False</span>, you’ll have the variable dimension of 8 (i.e., the length of the longest sequence in the batch).</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1123984"></a>Remember that we had a <span class="fm-code-in-text">tf.RaggedTensor</span> initially fed to the <span class="fm-code-in-text">tf.data.Dataset.from_tensor_slices</span> function<a class="calibre8" id="marker-1123985"></a>. When returning slices, it will return slices with the same data type. Unfortunately, <span class="fm-code-in-text">tf.RaggedTensor</span> objects<a class="calibre8" id="marker-1123986"></a> are not compatible with the bucketing function. Therefore, we perform the following hack to convert slices back to <span class="fm-code-in-text">tf.Tensor</span> objects. We simply call the map function with the lambda function <span class="fm-code-in-text">lambda x: x</span><a class="calibre8" id="marker-1123987"></a>. With that, you can call the <span class="fm-code-in-text">tf.data.Dataset.apply()</span> function<a class="calibre8" id="marker-1123988"></a> with the <span class="fm-code-in-text">bucket_fn</span> as the argument:</p>
  <pre class="programlisting">text_ds = text_ds.map(lambda x: x).apply(bucket_fn)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123991"></a>At this point, we have done all the hard work. By now, you have implemented the functionality to accept a data set with arbitrary length sequences and to sample a batch of sequences from that using the bucketing strategy. The bucketing/binning strategy used here makes sure that we don’t group sequences with large differences in their lengths, which will lead to excessive padding.</p>

  <p class="body"><a class="calibre8" id="pgfId-1123992"></a>As we have done many times, let’s shuffle the data to make sure we observe enough randomness during the training phase:</p>
  <pre class="programlisting">if shuffle:
    text_ds = text_ds.shuffle(buffer_size=10*batch_size)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1123995"></a>Remember that we combined the target label and the input to ensure correspondence between inputs and targets. Now we can safely split the target and input into two separate tensors using tensor slicing syntax as shown:</p>
  <pre class="programlisting">text_ds = text_ds.map(lambda x: (x[:,1:], x[:,0]))    </pre>

  <p class="body"><a class="calibre8" id="pgfId-1123997"></a>Now we can let out a sigh of relief. We have completed all the steps of the journey from raw unclean text to clean semi-structured text that can be consumed by our model. Let’s wrap this in a function called <span class="fm-code-in-text">get_tf_pipeline()</span>, which takes a <span class="fm-code-in-text">text_seq</span> (list of lists of word IDs), <span class="fm-code-in-text">labels</span> (list of integers), <span class="fm-code-in-text">batch_size</span> (int), <span class="fm-code-in-text">bucket_boundaries</span> (list of ints), <span class="fm-code-in-text">max_length</span> (int), and <span class="fm-code-in-text">shuffle</span> (Boolean) arguments (see the following listing).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1124003"></a>Listing 9.4 The <span class="fm-code-in-listingcaption">tf.data</span> pipeline</p>
  <pre class="programlisting">def get_tf_pipeline(
    text_seq, labels, batch_size=64, bucket_boundaries=[5,15], 
<span class="fm-code-continuation-arrow">➥</span> max_length=50, shuffle=False
):
    """ Define a data pipeline that converts sequences to batches of data """
        
    data_seq = [[b]+a for a,b in zip(text_seq, labels) ]               <span class="fm-combinumeral">❶</span>
    
    tf_data = tf.ragged.constant(data_seq)[:,:max_length]              <span class="fm-combinumeral">❷</span>
    
    text_ds = tf.data.Dataset.from_tensor_slices(tf_data)              <span class="fm-combinumeral">❸</span>
    
    bucket_fn = tf.data.experimental.bucket_by_sequence_length(        <span class="fm-combinumeral">❹</span>
        lambda x: tf.cast(tf.shape(x)[0],'int32'), 
        bucket_boundaries=bucket_boundaries,                           <span class="fm-combinumeral">❺</span>
        bucket_batch_sizes=[batch_size,batch_size,batch_size], 
        padded_shapes=None,
        padding_values=0, 
        pad_to_bucket_boundary=False
    )
 
    text_ds = text_ds.map(lambda x: x).apply(bucket_fn)                <span class="fm-combinumeral">❻</span>
    
    if shuffle:
        text_ds = text_ds.shuffle(buffer_size=10*batch_size)           <span class="fm-combinumeral">❼</span>
        
    text_ds = text_ds.map(lambda x: (x[:,1:], x[:,0]))                 <span class="fm-combinumeral">❽</span>
    
    return text_ds</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1139649"></a><span class="fm-combinumeral">❶</span> Concatenate the label and the input sequence so that we don’t mess up the order when we shuffle.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1139677"></a><span class="fm-combinumeral">❷</span> Define the variable sequence data set as a ragged tensor.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1139694"></a><span class="fm-combinumeral">❸</span> Create a data set out of the ragged tensor.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1139711"></a><span class="fm-combinumeral">❹</span> Bucket the data (assign each sequence to a bucket depending on the length).</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1139728"></a><span class="fm-combinumeral">❺</span> For example, for bucket boundaries [5, 15], you get buckets [0, 5], [5, 15], [15,inf].</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1139745"></a><span class="fm-combinumeral">❻</span> Apply bucketing.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1139762"></a><span class="fm-combinumeral">❼</span> Shuffle the data.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1139779"></a><span class="fm-combinumeral">❽</span> Split the data to inputs and labels.</p>

  <p class="body"><a class="calibre8" id="pgfId-1124042"></a>It’s been a long journey. Let’s reflect on what we have done so far. With the data pipeline done and dusted, let’s learn about the models that can consume this type of sequential data. Next, we will define the sentiment analyzer model that we’ve been waiting to implement.</p>

  <p class="fm-head2"><a id="pgfId-1124043"></a>Exercise 3</p>

  <p class="body"><a class="calibre8" id="pgfId-1124044"></a>If you want have the buckets (0, 10], (10, 25], (25, 50], [50, inf) and always return padded to the boundary of the bucket, how would you modify this bucketing function? Note that the number of buckets has changed from the number we have in the<a class="calibre8" id="marker-1124045"></a><a class="calibre8" id="marker-1124046"></a> text.</p>

  <h2 class="fm-head" id="sigil_toc_id_125"><a id="pgfId-1124047"></a>9.4 Happy reviews mean happy customers: Sentiment analysis</h2>

  <p class="body"><a class="calibre8" id="pgfId-1124051"></a>Imagine<a class="calibre8" id="marker-1124048"></a><a class="calibre8" id="marker-1124049"></a><a class="calibre8" id="marker-1124050"></a> you have converted the reviews to numbers and defined a data pipeline that generates batches of inputs and labels. Now it’s time to crunch them using a model to train a model that can accurately identify sentiments in a posted review. You have heard that long short-term memory models (LSTMs) are a great starting point for processing textual data. The goal is to implement a model based on LSTMs that produces one of two possible outcomes for a given review: a negative or a positive sentiment.</p>

  <p class="body"><a class="calibre8" id="pgfId-1124052"></a>If you have reached this point, you should be happy. You have ploughed through a lot. Now it’s time to reward yourself with information about a compelling family of models known as deep sequential models. Some example models of this family are as follows:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1124055"></a>Simple recurrent neural networks (RNNs<a class="calibre8" id="marker-1124054"></a>)</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1124056"></a>Long short-term memory (LSTM) networks</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1124059"></a>Gated recurrent units (GRUs<a class="calibre8" id="marker-1124057"></a><a class="calibre8" id="marker-1124058"></a>)</p>
    </li>
  </ul>

  <h3 class="fm-head1" id="sigil_toc_id_126"><a id="pgfId-1124060"></a>9.4.1 LSTM Networks</h3>

  <p class="body"><a class="calibre8" id="pgfId-1124064"></a>Previously<a class="calibre8" id="marker-1124061"></a><a class="calibre8" id="marker-1124062"></a><a class="calibre8" id="marker-1124063"></a> we discussed a simple recurrent neural network and its application in predicting CO2 concentration levels in the future. In this chapter, we will look at the mechanics of LSTM networks. LSTMs models were very popular for almost a decade. They are a great choice for processing sequential data and typically have three important dimensions:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1124065"></a>A batch dimension</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1124066"></a>A time dimension</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1124067"></a>A feature dimension</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1124068"></a>If you think about the data returned by the NLP pipeline we discussed, it has all these dimensions. The batch dimension is represented by each different review sampled into that batch. The time dimension is represented by the sequence of word IDs appearing in a single review. Finally, you can think of feature dimension being 1, as a single feature is represented by a single numerical value (i.e., an ID; see figure 9.2). The feature dimension has values corresponding to features on that dimension. For example, if you have a weather model that has three features (e.g., temperature, precipitation, wind speed), the input to the model would be <span class="fm-code-in-text">[&lt;batch size&gt;, &lt;sequence length&gt;, 3]</span>-sized input.</p>

  <p class="fm-figure"><img alt="09-02" class="calibre10" src="../../OEBPS/Images/09-02.png" width="581" height="311"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1142181"></a>Figure 9.2 3D view of sequential data. Typically, sequential data is found with three dimensions; batch size, sequence/time, and feature.</p>

  <p class="body"><a class="calibre8" id="pgfId-1124075"></a>The LSTM takes an input with the three dimensions discussed. Let’s zoom in a bit more to see how an LSTM operates on such data. To keep the discussion simple, assume a batch size of 1 or a single review. If we assume a single review r that has n words it can be written as</p>

  <p class="fm-equation"><a id="pgfId-1124076"></a><i class="fm-italics">r</i> = <i class="fm-italics">w</i><sub class="fm-subscript">1,</sub><i class="fm-italics">w</i><sub class="fm-subscript">2,...,</sub><i class="fm-italics">w</i><sub class="fm-subscript">t,...,</sub><i class="fm-italics">w</i><sub class="fm-subscript">n</sub>, where <i class="fm-italics">w</i><sub class="fm-subscript">t</sub> represents the ID of the word in the <i class="fm-italics">t</i><sup class="fm-superscript">th</sup> position.</p>

  <p class="body"><a class="calibre8" id="pgfId-1124077"></a>At time step t, the LSTM model starts with</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1124078"></a>The previous output state vector <i class="fm-timesitalic1">h</i><sub class="fm-subscript">t-1</sub></p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1124079"></a>The previous cell state vector <i class="fm-timesitalic1">c</i><sub class="fm-subscript">t-1</sub></p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1124080"></a>and computes</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1124081"></a>The current cell state <i class="fm-timesitalic1">c</i><sub class="fm-subscript">t</sub> using the current input <i class="fm-timesitalic1">w</i><sub class="fm-subscript">t</sub> and the previous cell state <i class="fm-timesitalic1">c</i><sub class="fm-subscript">t-1</sub> and output state <i class="fm-timesitalic1">h</i><sub class="fm-subscript">t-1</sub> and</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1124082"></a>The current output state <i class="fm-timesitalic1">h</i><sub class="fm-subscript">t</sub> using the current input <i class="fm-timesitalic1">w</i><sub class="fm-subscript">t</sub> and the previous state <i class="fm-timesitalic1">h</i><sub class="fm-subscript">t-1</sub> and the current cell state <i class="fm-timesitalic1">c</i><sub class="fm-subscript">t</sub></p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1124083"></a>This way, the model keeps iterating over all the timesteps (in our example, it’s word IDs in the sequence) until it reaches the end. While iterating this way, the model keeps producing a cell state and an output state (figure 9.3).</p>

  <p class="fm-figure"><img alt="09-03" class="calibre10" src="../../OEBPS/Images/09-03.png" width="753" height="278"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1142215"></a>Figure 9.3 High-level longitudinal view of an LSTM cell. At a given time step t, the LSTM cell takes in two previous states (<i class="fm-timesitalic">h</i><sub class="fm-subscript">t-1</sub> and <i class="fm-timesitalic">c</i><sub class="fm-subscript">t-1</sub>), along with the input, and produces two states (<i class="fm-timesitalic">h</i><sub class="fm-subscript">t</sub> and <i class="fm-timesitalic">c</i><sub class="fm-subscript">t</sub>).</p>

  <p class="body"><a class="calibre8" id="pgfId-1124090"></a>With a good high-level understanding of the LSTM cell, let’s look at the equations that are cranking the gears of this model. The LSTM takes three inputs:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1124091"></a><i class="fm-timesitalic1">x</i><sub class="fm-subscript">t</sub>—The input at timestep <i class="fm-timesitalic1">t</i></p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1124092"></a><i class="fm-timesitalic1">h</i><sub class="fm-subscript">t-1</sub>—The output state at timestep <i class="fm-timesitalic1">t</i>-1</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1124093"></a><i class="fm-timesitalic1">c</i><sub class="fm-subscript">t-1</sub>—The cell state at timestep <i class="fm-timesitalic1">t</i>-1</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1124094"></a>With that, the LSTM produces two outputs:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1124095"></a>(<i class="fm-timesitalic1">c</i><sub class="fm-subscript">t</sub>)—The cell state at timestep <i class="fm-timesitalic1">t</i></p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1124096"></a>(<i class="fm-timesitalic1">h</i><sub class="fm-subscript">t</sub>)—The output state at timestep <i class="fm-timesitalic1">t</i></p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1124097"></a>To produce these outputs, the LSTM model leverages a gating mechanism. These gates determine how much information flows through them to the next stage of computations. The LSTM cell has three gates:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1124098"></a><i class="fm-italics">An input gate</i> (<i class="fm-timesitalic1">i</i><sub class="fm-subscript">t</sub>)—Determines how much of the current input will affect the subsequent computations</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1124099"></a><i class="fm-italics">A forget gate</i> (<i class="fm-timesitalic1">f</i><sub class="fm-subscript">t</sub>)—Determines how much of the previous cell state is discarded when computing the new cell state</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1124100"></a><i class="fm-italics">An output gate</i> (<i class="fm-timesitalic1">o</i><sub class="fm-subscript">t</sub>)—Determines how much of the current cell state contributes to the final output</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1124101"></a>These gates are made of trainable weights. This means that when training an LSTM model on a certain task, the gating mechanism will be jointly optimized to produce the optimal flow of information required to solve that task.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1124102"></a>What carries long-term and short-term memories in LSTMs?</p>

    <p class="fm-sidebar-text"><a id="pgfId-1124103"></a>The cell state preserves the long-term information and relationships as the model progresses through the input over the time dimension. In fact, it has been found that LSTMs can remember up to hundreds of timesteps when learning time-series problems.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1124104"></a>On the other hand, the output state can be thought as the short-term memory where it will look at the input, the long-term memory stored in the cell state, and decide the optimal amount of information required at that stage of the computation.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1124106"></a>You might still be wondering, “What is this gating mechanism actually achieving?” Let me illustrate that with a sentence. To solve almost all NLP tasks, capturing syntactic and semantic information as well parsing dependencies correctly in a given text input is imperative. Let’s see how an LSTM can help us to achieve that. Assume you’ve been given the sentence:</p>

  <p class="fm-quote1"><a id="pgfId-1124108"></a>the dog ran after the green ball and it got tired and barked at a plane</p>

  <p class="body"><a class="calibre8" id="pgfId-1124109"></a>In your mind, picture an LSTM model hopping from one word to another, processing them. Then assume you query the model for answers to questions at various stages of processing the sentence. Say you ask the question “Who ran?” while it’s processing the phrase “the dog ran.” The model will probably have its input gate widely open to absorb as much information as the model can, because the model starts with no prior knowledge about what language looks like. And if you think about it, the model does not really need to pay attention to its memory because the answer is one word away from the word “ran.”</p>

  <p class="body"><a class="calibre8" id="pgfId-1124110"></a>Next, you ask “Who got tired?” When processing “it got tired,” the model might want to tap into its cell state instead of focusing on the input, as the only clue in this phrase is “it.” If the model is to identify the relationship between it and dog, it will need to close the input gate slightly and open the forget gate so that more information flows from the past memory (about the dog) into the current memory.</p>

  <p class="body"><a class="calibre8" id="pgfId-1124112"></a>Finally, let’s say you ask “What was barked at?” by the time the model reaches the “barked at a plane” section. To produce the final output, you don’t need much information from past memory, so you might tighten the output gate to avoid too much information coming from the past memory. I hope these walkthroughs were useful in order to grok the purpose of these gates. Remember that this is just an analogy to understand the purpose of these gates. But in practice, the actual behavior can differ. It is also worth noticing that these gates are not binary; rather, the output of the gate is controlled by a sigmoidal function, meaning that it leads to a soft open/close state at a given time rather than a hard open/close state.</p>

  <p class="body"><a class="calibre8" id="pgfId-1124114"></a>To complete our discussion, let’s inspect the equations that drive the computations in an LSTM cell. But you don’t have to memorize or understand these equations in detail, as it’s not a requirement to use LSTMs. But to make our discussion holistic, let’s look at them. The first computation computes the input gate:</p>

  <p class="fm-equation"><a id="pgfId-1124115"></a><i class="fm-italics">i</i><sub class="fm-subscript">t</sub> <i class="fm-italics">= σ</i>(<i class="fm-italics">W</i><sub class="fm-subscript">ih</sub><i class="fm-italics">h</i><sub class="fm-subscript">t-1</sub> <i class="fm-italics">+ W</i><sub class="fm-subscript">ix</sub><i class="fm-italics">x</i><sub class="fm-subscript">t</sub> <i class="fm-italics">+ b</i><sub class="fm-subscript">f</sub>)</p>

  <p class="body"><a class="calibre8" id="pgfId-1124116"></a>Here, <i class="fm-timesitalic">W</i><sub class="fm-subscript">ih</sub> and <i class="fm-timesitalic">W</i><sub class="fm-subscript">ix</sub> are trainable weights that produce the gate value, where <i class="fm-timesitalic">b</i><sub class="fm-subscript">i</sub> is the bias. The computations here closely resemble the computations of a fully connected layer. This gate results in a vector for a given input whose values are between 0 and 1. You can see its resemblance to a gate (assuming 0 means closed and 1 means open). The rest of the gates follow a similar pattern of computations. The forget gate is computed as</p>

  <p class="fm-equation"><a id="pgfId-1124117"></a><i class="fm-italics">f</i><sub class="fm-subscript">t</sub> <i class="fm-italics">= σ</i>(<i class="fm-italics">W</i><sub class="fm-subscript">fh</sub><i class="fm-italics">h</i><sub class="fm-subscript">t-1</sub> <i class="fm-italics">+ W</i><sub class="fm-subscript">fx</sub><i class="fm-italics">x</i><sub class="fm-subscript">t</sub> <i class="fm-italics">+ b</i><sub class="fm-subscript">f</sub>)</p>

  <p class="body"><a class="calibre8" id="pgfId-1124118"></a>Then the cell state is computed. Cell state is computed from a two-fold computation:</p>

  <p class="fm-equation"><a id="pgfId-1128938"></a><i class="fm-italics">C̃</i><sub class="fm-subscript">t</sub> = tanh(<i class="fm-italics">W</i><sub class="fm-subscript">ch</sub> <i class="fm-italics">h</i><sub class="fm-subscript">t</sub><sub class="fm-subscript">-1</sub> + <i class="fm-italics">W</i><sub class="fm-subscript">cx</sub><i class="fm-italics">x</i><sub class="fm-subscript">t</sub> = <i class="fm-italics">b</i><sub class="fm-subscript">c</sub>)</p>

  <p class="fm-equation"><a id="pgfId-1129060"></a><i class="fm-italics">c</i><sub class="fm-subscript">t</sub> = <i class="fm-italics">f</i><sub class="fm-subscript">t</sub><i class="fm-italics">h</i><sub class="fm-subscript">t</sub><sub class="fm-subscript">-1</sub> + <i class="fm-italics">i</i><sub class="fm-subscript">t</sub><i class="fm-italics">C̃</i><sub class="fm-subscript">t</sub></p>

  <p class="body"><a class="calibre8" id="pgfId-1124127"></a>The computation is quite intuitive. It uses the forget gate to control the previous cell state, where it uses the input gate to control <i class="fm-timesitalic">C̃</i><sub class="fm-subscript">t</sub> computed using <i class="fm-timesitalic">x</i><sub class="fm-subscript">t</sub> (the current input). Finally, the output gate and the state are computed as</p>

  <p class="fm-equation"><a id="pgfId-1124131"></a><i class="fm-italics">o</i><sub class="fm-subscript">t</sub> <i class="fm-italics">= σ</i>(<i class="fm-italics">W</i><sub class="fm-subscript">oh</sub><i class="fm-italics">h</i><sub class="fm-subscript">t-1</sub> <i class="fm-italics">+ W</i><sub class="fm-subscript">ox</sub><i class="fm-italics">x</i><sub class="fm-subscript">t</sub> <i class="fm-italics">+ b</i><sub class="fm-subscript">0</sub>)</p>

  <p class="fm-equation"><a id="pgfId-1124132"></a><i class="fm-italics">h</i><sub class="fm-subscript">t</sub> <i class="fm-italics">= o</i><sub class="fm-subscript">t</sub><i class="fm-italics">tanh</i>(<i class="fm-italics">c</i><sub class="fm-subscript">t</sub>)</p>

  <p class="body"><a class="calibre8" id="pgfId-1124133"></a>Here, <i class="fm-timesitalic">c</i><sub class="fm-subscript">t</sub> is computed using the inputs controlled via the forget gate and the input gate. Therefore, in a way, <i class="fm-timesitalic">o</i><sub class="fm-subscript">t</sub> gets to control how much the current input, the current cell state, the previous cell state, and the previous output state contribute to the final state output of the LSTM cell. In TensorFlow and Keras, you can define an LSTM with</p>
  <pre class="programlisting">import tensorflow as tf
tf.keras.layers.LSTM(units=128, return_state=False, return_sequences=False)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124136"></a>The first argument <span class="fm-code-in-text">units</span> is a hyperparameter of the LSTM layer. Similar to how the number of units defines the output size of a fully connected layer, the units argument defines the output, state, and gate vector dimensionality. The higher this number, the more representative power the model possesses. Next, the <span class="fm-code-in-text">return_state=False</span> means only the output state will be returned when the layer is called on an input. If <span class="fm-code-in-text">return_state=True</span>, both the cell state and the output state are returned. Finally, <span class="fm-code-in-text">return_sequences=False</span> means that only the final state(s) after processing the whole sequence is returned. If <span class="fm-code-in-text">return_sequences=True</span>, all the state(s) returned during processing every element in the sequence are returned. Figure 9.4 depicts the differences in these arguments’ results.</p>

  <p class="fm-figure"><img alt="09-04" class="calibre10" src="../../OEBPS/Images/09-04.png" width="1061" height="1133"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1142249"></a>Figure 9.4 The changes to the output of the LSTM layer resulted in changes to the <span class="fm-code-in-figurecaption">return_state</span> and <span class="fm-code-in-figurecaption">return_sequences</span> arguments.</p>

  <p class="body"><a class="calibre8" id="pgfId-1124143"></a>Next, let’s define the final model.</p>

  <h3 class="fm-head1" id="sigil_toc_id_127"><a id="pgfId-1124144"></a>9.4.2 Defining the final model</h3>

  <p class="body"><a class="calibre8" id="pgfId-1124148"></a>We will define the final<a class="calibre8" id="marker-1124145"></a><a class="calibre8" id="marker-1124146"></a><a class="calibre8" id="marker-1124147"></a> model using the Sequential API. Our model will have the following layers (figure 9.5):</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1124150"></a><i class="fm-italics">A masking layer</i><a class="calibre8" id="marker-1124149"></a>—This layer plays an important role in which input elements in the sequence will contribute to the training. We<a class="calibre8" id="marker-1124151"></a><a class="calibre8" id="marker-1124152"></a> will learn more about this soon.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1124153"></a><i class="fm-italics">A one-hot encoding layer</i>—This layer will convert the word IDs to one-hot encoded sequences. This is an important transformation we have to perform before feeding our inputs to the model.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1124154"></a><i class="fm-italics">An LSTM layer</i>—The LSTM layer will return the final output state as the output.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1124155"></a><i class="fm-italics">A</i> <span class="fm-code-in-text">Dense</span> <i class="fm-italics">layer with 512 nodes (ReLU activation)</i>—A <span class="fm-code-in-text">Dense</span> layer takes the output of the LSTM cell and produces an interim hidden output.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1124157"></a><i class="fm-italics">A</i> <span class="fm-code-in-text">Dropout</span> <i class="fm-italics">layer</i><a class="calibre8" id="marker-1124156"></a>—<span class="fm-code-in-text">Dropout</span> is a regularization technique that randomly switches outputs during the training process. We discussed the purpose of <span class="fm-code-in-text">Dropout</span> and how it works in chapter 7.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1124159"></a><i class="fm-italics">A final output layer</i><a class="calibre8" id="marker-1124158"></a> <i class="fm-italics">with a single node (sigmoid activation)</i>—Note that we only need a single node to represent the output. If the value of the output is 0, it’s a negative sentiment. If the value is 1, it’s a positive sentiment.</p>
    </li>
  </ul>

  <p class="fm-figure"><img alt="09-05" class="calibre10" src="../../OEBPS/Images/09-05.png" width="983" height="617"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1142290"></a>Figure 9.5 The high-level model architecture of the sentiment analyzer</p>

  <p class="body"><a class="calibre8" id="pgfId-1124167"></a>Our <span class="fm-code-in-text">tf.data</span> pipeline<a class="calibre8" id="marker-1124166"></a> produces a <span class="fm-code-in-text">[&lt;batch size&gt;, &lt;sequence length&gt;]</span>-shaped 2D tensor. In practice, they would both be <span class="fm-code-in-text">None</span>. In other words, it will be a <span class="fm-code-in-text">[None, None]</span>-sized tensor as we have to support variable-sized batches and variable-sized sequence lengths in our model. A dimension of size <span class="fm-code-in-text">None</span> means that the model can accept any sized tensor on that dimension. For example, with a [<span class="fm-code-in-text">None, None</span>] tensor, when actual data is retrieved, it can be a [5, 10]-, [12, 54]-, or [102, 14]-sized tensor. As the entry point to the model, we will use a reshaping layer wrapped in a lambda layer as follows:</p>
  <pre class="programlisting">tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=(None,)),</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124170"></a>This layer takes our [<span class="fm-code-in-text">None, None</span>] input produced by the data pipeline and reshapes it to a [<span class="fm-code-in-text">None, None</span>, 1]-sized tensor. This reshaping is necessary for the next layer in line, making it a perfect opportunity to discuss the next layer. The next layer is a masking layer that serves a very special purpose. We have not seen a masking layer used in previous chapters. However, masking is commonly used in NLP problems. The need for masking arises from the padding operation we perform on the inputs during the bucketing of input sequences. In NLP data sets, you will seldom see text appearing with a fixed length. Typically, each text record has a different length. To batch these variable-sized text records together for the model, padding plays an essential role. Figure 9.6 illustrates what the data looks like after padding.</p>

  <p class="fm-figure"><img alt="09-06" class="calibre10" src="../../OEBPS/Images/09-06.png" width="708" height="294"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1142324"></a>Figure 9.6 Text sequences before and after padding</p>

  <p class="body"><a class="calibre8" id="pgfId-1124177"></a>But this introduces an extra burden. The values introduced by padding (typically zero) do not carry any information. Therefore, they should be ignored in any computation that happens in the model. For example, the LSTM model should halt processing and return that last state just before encountering padded values when padding is used in the input. The <span class="fm-code-in-text">tf.keras.layers.Masking</span> layer<a class="calibre8" id="marker-1124178"></a> helps us to do exactly that. The input to the masking layer must be a <span class="fm-code-in-text">[batch size, sequence length, feature dimension]</span>-sized 3D tensor. This alludes to our last point about reshaping the output of our <span class="fm-code-in-text">tf.data</span> pipeline<a class="calibre8" id="marker-1124179"></a> to a 3D tensor. In TensorFlow you define a mask as follows:</p>
  <pre class="programlisting">tf.keras.layers.Masking(mask_value=0)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124182"></a>The masking layer creates a special mask, and this mask is propagated to the subsequent layers in the model. Layers like the LSTM layer know what to do if a mask is passed from a layer below. More specifically, the LSTM model will output its state values just before it encountered zeros (if a mask is provided). It is also worth paying attention to the <span class="fm-code-in-text">input_shape</span> argument. The input to our model will be a two-dimensional tensor: an arbitrary-sized batch with an arbitrary-sized sequence length (due to bucketing). Therefore, we cannot specify a sequence length in the <span class="fm-code-in-text">input_shape</span> argument, so the model expects a <span class="fm-code-in-text">(None, None, 1)</span>-sized tensor as the input (the extra <span class="fm-code-in-text">None</span> is added automatically to represent the batch dimension).</p>

  <p class="body"><a class="calibre8" id="pgfId-1124183"></a>With the mask defined, we will convert the word IDs to one-hot vectors using a custom layer. This is an essential step before feeding data to the LSTM. This can be achieved as follows:</p>
  <pre class="programlisting">class OnehotEncoder(tf.keras.layers.Layer):
    def __init__(self, depth, **kwargs):
        super(OnehotEncoder, self).__init__(**kwargs)
        self.depth = depth
 
 
    def build(self, input_shape):
        pass
 
 
    def call(self, inputs):        
        
        inputs = tf.cast(inputs, 'int32')
        
        if len(inputs.shape) == 3:
            inputs = inputs[:,:,0]
  
        return tf.one_hot(inputs, depth=self.depth)
 
            
    def compute_mask(self, inputs, mask=None):
        return mask
 
 
    def get_config(self):
        config = super().get_config().copy()
        config.update({'depth': self.depth})
        return config</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124213"></a>Then call it with</p>
  <pre class="programlisting">OnehotEncoder(depth=n_vocab),</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124216"></a>The layer is a mouthful, so let’s break it down. First you define a user-defined parameter called <span class="fm-code-in-text">depth</span>. This defines the feature dimension of the final result. Next, you have to define the <span class="fm-code-in-text">call()</span> function<a class="calibre8" id="marker-1124217"></a>. The <span class="fm-code-in-text">call()</span> function<a class="calibre8" id="marker-1124218"></a> takes in the inputs, casts them to <span class="fm-code-in-text">'int32'</span>, and then removes the final dimension if the input is three-dimensional. This is because the masking layer we defined has a dimension of size 1 to represent the feature dimension. This dimension is not understood by the <span class="fm-code-in-text">tf.one_hot()</span> function<a class="calibre8" id="marker-1124219"></a> that we use to generate one-hot encoded vectors. Therefore, it must be removed. Finally, we return the result of the <span class="fm-code-in-text">tf.one_hot()</span> function<a class="calibre8" id="marker-1124220"></a>. Remember to provide the <span class="fm-code-in-text">depth</span> parameter when using <span class="fm-code-in-text">tf.one_hot()</span>. If it is not provided, TensorFlow tries to automatically infer the value, which leads to inconsistently sized tensors between different batches. We define the <span class="fm-code-in-text">compute_mask()</span> function to make sure we propagate the mask to the next layer. The layer simply takes the mask and passes it to the next layer. Finally, we define a <span class="fm-code-in-text">get_config()</span> function to update the parameters in that layer. It is essential for config to return the correct set of parameters; otherwise, you will run into problems saving the model. We define the LSTM layer as the next layer of the model:</p>
  <pre class="programlisting">tf.keras.layers.LSTM(units=128, return_state=False, return_sequences=False)</pre>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1124225"></a>More about propagating masks within models</p>

    <p class="fm-sidebar-text"><a id="pgfId-1124226"></a>It is important to remember a few things when using a masking layer. First, it is better to avoid using lambda layers in your model when using masking. This is because there have been several issues raised when using masking in conjunction with lambda layers (e.g., <span class="fm-hyperlink"><a class="url" href="https://github.com/tensorflow/tensorflow/issues/40085">https://github.com/tensorflow/tensorflow/issues/40085</a></span>). The best option is to write a custom layer as we have done. After defining a custom layer, you have to override the <span class="fm-code-in-text1">compute_mask()</span> function to return the mask (with modifications, if required) for the next layer.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1124230"></a>We have to be extra careful here. Depending on the arguments you provide when defining this layer, you will get vastly different outputs. To define our sentiment analyzer, we only want the final output state of the model. This means we’re not interested in the cell state, nor all the output states computed during processing the sequence. Therefore, we have to set the arguments accordingly. According to our requirements, we must set <span class="fm-code-in-text">return_state=False</span> and <span class="fm-code-in-text">return_sequences=False</span>. Finally, the final state output goes to a <span class="fm-code-in-text">Dense</span> layer<a class="calibre8" id="marker-1124231"></a> with 512 units and ReLU activation:</p>
  <pre class="programlisting">tf.keras.layers.Dense(512, activation='relu'),</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124235"></a>The dense layer is followed by a <span class="fm-code-in-text">Dropout</span> layer<a class="calibre8" id="marker-1124234"></a> that will drop 50% of the inputs of the previous <span class="fm-code-in-text">Dense</span> layer<a class="calibre8" id="marker-1124236"></a> during training.</p>
  <pre class="programlisting">tf.keras.layers.Dropout(0.5)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124240"></a>Finally, the model is crowned with a <span class="fm-code-in-text">Dense</span> layer<a class="calibre8" id="marker-1124239"></a> having a single unit and sigmoidal activation, which will produce the final prediction. If the produced value is less than 0.5, it is considered label 0 and 1 otherwise:</p>
  <pre class="programlisting">tf.keras.layers.Dense(1, activation='sigmoid')</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124243"></a>We can define the full model as shown in the next listing.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1124245"></a>Listing 9.5 Implementation of the full sentiment analysis model</p>
  <pre class="programlisting">model = tf.keras.models.Sequential([
    
    tf.keras.layers.Masking(mask_value=0.0, input_shape=(None,1)),         <span class="fm-combinumeral">❶</span>
    OnehotEncoder(depth=n_vocab),                                          <span class="fm-combinumeral">❷</span>
    tf.keras.layers.LSTM(128, return_state=False, return_sequences=False), <span class="fm-combinumeral">❸</span>
    tf.keras.layers.Dense(512, activation='relu'),                         <span class="fm-combinumeral">❹</span>
    tf.keras.layers.Dropout(0.5),                                          <span class="fm-combinumeral">❺</span>
    tf.keras.layers.Dense(1, activation='sigmoid')                         <span class="fm-combinumeral">❻</span>
])</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1139240"></a><span class="fm-combinumeral">❶</span> Create a mask to mask out zero inputs.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1139257"></a><span class="fm-combinumeral">❷</span> After creating the mask, convert inputs to one-hot encoded inputs.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1139284"></a><span class="fm-combinumeral">❸</span> Define an LSTM layer that returns the last state output vector (from unmasked inputs).</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1139301"></a><span class="fm-combinumeral">❹</span> Define a Dense layer with ReLU activation.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1139318"></a><span class="fm-combinumeral">❺</span> Define a Dropout layer with 50% dropout.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1139335"></a><span class="fm-combinumeral">❻</span> Define a final prediction layer with a single node and sigmoidal activation.</p>

  <p class="body"><a class="calibre8" id="pgfId-1124261"></a>Next, we’re off to compiling the model. Again, we have to be careful about the loss function we will be using. So far, we have used the <span class="fm-code-in-text">categorical_crossentropy</span> loss<a class="calibre8" id="marker-1124262"></a>. This loss is used for multiclass classification problems (greater than two classes). Since we’re solving a binary classification problem, we must switch to <span class="fm-code-in-text">binary_crossentropy</span> instead. Using the wrong loss function can lead to numerical instabilities and inaccurately trained models:</p>
  <pre class="programlisting">model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124264"></a>Finally, let’s examine the model by printing out the summary by running <span class="fm-code-in-text">model.summary()</span>:</p>
  <pre class="programlisting">Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
masking (Masking)            (None, None)              0         
_________________________________________________________________
lambda (Lambda)              (None, None, 11865)       0         
_________________________________________________________________
lstm (LSTM)                  (None, 128)               6140928   
_________________________________________________________________
dense (Dense)                (None, 512)               66048     
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 513       
=================================================================
Total params: 6,207,489
Trainable params: 6,207,489
Non-trainable params: 0
_________________________________________________________________</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124285"></a>This is our first encounter with a sequential model. Let’s review the model summary in more detail. First, we have a masking layer that returns an output of the same size as the input (i.e., <span class="fm-code-in-text">[None, None]</span>-sized tensor). Then a one-hot encoding layer returns a tensor with a feature dimension of 11865 (which is the vocabulary size). This is because, unlike the input that had a word represented by a single integer, one-hot encoding converts it to a vector of zeros, of the size of the vocabulary, and sets the value indexed by the word ID to 1. The LSTM layer returns a [<span class="fm-code-in-text">None</span>, 128]-sized tensor. Remember that we are only getting the final state output vector, which will be a [<span class="fm-code-in-text">None</span>, 128]-sized tensor, where 128 is the number of units. This last output returned by the LSTM goes to a <span class="fm-code-in-text">Dense</span> layer with 512 nodes and ReLU activation. A dropout layer with 50% dropout follows it. Finally, a <span class="fm-code-in-text">Dense</span> layer<a class="calibre8" id="marker-1124286"></a> with one node produces the final prediction: a value between 0 and 1.</p>

  <p class="body"><a class="calibre8" id="pgfId-1124287"></a>In the following section, we will train the model on training data and evaluate it on validation and testing data to assess the performance of the model.</p>

  <p class="fm-head2"><a id="pgfId-1124288"></a>Exercise 4</p>

  <p class="body"><a class="calibre8" id="pgfId-1124290"></a>Define a model that has a single LSTM layer and a single <span class="fm-code-in-text">Dense</span> layer<a class="calibre8" id="marker-1124289"></a>. The LSTM model has 32 units and accepts a (<span class="fm-code-in-text">None</span>, <span class="fm-code-in-text">None</span>, 30)-sized input (this includes the batch dimension) and produces all the state outputs (instead of the final one). Next, a lambda layer should sum up the states on the time dimension to produce a (<span class="fm-code-in-text">None</span>, 32)-sized output. This output goes to the <span class="fm-code-in-text">Dense</span> layer<a class="calibre8" id="marker-1124291"></a><a class="calibre8" id="marker-1124292"></a> with 10 nodes and softmax activation. You can use the <span class="fm-code-in-text">tf.keras.layers.Add</span> layer<a class="calibre8" id="marker-1124293"></a> to sum up the state vectors. You will need to use the functional API to<a class="calibre8" id="marker-1124294"></a><a class="calibre8" id="marker-1124295"></a> implement<a class="calibre8" id="marker-1124296"></a><a class="calibre8" id="marker-1124297"></a><a class="calibre8" id="marker-1124298"></a> this.</p>

  <h2 class="fm-head" id="sigil_toc_id_128"><a id="pgfId-1124299"></a>9.5 Training and evaluating the model</h2>

  <p class="body"><a class="calibre8" id="pgfId-1124304"></a>We’re<a class="calibre8" id="marker-1124301"></a><a class="calibre8" id="marker-1124302"></a><a class="calibre8" id="marker-1124303"></a> all set to train the model we just defined. As the first step, let’s define two pipelines: one for the training data and one for the validation data. Remember that we split our data and created three different sets: training (<span class="fm-code-in-text">tr_x</span> and <span class="fm-code-in-text">tr_y</span>), validation (<span class="fm-code-in-text">v_x</span> and <span class="fm-code-in-text">v_y</span>), and testing (<span class="fm-code-in-text">ts_x</span> and <span class="fm-code-in-text">ts_y</span>). We will use a batch size of 128:</p>
  <pre class="programlisting"># Using a batch size of 128
batch_size =128
 
train_ds = get_tf_pipeline(tr_x, tr_y, batch_size=batch_size, shuffle=True)
valid_ds = get_tf_pipeline(v_x, v_y, batch_size=batch_size)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124310"></a>Then comes a very important calculation. In fact, doing or not doing this computation can decided whether your model is going to work or not. Remember that we noticed a significant class imbalance in our data set in section 9.1. Specifically, there are more positive classes than negative classes in the data set. Here we will define a weighing factor to assign a greater weight to negative samples when computing the loss and updating weights of the model. To do that, we will define the weighing factor:</p>

  <p class="fm-equation"><a id="pgfId-1124311"></a><i class="fm-italics">weight</i><sub class="fm-subscript">neg</sub><i class="fm-italics">= count(positive samples)/count(negative samples)</i></p>

  <p class="body"><a class="calibre8" id="pgfId-1124312"></a>This will result in a &gt; 1 factor as there are more positive samples than negative samples. We can easily compute this using the following logic:</p>
  <pre class="programlisting">neg_weight = (tr_y==1).sum()/(tr_y==0).sum()</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124314"></a>This results in <i class="fm-italics">weight</i><sub class="fm-subscript">neg</sub>~6 (i.e., approximately 6). Next, we will define the training step as follows:</p>
  <pre class="programlisting">model.fit(
    x=train_ds, 
    validation_data=valid_ds, 
    epochs=10, 
    class_weight={0:neg_weight, 1:1.0}
)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124321"></a>Here, <span class="fm-code-in-text">train_ds</span> is passed to <span class="fm-code-in-text">x</span>, but in fact contains both the inputs and targets. <span class="fm-code-in-text">valid_ds</span>, containing validation samples, is passed to <span class="fm-code-in-text">validation_data</span> argument. We will run this for 10 epochs. Finally, note that we are using the <span class="fm-code-in-text">class_weight</span> argument to tell the model that negative samples must be prioritized over positive samples (due to the under-representation in the data set). <span class="fm-code-in-text">class_weight</span> is defined as a dictionary, where the key is the class label and the value represents the weight given to the samples of that class. When passed, during the loss computations the losses resulting from negative classes will be multiplied by a factor of <span class="fm-code-in-text">neg_weight</span>, leading to more attention being given to negative samples during the optimization process. In practice, we are going to follow the same pattern as in other chapters and run the training process with three callbacks:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1124322"></a>A CSV logger</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1124323"></a>A learning rate scheduler</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1124324"></a>Early stopping</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1124325"></a>The full code looks like the following listing.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1124327"></a>Listing 9.6 Training procedure for the sentiment analyzer</p>
  <pre class="programlisting">os.makedirs('eval', exist_ok=True)
 
csv_logger = tf.keras.callbacks.CSVLogger(
       os.path.join('eval','1_sentiment_analysis.log'))                       <span class="fm-combinumeral">❶</span>
 
monitor_metric = 'val_loss'
mode = 'min'
print("Using metric={} and mode={} for EarlyStopping".format(monitor_metric, mode))
 
lr_callback = tf.keras.callbacks.ReduceLROnPlateau(
    monitor=monitor_metric, factor=0.1, patience=3, mode=mode, min_lr=1e-8    <span class="fm-combinumeral">❷</span>
) 
 
es_callback = tf.keras.callbacks.EarlyStopping(
    monitor=monitor_metric, patience=6, mode=mode, restore_best_weights=False <span class="fm-combinumeral">❸</span>
)
 
model.fit(
    train_ds,                                                                 <span class="fm-combinumeral">❹</span>
    validation_data=valid_ds, 
    epochs=10, 
    class_weight={0:neg_weight, 1:1.0}, 
    callbacks=[es_callback, lr_callback, csv_logger])</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1138966"></a><span class="fm-combinumeral">❶</span> Log the performance metrics to a CSV file.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1138987"></a><span class="fm-combinumeral">❷</span> The learning rate reduction callback</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1139004"></a><span class="fm-combinumeral">❸</span> The early stopping callback</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1139021"></a><span class="fm-combinumeral">❹</span> Train the model.</p>

  <p class="body"><a class="calibre8" id="pgfId-1124355"></a>You should get similar results:</p>
  <pre class="programlisting">Using metric=val_loss and mode=min for EarlyStopping
Epoch 1/10
2427/2427 [==============================] - 72s 30ms/step - loss: 0.7640 - accuracy: 0.7976 - val_loss: 0.4061 - val_accuracy: 0.8193 - lr: 0.0010
 
...
 
Epoch 7/10
2427/2427 [==============================] - 73s 30ms/step - loss: 0.2752 - accuracy: 0.9393 - val_loss: 0.7474 - val_accuracy: 0.8026 - lr: 1.0000e-04
Epoch 8/10
2427/2427 [==============================] - 74s 30ms/step - loss: 0.2576 - accuracy: 0.9439 - val_loss: 0.8398 - val_accuracy: 0.8041 - lr: 1.0000e-04</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124366"></a>It seems that by the end of the training, we have reached above 80% validation accuracy. That’s great news, because we made sure that the validation data set is a balanced data set. But we can’t be too sure. We will need to test our model on a data set that it hasn’t had the chance to see: the testing set. Before that, let’s save the model:</p>
  <pre class="programlisting">os.makedirs('models', exist_ok=True)
tf.keras.models.save_model(model, os.path.join('models', '1_sentiment_analysis.h5'))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124369"></a>We have already created the test data set and have defined the NLP pipeline to process the data, so it’s a matter of calling the <span class="fm-code-in-text">get_tf_pipeline()</span> function with the data:</p>
  <pre class="programlisting">test_ds = get_tf_pipeline(ts_x, ts_y, batch_size=batch_size)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124371"></a>It’s now as simple as calling the following one-liner to get the test performance of the model:</p>
  <pre class="programlisting">model.evaluate(test_ds)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124373"></a>The final result looks like this:</p>
  <pre class="programlisting">87/87 [==============================] - 2s 27ms/step - loss: 0.8678 - accuracy: 0.8038</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124375"></a>We can now go to sleep knowing our model’s performance on unseen data is on par with the validation performance we saw during training.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1124376"></a>Is good accuracy all we’re after?</p>

    <p class="fm-sidebar-text"><a id="pgfId-1124377"></a>The short answer is no. Solving a machine learning task involves many tasks working in harmony. And during the execution of these tasks, we perform various transformations/computations on inputs as well as outputs. The complexity of the whole process means that there are more chances for things to go wrong. Therefore, we should check as many things as we can during the process.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1124378"></a>Speaking solely about testing, we have to make sure that the test data is correctly processed while going through the data pipeline. Furthermore, we should check the final predictions. Among many other checks, you can check the topmost positive predictions and negative predictions to make sure the model’s decisions are sensible. You can simply visually inspect the input text and the corresponding prediction. We will discuss the specifics in a coming section.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1124379"></a>It will only slightly increase the time spent on your model. But it can save you hours of debugging as well as embarrassment or loss of reputation from releasing an inaccurate model.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1124380"></a>In the next section, we will further enhance our model by using word vectors to represent tokens fed into the model. Word vectors help machine learning models understand language better.</p>

  <p class="fm-head2"><a id="pgfId-1124381"></a>Exercise 5</p>

  <p class="body"><a class="calibre8" id="pgfId-1124382"></a>Assume you have three classes in your training data set: A, B and C. You have 10 records for A, 25 for B, and 50 for C. What do you think will be good weights for the three classes? Remember that the majority class should get a smaller<a class="calibre8" id="marker-1124384"></a><a class="calibre8" id="marker-1124385"></a><a class="calibre8" id="marker-1124386"></a> weight.</p>

  <h2 class="fm-head" id="sigil_toc_id_129"><a id="pgfId-1124387"></a>9.6 Injecting semantics with word vectors</h2>

  <p class="body"><a class="calibre8" id="pgfId-1124391"></a>You<a class="calibre8" id="marker-1124388"></a><a class="calibre8" id="marker-1124389"></a><a class="calibre8" id="marker-1124390"></a> have built a model that can measure sentiments with ~80% accuracy, but you want to improve further. You believe word embeddings will provide the edge required to attain a higher accuracy. Word embeddings are a way to encode words as a feature vector. Word embeddings learn feature vectors for the words in the vocabulary jointly with the model training. An embedding layer introduces a trainable matrix, where each row represents a trainable vector for a single word in the vocabulary. This is much better than one-hot encoding because one-hot encoding suffers from the curse of dimensionality, which means that as the number of words grows, so does the dimensionality of the inputs to the model.</p>

  <p class="body"><a class="calibre8" id="pgfId-1124392"></a>You should feel proud about having a reasonable model that can accurately classify positive and negative sentiments; 80% accuracy is a great starting point. But let’s see what we can improve in the already good model we have.</p>

  <p class="body"><a class="calibre8" id="pgfId-1124393"></a>A bottleneck that’s staring us right in the face is the one-hot encoding layer that’s in our model. One-hot encoding, despite its simplicity, is a very inefficient representation of words. It is a localized representation of words, meaning only one element (set to value 1) in the representation carries information. In other words, it’s a very sparse representation that has a very large number of elements set to zero and does not contribute with information. One-hot encoding also suffers from the curse of dimensionality. Finally, one-hot encoding completely disregards the valuable semantics present in the text. With one-hot encoding, you can’t say if a cat is more similar to a dog than it is to a volcano. Now the question is, are there better ways to represent words?</p>

  <h3 class="fm-head1" id="sigil_toc_id_130"><a id="pgfId-1124394"></a>9.6.1 Word embeddings</h3>

  <p class="body"><a class="calibre8" id="pgfId-1124398"></a>It’s<a class="calibre8" id="marker-1124395"></a><a class="calibre8" id="marker-1124396"></a><a class="calibre8" id="marker-1124397"></a> time to usher in a new era of word representations known as <i class="fm-italics">word embeddings</i>. Word embeddings, sometimes called word vectors, are a very information-rich and efficient representation of words. As opposed to the localized representations like one-hot vectors, word vectors provide a distributed representation. This means that all the elements in the vector play a role in defining the word represented by the vector. In other words, word vectors have dense representations, in contrast to the sparse representation of one-hot vectors. The dimensionality of word vectors does not depend on the size of vocabulary, which allows you to save memory and computational time. Finally, but most importantly, word vectors capture the semantics or similarity of words. With word vectors, you know that a cat is more similar to a dog than to a volcano. Before understanding word vectors, you have to understand the important role played by the context around a word. Word vectors heavily rely on the context of words to generate rich representations of words. The importance of the context is subliminally captured by a famous quote by J.R. Firth, an English linguist:</p>

  <p class="fm-quote1"><a id="pgfId-1124399"></a>You shall know a word by the company it keeps.</p>

  <p class="body"><a class="calibre8" id="pgfId-1124400"></a>To expand on this a little more, the context of a word plays an important role in defining the semantics of that word. For example, take the following sentence:</p>

  <p class="body"><a class="calibre8" id="pgfId-1124401"></a>Our pet Toby is a ____; he enjoys playing with a ball.</p>

  <p class="body"><a class="calibre8" id="pgfId-1124402"></a>What do you think is the right word here? We see words like “pet,” “playing,” and “ball” in the context. Most likely it’s a cat or a dog. This means that only a certain type of words (i.e., a type of pet) will appear in this context. Using this property, word vectors can generate vectors that preserve the semantics of the text. In this example, word vectors will capture that cats and dogs are very similar (not biologically, of course, but in the way we interact with or perceive them). In a more technical stance, the objective of word vector algorithms is as follows: if word <i class="fm-italics">w</i><sub class="fm-subscript">i</sub> and <i class="fm-italics">w</i><sub class="fm-subscript">j</sub> appear in the same context, for some distance measure <i class="fm-italics">Dist(a,b)</i>, that measures the distance between two vectors <i class="fm-italics">a</i> and <i class="fm-italics">b</i>:</p>

  <p class="fm-equation"><a id="pgfId-1124403"></a><i class="fm-italics">Dist</i>(<i class="fm-italics">w</i><sub class="fm-subscript">i</sub><i class="fm-italics">, w</i><sub class="fm-subscript">j</sub>) <i class="fm-italics">~</i> 0</p>

  <p class="body"><a class="calibre8" id="pgfId-1124404"></a>The actual word vector algorithms are out of scope for this book. A few note-worthy algorithms are Skip-gram, CBoW (Continuous Bag-of-Words), GloVe (global vectors) and ELMo (Embeddings from Language Models). You can read more details about the Skip-gram and CBoW algorithms by reading the paper “Efficient Estimation of Word Representations in Vector Space” by Tomas Mikolov et al. (<span class="fm-hyperlink"><a class="url" href="https://arxiv.org/pdf/1301.3781.pdf">https://arxiv.org/pdf/1301.3781.pdf</a></span>).</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1124405"></a>Show me the word vector algorithms</p>

    <p class="fm-sidebar-text"><a id="pgfId-1124406"></a>Word vector algorithms train in an unsupervised manner. The training algorithm specifics will differ depending on the algorithm. The Skip-gram algorithm generates input target pairs by picking a probe word as the input and the context words as the targets. For example, from the sentence “I went to buy flowers” it will generate input target pairs such as [(went, I), (went, to), (to, went), (to, buy), . . . ]. Then it will solve the classification task of predicting the context of a probe word, which will lead to identifying good word vectors. However, word vector algorithms like Skip-gram suffer from the lack of a global view of the corpus because the algorithm only considers the small context around a word.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1124407"></a>GloVe, on the other hand, uses both local and global information to generate word vectors. To extract global information of a corpus, it leverages a co-occurrence matrix, which contains how many times the word “i” appeared in the context of “j” in the corpus. You can read more about this in the paper, “GloVe: Global Representations for Word Vectors” by Pennington et al. (<span class="fm-hyperlink"><a class="url" href="https://nlp.stanford.edu/pubs/glove.pdf">https://nlp.stanford.edu/pubs/glove.pdf</a></span>). GloVe still does not address the problem of ambiguous words. By ambiguous words, I mean words that have different meanings depending on the context. For example, the word “bank” in the sentences “I went to the bank to deposit money” and “I walked on the river bank” has entirely different meanings. GloVe would give the same vector for both cases, which is not accurate.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1124408"></a>Enter ELMo! ELMo was introduced in the paper “Deep Contextualized Word Representations” by Peters et al. (<span class="fm-hyperlink"><a class="url" href="https://arxiv.org/pdf/1802.05365.pdf">https://arxiv.org/pdf/1802.05365.pdf</a></span>). ELMo uses bidirectional LSTM models to generate word vectors. A bidirectional LSTM is similar to a standard LSTM but reads the sequence both forward and backward.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1124410"></a>The final output of a word vector algorithm is a V × d-sized embedding matrix. The <i class="fm-timesitalic">i</i><sup class="fm-superscript">th</sup> row of this matrix represents the word vector for the word represented by the ID <i class="fm-timesitalic">i</i>. <i class="fm-timesitalic">d</i> is typically &lt; 300 and is selected using a hyperparameter algorithm. Figure 9.7 depicts the word embedding matrix.</p>

  <p class="fm-figure"><img alt="09-07" class="calibre10" src="../../OEBPS/Images/09-07.png" width="1047" height="694"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1142358"></a>Figure 9.7 An overview of how the embedding matrix is used to obtain word vectors. A lookup is performed using the input word IDs to fetch the vectors corresponding to those indices. The actual values of the vectors are learned during the model training.</p>

  <p class="body"><a class="calibre8" id="pgfId-1124417"></a>We will now enhance our sentiment analyzer model with word embeddings.</p>

  <h3 class="fm-head1" id="sigil_toc_id_131"><a id="pgfId-1124418"></a>9.6.2 Defining the final model with word embeddings</h3>

  <p class="body"><a class="calibre8" id="pgfId-1124425"></a>In<a class="calibre8" id="marker-1124419"></a><a class="calibre8" id="marker-1124420"></a><a class="calibre8" id="marker-1124421"></a> general, any deep sequential model can benefit from word<a class="calibre8" id="marker-1124422"></a><a class="calibre8" id="marker-1124423"></a><a class="calibre8" id="marker-1124424"></a> embeddings. As an added benefit, most of the time you don’t need to worry about the word vector algorithms themselves and can enjoy good performance by introducing a randomly initialized embedding space. Then the embeddings can be trained jointly with the other parts of the models during the specific NLP task we’re solving. Following the same pattern, let’s introduce a randomly initialized embedding space to our sentiment analyzer.</p>

  <p class="body"><a class="calibre8" id="pgfId-1124426"></a>Let’s remind ourselves of the previous model we implemented. The model consisted of a masking layer, a one-hot encoder layer, and an LSTM layer followed by two <span class="fm-code-in-text">Dense</span> layers (with dropout in between):</p>
  <pre class="programlisting">model = tf.keras.models.Sequential([
    # Create a mask to mask out zero inputs
    tf.keras.layers.Masking(mask_value=0.0, input_shape=(None,1)),
    # After creating the mask, convert inputs to onehot encoded inputs
    OnehotEncoder(depth=n_vocab),
    # Defining an LSTM layer
    tf.keras.layers.LSTM(128, return_state=False, return_sequences=False),
    # Defining a Dense layer
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(1, activation='sigmoid')
])</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124439"></a>The model will remain more or less the same, except for two changes:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1124440"></a>We will replace the one-hot encoder layer with an <span class="fm-code-in-text">tf.keras.layers.Embedding</span> layer.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1124441"></a>The masking functionality will be absorbed into the <span class="fm-code-in-text">tf.keras.layers.Embedding</span> layer by setting the <span class="fm-code-in-text">mask_zero=True</span> in the layer.</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1124442"></a>The <span class="fm-code-in-text">tf.keras.layers.Embeddings</span> layer introduces a large trainable matrix into the model. This matrix is a <span class="fm-code-in-text">(V+1) x d</span>-sized matrix, where <span class="fm-code-in-text">V</span> is the vocabulary size. The additional one is required as we use the special reserved ID zero. <span class="fm-code-in-text">d</span> is chosen through a hyperparameter optimization algorithm. In the following model, we will set <span class="fm-code-in-text">d = 128</span> empirically. The line that has changed has been highlighted in bold in the listing.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1124444"></a>Listing 9.7 Implementing the sentiment analyzer with word embeddings</p>
  <pre class="programlisting">model = tf.keras.models.Sequential([                                               <span class="fm-combinumeral">❶</span>
    
    <b class="fm-bold">tf.keras.layers.Embedding(input_dim=n_vocab+1, output_dim=128, mask_zero=True)</b>,<span class="fm-combinumeral">❷</span>
    
    tf.keras.layers.LSTM(128, return_state=False, return_sequences=False),         <span class="fm-combinumeral">❸</span>
    tf.keras.layers.Dense(512, activation='relu'),                                 <span class="fm-combinumeral">❹</span>
    tf.keras.layers.Dropout(0.5),                                                  <span class="fm-combinumeral">❺</span>
    tf.keras.layers.Dense(1, activation='sigmoid')                                 <span class="fm-combinumeral">❻</span>
])
 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  <span class="fm-combinumeral">❼</span>
model.summary()</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1138480"></a><span class="fm-combinumeral">❶</span> Create a mask to mask out zero inputs.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1138520"></a><span class="fm-combinumeral">❷</span> Add an Embedding layer. It will look up word vectors for the word IDs passed in as the input.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1138537"></a><span class="fm-combinumeral">❸</span> Define an LSTM layer.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1138554"></a><span class="fm-combinumeral">❹</span> Define Dense layers.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1138571"></a><span class="fm-combinumeral">❺</span> Define a Dropout layer.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1138588"></a><span class="fm-combinumeral">❻</span> Define the final Dense layer with sigmoidal activation.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1138481"></a><span class="fm-combinumeral">❼</span> Compile the model with binary cross-entropy as the loss.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1124465"></a>Empty inputs, the mask, and the LSTM layer</p>

    <p class="fm-sidebar-text"><a id="pgfId-1124466"></a>We made sure that we don’t have any empty reviews in our data set by introducing a filter to filter out the empty reviews. It is worth understanding why we did this. In addition to playing a role as a data cleaning step, it serves an important purpose. Having empty reviews in the data set will result in an all-zero vector in our data pipeline. For example, an empty review if the sequence length is 5 will return [0,0,0,0,0]. When using a Masking layer, all the inputs will be ignored. This is a problematic edge case for the LSTM layer and will raise the following error:</p>
    <pre class="programlisting">UnknownError:  [_Derived_]  CUDNN_STATUS_BAD_PARAM
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1496): 
<span class="fm-code-continuation-arrow">➥</span> 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, 
<span class="fm-code-continuation-arrow">➥</span> max_seq_length, batch_size, data_size, seq_lengths_array, 
<span class="fm-code-continuation-arrow">➥</span> (void*)&amp;padding_fill)'
         [[{{node cond_38/then/_0/CudnnRNNV3}}]]
         [[sequential/lstm/StatefulPartitionedCall]]
         [[gradient_tape/sequential/embedding/embedding_lookup/Reshape/_42]] 
<span class="fm-code-continuation-arrow">➥</span> [Op:__inference_train_function_8225]
 
Function call stack:
train_function -&gt; train_function -&gt; train_function</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1124477"></a>For this reason, you must make sure that the empty reviews are filtered from the data before feeding the data to the model.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1124482"></a>With that, we will train the model we<a class="calibre8" id="marker-1124479"></a><a class="calibre8" id="marker-1124480"></a><a class="calibre8" id="marker-1124481"></a> defined.</p>

  <h3 class="fm-head1" id="sigil_toc_id_132"><a id="pgfId-1124483"></a>9.6.3 Training and evaluating the model</h3>

  <p class="body"><a class="calibre8" id="pgfId-1124488"></a>Training<a class="calibre8" id="marker-1124484"></a><a class="calibre8" id="marker-1124485"></a><a class="calibre8" id="marker-1124486"></a><a class="calibre8" id="marker-1124487"></a> and evaluation code are identical to the implementation we discussed earlier. Therefore, we will not reiterate the discussion. When you train the new model, you will see a result similar to the following.</p>

  <p class="body"><a class="calibre8" id="pgfId-1124489"></a>When you train the model, you will reach a validation accuracy that is slightly above the previous validation accuracy we experienced:</p>
  <pre class="programlisting">Epoch 1/25
2427/2427 [==============================] - 30s 12ms/step - loss: 0.7552 - 
<span class="fm-code-continuation-arrow">➥</span> accuracy: 0.7949 - val_loss: 0.3942 - val_accuracy: 0.8277 - lr: 0.0010
Epoch 2/25
 
...
 
Epoch 8/25
2427/2427 [==============================] - 29s 12ms/step - loss: 0.3059 - 
<span class="fm-code-continuation-arrow">➥</span> accuracy: 0.9312 - val_loss: 0.6839 - val_accuracy: 0.8130 - lr: 1.0000e-04</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124499"></a>Evaluating the model can be done by running the following:</p>
  <pre class="programlisting">test_ds = get_tf_pipeline(ts_x, ts_y, batch_size=128)
model.evaluate(test_ds)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124503"></a>It seems that adding an embedding layer leads to a slightly higher testing performance as well:</p>
  <pre class="programlisting">87/87 [==============================] - 0s 5ms/step - loss: 0.7214 - accuracy: 0.8111</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124507"></a>Remember that we said we wouldn’t trust the accuracy alone. Now let’s dig in a bit deeper and see if our model is giving out sensible predictions. An easy way to do this is to check the top-k positive reviews and the top-k negative reviews in the test set and do a visual inspection. We exhausted the <span class="fm-code-in-text">tf.data</span> pipeline when we finished the evaluation. Therefore, we need to redefine the data pipeline:</p>
  <pre class="programlisting">test_ds = get_tf_pipeline(ts_x, ts_y, batch_size=128)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124511"></a>Then we will go batch by batch, and for each batch, we will store the inputs, predictions, and targets in three separate lists: <span class="fm-code-in-text">test_x</span>, <span class="fm-code-in-text">test_pred</span>, and <span class="fm-code-in-text">test_y</span>, respectively:</p>
  <pre class="programlisting">test_x = []
test_pred = []
test_y = []
for x, y in test_ds:
    test_x.append(x)    
    test_pred.append(model.predict(x))
    test_y.append(y)
test_x = [doc for t in test_x for doc in t.numpy().tolist()]
test_pred = tf.concat(test_pred, axis=0).numpy()
test_y = tf.concat(test_y, axis=0).numpy()</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124525"></a>We will use <span class="fm-code-in-text">argsort</span> to get the indices of the sorted prediction array. This way, the start of the array will have the indices of the most negative reviews, whereas the end of the array will contain the most positive review indices. Let’s take the five top-most and five bottom-most reviews to visually check:</p>
  <pre class="programlisting">sorted_pred = np.argsort(test_pred.flatten())
min_pred = sorted_pred[:5]
max_pred = sorted_pred[-5:]
 
print("Most negative reviews\n")
print("="*50)
for i in min_pred:    
    print(" ".join(tokenizer.sequences_to_texts([test_x[i]])), '\n')
    
print("\nMost positive reviews\n")
print("="*50)
for i in max_pred:
    print(" ".join(tokenizer.sequences_to_texts([test_x[i]])), '\n')</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124541"></a>Let’s check the results:</p>
  <pre class="programlisting">Most negative reviews
==================================================
buy game high rating promise gameplay saw youtube story so-so graphic 
<span class="fm-code-continuation-arrow">➥</span> mediocre control terrible could not adjust control option preference ...
 
attempt install game quad core windows 7 pc zero luck go back forth try 
<span class="fm-code-continuation-arrow">➥</span> every suggestion rockstar support absolutely useless game ... 
 
way product 5 star 28 review write tone lot review similar play 2 song 
<span class="fm-code-continuation-arrow">➥</span> expert drum say unless play tennis shoe fact screw not flush mean feel 
<span class="fm-code-continuation-arrow">➥</span> every kick specifically two screw leave plus pedal completely torn 
<span class="fm-code-continuation-arrow">➥</span> mount screw something actually go wrong pedal instal unscrew send back 
<span class="fm-code-continuation-arrow">➥</span> ea 
 
unk interactive stranger unk unk genre develop operation flashpoint various 
<span class="fm-code-continuation-arrow">➥</span> real-life computer sims military folk unk know come deliver good 
<span class="fm-code-continuation-arrow">➥</span> recreation ultra deadly unk modern combat engagement arma attempt 
<span class="fm-code-continuation-arrow">➥</span> simulate `` unk firepower '' soldier combine arm warfare set fictional 
<span class="fm-code-continuation-arrow">➥</span> sprawl island nation conveniently mirror terrain middle eastern country 
 
not cup tea 
 
 
Most positive reviews
==================================================
find something love every final fantasy game play thus far ff13 different 
<span class="fm-code-continuation-arrow">➥</span> really appreciate square enix 's latest trend shake gameplay new 
<span class="fm-code-continuation-arrow">➥</span> release still hammer best look ... 
know little squad base game genre know game fun not unk fun obliterate 
<span class="fm-code-continuation-arrow">➥</span> enemy mobile suit satisfy blow zombie head resident evil graphic 
<span class="fm-code-continuation-arrow">➥</span> presentation solid best franchise yes ... 
 
okay hdtv monitor cause ps3 switch game movie widescreen fullscreen every 5 
<span class="fm-code-continuation-arrow">➥</span> minute someone tell need get hd component cable look price saw brand 
<span class="fm-code-continuation-arrow">➥</span> name sony much money basically name brand pay fancy retail packaging 
<span class="fm-code-continuation-arrow">➥</span> generic cable get quality without fancy packaging name brand embed 
<span class="fm-code-continuation-arrow">➥</span> cable favor save money 
 
absolutely phenomenal gaming mouse love programmable size button mouse 
<span class="fm-code-continuation-arrow">➥</span> surprising ergonomic design ... 
 
first motorstorm come unk racing type one pioneer physic base race every 
<span class="fm-code-continuation-arrow">➥</span> track lot branch path every branch suitable different class vehicle 
<span class="fm-code-continuation-arrow">➥</span> take next level race much bigger apart mud also obstacles individual 
<span class="fm-code-continuation-arrow">➥</span> vehicle class small vehicle get stuck plant unk hazard time lot physic 
<span class="fm-code-continuation-arrow">➥</span> people complain vehicle slide </pre>

  <p class="body"><a class="calibre8" id="pgfId-1124568"></a>We can confidently say that our sentiment analyzer was a success! The negative and positive reviews identified by the model seem like they’re in the correct places. We had to overcome many obstacles having to do with data quality, data preparation, and model design. Through them all, we persevered! In the next chapter, we will discuss another NLP task known as language modeling.</p>

  <p class="fm-head2"><a id="pgfId-1124569"></a>Exercise 6</p>

  <p class="body"><a class="calibre8" id="pgfId-1124570"></a>Word vector algorithms like Skip-gram use an embedding layer directly connected to a <span class="fm-code-in-text">Dense</span> layer that has the same size as the vocabulary. If you have a vocabulary of size 500, want to produce 32 dimensional word vectors, and have a dense layer with 500 units and a softmax activation, how would you implement a model? The model accepts a (<span class="fm-code-in-text">None</span>, 500)-sized (batch dimension is <span class="fm-code-in-text">None</span>) one-hot encoded vectors<a class="calibre8" id="marker-1124571"></a><a class="calibre8" id="marker-1124572"></a><a class="calibre8" id="marker-1124573"></a><a class="calibre8" id="marker-1124574"></a> of<a class="calibre8" id="marker-1124575"></a><a class="calibre8" id="marker-1124576"></a><a class="calibre8" id="marker-1124577"></a> words.</p>

  <h2 class="fm-head" id="sigil_toc_id_133"><a id="pgfId-1124578"></a>Summary</h2>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1124579"></a>The NLTK library provides an API to perform various text preprocessing tasks, such as tokenizing to words, removing stop words, lemmatization, and so on.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1124580"></a>Preprocessing tasks need to be applied with care. For example, when removing stop words, the word “not” should not be removed. This is because in a sentiment analysis task, the word “not” carries very important information.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1124581"></a><span class="fm-code-in-text">tensorflow.keras.preprocessing.text.Tokenizer</span> can be used to convert text to numbers. This is done by the <span class="fm-code-in-text">Tokenizer</span> first building a dictionary that maps each unique word to a unique ID. Then a given text can be converted to a sequence of IDs.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1124582"></a>Padding is a technique used to bring variable-length text to the same length.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1124583"></a>Padding works by padding all sequences in a given text corpus to a fixed length by inserting zeros (at the end or at the beginning).</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1124584"></a>When processing variable-length sequences like text, there’s another strategy known as bucketing that is used to batch similar-length text sequences together. This helps the model keep the memory footprint small as well as to not waste computation on excessing padding.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1124585"></a>In TensorFlow, you can use <span class="fm-code-in-text">tf.data.experimental.bucket_by_sequence_ length()</span> <i class="fm-italics">to bucket text sequences.</i></p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1124586"></a>LSTM (long short-term memory) models have shown superior performance in solving NLP tasks.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1124587"></a>LSTM models work by going from one timestep to the next, while processing the input at that time step to produce an output for each timestep.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1124588"></a>LSTM models have a mechanism to store both long-term and short-term memory. This is achieved through a gating mechanism that controls the flow of information in the LSTM cell.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1124589"></a>Word embeddings are a text encoding method that is superior to one-hot encoding and that has the ability to preserve the semantics of words when generating the numerical representation of words.</p>
    </li>
  </ul>

  <h2 class="fm-head" id="sigil_toc_id_134"><a id="pgfId-1124590"></a>Answers to exercises</h2>

  <p class="body"><a class="calibre8" id="pgfId-1124591"></a><b class="fm-bold">Exercise 1</b></p>
  <pre class="programlisting">s = s.replace("-", ' ')
   tokens = word_tokenize(s)                                                             
pos_tags = nltk.pos_tag(tokens)
clean_text = [
        lemmatizer.lemmatize(w, pos=p[0].lower()) if p[0]=='V' else w
        for (w, p) in pos_tags
]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124600"></a><b class="fm-bold">Exercise 2</b></p>
  <pre class="programlisting">s = “a_b_B_c_d_a_D_b_d_d”
tok = Tokenizer(num_words = 3, split=”_”, lower=True)
tok.fit_on_texts([s])
 
Most common words get the lowest word ID (starting from 1). 
<span class="fm-code-continuation-arrow">➥</span> tok.texts_to_sequences([s]) will produce [[3,2,2,1,3,1,2,1,1]]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124607"></a><b class="fm-bold">Exercise 3</b></p>
  <pre class="programlisting">bucket_fn = tf.data.experimental.bucket_by_sequence_length(
        lambda x: tf.cast(tf.shape(x)[0],'int32'), 
        bucket_boundaries=[10, 25, 30], 
        bucket_batch_sizes=[batch_size,batch_size,batch_size, batch_size], 
        padded_shapes=None,
        padding_values=0, 
        pad_to_bucket_boundary=True
    )</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124616"></a><b class="fm-bold">Exercise 4</b></p>
  <pre class="programlisting">inp = tf.keras.layers.Input(shape=(None, 30))
lstm_out = tf.keras.layers.LSTM(32, return_sequences=True)(inp)
sum_out = tf.keras.layers.Add(axis=1)(lstm_out)
dense_out = tf.keras.layers.Dense(10, activation=’softmax’)(sum_out)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124623"></a><b class="fm-bold">Exercise 5</b></p>
  <pre class="programlisting">A - (25+50)/(10+25+50)
B - (10+50)/(10+25+50)
C - (10+25)/(10+25+50)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1124631"></a><b class="fm-bold">Exercise 6</b><a class="calibre8" id="marker-1124629"></a><a class="calibre8" id="marker-1124630"></a></p>
  <pre class="programlisting">tf.keras.models.Sequential(
[
    tf.keras.layers.Embedding(input_dim=500, output_dim=32, input_shape=(500,)),
    tf.keras.layers.Dense(500, activation=’softmax’)
])</pre>
</div>
</div>
</body>
</html>