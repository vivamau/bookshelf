<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xml:lang="en"
      lang="en"
      xmlns="http://www.w3.org/1999/xhtml"
      xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<title>TensorFlow in Action</title>
<link rel="stylesheet" type="text/css" href="../../override_v1.css"/>
<link rel="stylesheet" type="text/css" href="../../stylesheet.css"/><link rel="stylesheet" type="text/css" href="../../page_styles.css"/>
</head>
<body>
<div id="book-content">
<div id="sbo-rt-content" class="calibre"><h1 class="tochead"><a id="pgfId-1109479"></a>8 Telling things apart: Image segmentation</h1>

  <p class="co-summary-head"><a id="pgfId-1109481"></a>This chapter<a id="marker-1112039"></a> covers</p>

  <ul class="calibre9">
    <li class="co-summary-bullet"><a class="calibre8" id="pgfId-1109482"></a>Understanding segmentation data and working with it in Python</li>

    <li class="co-summary-bullet"><a class="calibre8" id="pgfId-1109483"></a>Implementing a fully fledged segmentation data pipeline</li>

    <li class="co-summary-bullet"><a class="calibre8" id="pgfId-1109484"></a>Implementing an advanced segmentation model (DeepLab v3)</li>

    <li class="co-summary-bullet"><a class="calibre8" id="pgfId-1109485"></a>Compiling models with custom-built image segmentation loss functions/metrics</li>

    <li class="co-summary-bullet"><a class="calibre8" id="pgfId-1109486"></a>Training the image segmentation model on the clean and processed image data</li>

    <li class="co-summary-bullet"><a class="calibre8" id="pgfId-1109487"></a>Evaluating the trained segmentation model</li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1109488"></a>In the last chapter, we learned about various advanced computer vision models and techniques to push the performance of an image classifier. We learned about the architecture of Inception net v1 as well as its successors (e.g., Inception net v2, v3, and v4). Our objective was to lift the performance of the model on an image classification data set with 64 × 64-sized RGB images of objects belonging to 200 different classes. While trying to train a model on this data set, we learned many important concepts:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1109491"></a><i class="fm-italics">Inception blocks</i><a class="calibre8" id="marker-1109490"></a>—A way to group convolutional layers having different-sized windows (or kernels) to encourage learning features at different scales while making the model parameter efficient due to the smaller-sized kernels.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1109493"></a><i class="fm-italics">Auxiliary outputs</i><a class="calibre8" id="marker-1109492"></a>—Inception net uses a classification layer (i.e., a fully connected layer with softmax activation) not only at the end of the network, but also in the middle of the network. This enables the gradients from the final layer to flow strongly all the way to the first layer.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1109495"></a><i class="fm-italics">Augmenting data</i><a class="calibre8" id="marker-1109494"></a>—Using various image transformation techniques (adjusting brightness/contrast, rotating, translating, etc.) to increase the amount of labeled data using the <span class="fm-code-in-text">tf.keras.preprocessing.image.ImageDataGenerator</span>.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1109496"></a><i class="fm-italics">Dropout</i>—Switching on and off nodes in the layers randomly. This forces the neural networks to learn more robust features as the network does not always have all the nodes activated.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1109498"></a><i class="fm-italics">Early stopping</i><a class="calibre8" id="marker-1109497"></a>—Using the performance on the validation data set as a way to control when the training stops. If the validation performance has not increased in a certain number of epochs, training is halted.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1109500"></a><i class="fm-italics">Transfer learning</i><a class="calibre8" id="marker-1109499"></a>—Downloading and using a pretrained model (e.g., Inception-ResNet v2) trained on a larger, similar data set as the initialization and fine-tuning it to perform well on the task at hand.</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1109501"></a>In this chapter, we will learn about another important task in computer vision: image segmentation. In image classification, we only care if an object exists in a given image. Image segmentation, on the other hand, recognizes multiple objects in the same image as well as where they are in the image. It is a very important topic of computer vision, and applications like self-driving cars live and breathe image segmentation models. Self-driving cars need to precisely locate objects in their surroundings, which is where image segmentation comes into play. As you might have guessed already, they also have their roots in many other applications:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1109502"></a>Image retrieval</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1109504"></a>Identifying galaxies (<span class="fm-hyperlink"><a class="url" href="http://mng.bz/gwVx">http://mng.bz/gwVx</a></span>)</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1109505"></a>Medical image analysis</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1109506"></a>If you are a computer vision/deep learning engineer/researcher working on image-related problems, there is a high chance that your path will cross with image segmentation. Image segmentation models classify each pixel in the image to one of a predefined set of object categories. Image segmentation has ties to the image classification task we saw earlier. Both solve a classification task. Additionally, pretrained image classification models are used as the backbone of segmentation models, as they can provide crucial image features at different granularities to solve the segmentation task better and faster. A key difference is that image classifiers are solving a sparse prediction task, where each image has a single class label associated, as opposed to segmentation models that solve a dense prediction task that has a class label associated with every pixel in the image.</p>

  <p class="body"><a class="calibre8" id="pgfId-1109507"></a>Any image segmentation algorithm can be classified as one of the following:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1109508"></a><i class="fm-italics">Semantic segmentation</i>—The algorithm is only interested in identifying different categories of objects present in the image. For example, if there are multiple persons in the image, the pixels corresponding to all of them will be tagged with the same class.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1109511"></a><i class="fm-italics">Instance segmentation</i><a class="calibre8" id="marker-1109509"></a><a class="calibre8" id="marker-1109510"></a>—The algorithm is interested in identifying different objects separately. For example, if there are multiple persons in the image, pixels belonging to each person are represented by a unique class. Instance-based segmentation is considered more difficult than semantic segmentation.</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1109512"></a>Figure 8.1 depicts the difference between the data found in a semantic segmentation task and an instance-based segmentation task. In this chapter, we will focus on semantic segmentation (<span class="fm-hyperlink"><a class="url" href="http://mng.bz/5QAZ">http://mng.bz/5QAZ</a></span>).</p>

  <p class="fm-figure"><img alt="08-01" class="calibre10" src="../../OEBPS/Images/08-01.png" width="1019" height="289"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1137355"></a>Figure 8.1 Semantic segmentation versus instance segmentation</p>

  <p class="body"><a class="calibre8" id="pgfId-1109520"></a>In the next section, we will look at the data we are dealing with more closely.</p>

  <h2 class="fm-head" id="sigil_toc_id_101"><a id="pgfId-1109521"></a>8.1 Understanding the data</h2>

  <p class="body"><a class="calibre8" id="pgfId-1109524"></a>You<a class="calibre8" id="marker-1109523"></a> are experimenting with a startup idea. The idea is to develop a navigation algorithm for small remote-control (RC) toys. Users can choose between how safe or adventurous the navigation needs to be. As the first step, you plan to develop an image segmentation model. The output of the image segmentation model will later feed to a different model that will predict the navigation path depending on what the user requests.</p>

  <p class="body"><a class="calibre8" id="pgfId-1109525"></a>For this task, you feel the Pascal VOC 2012 data set will be a good fit as it mostly comprises indoor and outdoor images that are found in urban/domestic environments. It contains pairs of images: an input image containing some objects and an annotated image. In the annotated image, each pixel has an assigned color, depending on which object that pixel belongs to. Here, you plan to download the data set and load the data successfully into Python.</p>

  <p class="body"><a class="calibre8" id="pgfId-1109526"></a>After having a good understanding/framing of the problem you want to solve, your next focus point should be understanding and exploring the data. Segmentation data is different from the image classification data sets we’ve seen thus far. One major difference is that both the input and target are images. The input image is a standard image, similar to what you’d find in an image classification task. Unlike in image classification, the target is not a label, but an image, where each pixel has a color from a predefined palette of colors. In other words, each object we’re interested in segmenting is assigned a color. Then a pixel corresponding to that object in the input image is colored with that color. The number of available colors is the same as the number of different objects (plus background) that you’re interested in identifying (figure 8.2).</p>

  <p class="fm-figure"><img alt="08-02" class="calibre10" src="../../OEBPS/Images/08-02.png" width="603" height="672"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1137389"></a>Figure 8.2 Inputs and outputs of an image classifier versus an image segmentation model</p>

  <p class="body"><a class="calibre8" id="pgfId-1109533"></a>For this task, we will be using the PASCAL VOC 2012 data set, which is popular and consists of real-world scenes. The data set has labels for 22 different classes, as outlined in table 8.1.</p>

  <p class="fm-table-caption"><a id="pgfId-1112694"></a>Table 8.1 Different classes and their respective labels in the PASCAL VOC 2012 data set</p>

  <table border="1" class="contenttable" width="100%">
    <colgroup class="calibre12">
      <col class="calibre13" span="1" width="25%"/>
      <col class="calibre13" span="1" width="25%"/>
      <col class="calibre13" span="1" width="25%"/>
      <col class="calibre13" span="1" width="25%"/>
    </colgroup>

    <tr class="calibre14">
      <th class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1112702"></a><b class="fm-bold">Class</b></p>
      </th>

      <th class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1112704"></a><b class="fm-bold">Assigned Label</b></p>
      </th>

      <th class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1112706"></a><b class="fm-bold">Class</b></p>
      </th>

      <th class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1112708"></a><b class="fm-bold">Assigned Label</b></p>
      </th>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112710"></a>Background</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112712"></a>0</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112714"></a>Dining table</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112716"></a>11</p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112718"></a>Aeroplane</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112720"></a>1</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112722"></a>Dog</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112724"></a>12</p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112726"></a>Bicycle</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112728"></a>2</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112730"></a>Horse</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112732"></a>13</p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112734"></a>Bird</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112736"></a>3</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112738"></a>Motorbike</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112740"></a>14</p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112742"></a>Boat</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112744"></a>4</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112746"></a>Person</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112748"></a>15</p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112750"></a>Bottle</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112752"></a>5</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112754"></a>Potted plant</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112756"></a>16</p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112758"></a>Bus</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112760"></a>6</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112762"></a>Sheep</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112764"></a>17</p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112766"></a>Car</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112768"></a>7</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112770"></a>Sofa</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112772"></a>18</p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112774"></a>Cat</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112776"></a>8</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112778"></a>Train</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112780"></a>19</p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112782"></a>Chair</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112784"></a>9</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112786"></a>TV/monitor</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112788"></a>20</p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112790"></a>Cow</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112792"></a>10</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112794"></a>Boundaries/unknown object</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1112796"></a>255</p>
      </td>
    </tr>
  </table>

  <p class="body"><a class="calibre8" id="pgfId-1109634"></a>The white pixels represent object boundaries or unknown objects. Figure 8.3 illustrates the data set by showing a sample for every single object class present.</p>

  <p class="fm-figure"><img alt="08-03" class="calibre10" src="../../OEBPS/Images/08-03.png" width="1103" height="772"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1137430"></a>Figure 8.3 Samples from the PASCAL VOC 2012 data set. The data set shows a single example image, along with the annotated segmentation of it for the 20 different object classes.</p>

  <p class="body"><a class="calibre8" id="pgfId-1109641"></a>In figure 8.4, diving a bit deeper, you can see a single sample datapoint (best viewed in color) up close. It has two objects: a chair and a dog. As it is shown, different colors are assigned to different object categories. While the figure is best viewed in color, you still can distinguish different objects by paying attention to the white border that outlines the objects in the figure.</p>

  <p class="fm-figure"><img alt="08-04" class="calibre10" src="../../OEBPS/Images/08-04.png" width="703" height="261"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1137464"></a>Figure 8.4 An original input image in image segmentation and the corresponding target annotated/segmented image</p>

  <p class="body"><a class="calibre8" id="pgfId-1109648"></a>First, we’ll download the data set, if it does not exist, from <span class="fm-hyperlink"><a class="url" href="http://mng.bz/6XwZ">http://mng.bz/6XwZ</a></span> (see the next listing).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1109650"></a>Listing 8.1 Downloading data</p>
  <pre class="programlisting">import os
import requests
import tarfile
 
# Retrieve the data
if <i class="fm-italics">not</i> os.path.exists(os.path.join('data','VOCtrainval_11-May-2012.tar')): <span class="fm-combinumeral">❶</span>
    url = "http:/ /host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-
<span class="fm-code-continuation-arrow">➥</span> May-2012.tar"
    # Get the file from web
    r = requests.get(url)                                                  <span class="fm-combinumeral">❷</span>
 
    if <i class="fm-italics">not</i> os.path.exists('data'):
        os.mkdir('data')
    
    # Write to a file
    with open(os.path.join('data','VOCtrainval_11-May-2012.tar'), 'wb') as f:
        f.write(r.content)                                                 <span class="fm-combinumeral">❸</span>
else:
    print("The tar file already exists.")
    
if <i class="fm-italics">not</i> os.path.exists(os.path.join('data', 'VOCtrainval_11-May-2012')):    <span class="fm-combinumeral">❹</span>
    with tarfile.open(os.path.join('data','VOCtrainval_11-May-2012.tar'), 'r') as tar:
        tar.extractall('data')
else:
    print("The extracted data already exists")</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1136489"></a><span class="fm-combinumeral">❶</span> Check if the file is already downloaded. If so, don’t download again.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1136510"></a><span class="fm-combinumeral">❷</span> Get the content from the URL.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1136527"></a><span class="fm-combinumeral">❸</span> Save the file to disk.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1136544"></a><span class="fm-combinumeral">❹</span> If the file exists but is not extracted, extract the file.</p>

  <p class="body"><a class="calibre8" id="pgfId-1109679"></a>The data set download is quite similar to our past experience. The data exists as a tarfile. We download the file if it doesn’t exist and extract it. Next, we will discuss how to use the image library Pillow and NumPy to load the images into memory. Here, the target images will need special treatment, as you will see that they are not stored using the conventional approach. There are no surprises involved with loading input images to memory. Using the <span class="fm-code-in-text">PIL</span> (i.e., Pillow) library<a class="calibre8" id="marker-1109680"></a>, they can be loaded with a single line of code:</p>
  <pre class="programlisting">from PIL import Image
 
orig_image_path = os.path.join('data', 'VOCtrainval_11-May-2012', 
<span class="fm-code-continuation-arrow">➥</span> 'VOCdevkit', 'VOC2012', 'JPEGImages', '2007_000661.jpg')
 
orig_image = Image.open(orig_image_path)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1109686"></a>Next, you can inspect the image’s attributes:</p>
  <pre class="programlisting">print("The format of the data {}".format(orig_image.format))
&gt;&gt;&gt; The format of the data JPEG
 
print("This image is of size: {}".format(orig_image.shape))
&gt;&gt;&gt; This image is of size: (375, 500, 3)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1109692"></a>It’s time to load the corresponding annotated/segmented target images. As mentioned earlier, target images require special attention. The target images are not stored as standard images but as <i class="fm-italics">palettized</i> images<a class="calibre8" id="marker-1136779"></a><a id="marker-1116204"></a>. Palettization is a technique to reduce memory footprint while storing images with a fixed number of colors in the image. The crux of the method is to maintain a palette of colors. The palette is stored as a sequence of integers, which has a length of the number of colors or the number of channels. (E.g., in the case of RGB, where a pixel is made of three values corresponding to red, green, and blue, the number of channels is three. A grayscale image has a single channel, where each pixel is made of a single value). The image itself then stores an array of indices (size = height × width), where each index maps to a color in the palette. Finally, by mapping the palette indices from the image to palette colors, you can compute the original image. Figure 8.5 provides a visual exposition of this discussion.</p>

  <p class="fm-figure"><img alt="08-05" class="calibre10" src="../../OEBPS/Images/08-05.png" width="936" height="1075"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1137498"></a>Figure 8.5 The numerical representation of input images and target images in the PASCAL VOC 2012 data set</p>

  <p class="body"><a class="calibre8" id="pgfId-1109700"></a>The next listing shows the code for reconstructing the original image pixels from the palettized image.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1109702"></a>Listing 8.2 Reconstructing the original image from a palettized image</p>
  <pre class="programlisting">def rgb_image_from_palette(image):
 
    """ This function restores the RGB values form a palletted PNG image """
    palette = image.get_palette()                                         <span class="fm-combinumeral">❶</span>
 
    palette = np.array(pallette).reshape(-1,3)                            <span class="fm-combinumeral">❷</span>
    if isinstance(image, PngImageFile):
        h, w = image.height, image.width                                  <span class="fm-combinumeral">❸</span>
        # Squash height and width dimensions (makes slicing easier)
        image = np.array(image).reshape(-1)                               <span class="fm-combinumeral">❹</span>
    elif isinstance(image, np.ndarray):                                   <span class="fm-combinumeral">❺</span>
        h, w = image.shape[0], image.shape[1]
        image = image.reshape(-1)
        
    rgb_image = np.zeros(shape=(image.shape[0],3))                        <span class="fm-combinumeral">❻</span>
    rgb_image[(image != 0),:] = pallette[image[(image != 0)], :]          <span class="fm-combinumeral">❻</span>
    rgb_image = rgb_image.reshape(h, w, 3)                                <span class="fm-combinumeral">❼</span>
    
    return rgb_image</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1135941"></a><span class="fm-combinumeral">❶</span> Get the color palette from the image.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1135962"></a><span class="fm-combinumeral">❷</span> The palette is stored as a vector. We reshape it to an array, where each row represents a single RGB color.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1135982"></a><span class="fm-combinumeral">❸</span> Get the image’s height and width.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1135999"></a><span class="fm-combinumeral">❹</span> Convert the palettized image stored as an array to a vector (helps with our next steps).</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1136016"></a><span class="fm-combinumeral">❺</span> Get the image as a vector if the image is provided as an array instead of a Pillow image.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1136033"></a><span class="fm-combinumeral">❻</span> We first define a vector of zeros that has the same length as our image. Then, for all the indices found in the image, we gather corresponding colors from the palette and assign them to the same position in the rgb_image.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1136050"></a><span class="fm-combinumeral">❼</span> Restore the original shape.</p>

  <p class="body"><a class="calibre8" id="pgfId-1109730"></a>Here, we first obtain the palette of the image using the <span class="fm-code-in-text">get_palette()</span> function. This will be present as a one-dimensional array (of length number of classes × number of channels). Next, we need to reshape the array to a (number of classes, number of channels)-sized array. In our case, this will be converted to a (22,3)-sized array. As we define the first dimension of the reshape as -1, it will be automatically inferred from the original size of the data and the other dimensions of the reshape operation. Finally, we define an array of zeros, which will ultimately store the actual colors the indices found in the image. To do that, we index the <span class="fm-code-in-text">rgb_image</span> vector using the <span class="fm-code-in-text">image</span> (which contains indices) and assign matching colors from the palette to those indices.</p>

  <p class="body"><a class="calibre8" id="pgfId-1109732"></a>With the data we have looked at thus far, let’s define a TensorFlow data pipeline that can transform and convert the data to a format acceptable by the model.</p>

  <p class="fm-head2"><a id="pgfId-1109733"></a>Exercise 1</p>

  <p class="body"><a class="calibre8" id="pgfId-1109734"></a>You have been provided with an <span class="fm-code-in-text">rgb_image</span> in RGB format, where each pixel belongs to one of n distinctive colors and has been given a palette called palette, which is a [n,3]-sized array. How would you convert the <span class="fm-code-in-text">rgb_image</span> to a palettized<a class="calibre8" id="marker-1109736"></a> image?</p>

  <p class="fm-callout"><a id="pgfId-1109737"></a><span class="fm-callout-head">Hint</span> You can create the naïve solution by using three <span class="fm-code-in-text1">for</span> loops: two loops to get a single pixel of <span class="fm-code-in-text1">rgb_image</span> and then a final loop to traverse each color in the palette.</p>

  <h2 class="fm-head" id="sigil_toc_id_102"><a id="pgfId-1109738"></a>8.2 Getting serious: Defining a TensorFlow data pipeline</h2>

  <p class="body"><a class="calibre8" id="pgfId-1109741"></a>So<a class="calibre8" id="marker-1109739"></a><a class="calibre8" id="marker-1109740"></a> far, we have discussed the data that will help us build a navigation algorithm for the RC toy. Before building a model, an important task to complete is having a scalable data ingestion method from disk to the model. Doing this upfront will save us a lot of time when we’re ready to scale or productionize. You think the best way is to implement a <span class="fm-code-in-text">tf.data</span> pipeline to retrieve images from the disk, preprocess them, transform them, and have them ready for the model to grab them. This pipeline should read images in, reshape them to a fixed size (in the case of variable-sized images), augment them (during the training stage), batch them, and repeat this process for a desired number of epochs. Finally, we will define three pipelines: a training data pipeline, a validation data pipeline, and a testing data pipeline.</p>

  <p class="body"><a class="calibre8" id="pgfId-1109742"></a>Our goal at the end of the data exploration stage should be to build a reliable data pipeline from the disk to the model. This is what we will be looking at here. At a high level, we will build a TensorFlow data pipeline that will perform the following tasks:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1109743"></a>Get the filenames belonging to a certain subset (e.g., training, validation, or testing).</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1109744"></a>Read the specified images from the disk.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1109745"></a>Preprocess the images (this involves normalizing/resizing/cropping images).</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1109746"></a>Perform augmentation on the images to increase the volume of data.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1109747"></a>Batch the data in small batches.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1109748"></a>Optimize data retrieval using several built-in optimization techniques.</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1109749"></a>As the first step, we will write a function that returns a generator that will generate filenames of the data that we want to be fetched. We will also provide the ability to specify which subset the user wants to be fetched (e.g., training, validation, or testing). Returning data through a generator will make writing a <span class="fm-code-in-text">tf.data</span> pipeline<a class="calibre8" id="marker-1109750"></a> easier (see the following listing).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1109752"></a>Listing 8.3 Retrieving the filenames for a given subset of data</p>
  <pre class="programlisting">def get_subset_filenames(orig_dir, seg_dir, subset_dir, subset):
    """ Get the filenames for a given subset (train/valid/test)"""
 
    if subset.startswith('train'):
        ser = pd.read_csv(                                            <span class="fm-combinumeral">❶</span>
            os.path.join(subset_dir, "train.txt"), 
            index_col=None, header=None, squeeze=True
        ).tolist()
        elif subset.startswith('val') or subset.startswith('test'):
 
        random.seed(random_seed)                                      <span class="fm-combinumeral">❷</span>
 
        ser = pd.read_csv(                                            <span class="fm-combinumeral">❸</span>
            os.path.join(subset_dir, "val.txt"), 
            index_col=None, header=None, squeeze=True
        ).tolist()
  
        random.shuffle(ser)                                           <span class="fm-combinumeral">❹</span>
 
        if subset.startswith('val'):
            ser = ser[:len(ser)//2]                                   <span class="fm-combinumeral">❺</span>
        else:
            ser = ser[len(ser)//2:]                                   <span class="fm-combinumeral">❻</span>
    else:
        raise NotImplementedError("Subset={} is not recognized".format(subset))
    
    orig_filenames = [os.path.join(orig_dir,f+'.jpg') for f in ser]   <span class="fm-combinumeral">❼</span>
    seg_filenames = [os.path.join(seg_dir, f+'.png') for f in ser]    <span class="fm-combinumeral">❽</span>
    
    for o, s in zip(orig_filenames, seg_filenames):
        yield o, s                                                    <span class="fm-combinumeral">❾</span></pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1135281"></a><span class="fm-combinumeral">❶</span> Read the CSV file that contains the training instance filenames.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1135305"></a><span class="fm-combinumeral">❷</span> For validation/test subsets, perform a one-time shuffle to make sure we get a good mix with a fixed seed.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1135322"></a><span class="fm-combinumeral">❸</span> Read the CSV file that contains validation/test filenames.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1135339"></a><span class="fm-combinumeral">❹</span> Shuffle the data after fixing the seed.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1135356"></a><span class="fm-combinumeral">❺</span> Get the first half as the validation set.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1135373"></a><span class="fm-combinumeral">❻</span> Get the second half as the test set.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1135390"></a><span class="fm-combinumeral">❼</span> Form absolute paths to the input image files we captured (depending on the subset argument).</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1135407"></a><span class="fm-combinumeral">❽</span> Return the filename pairs (input and annotations) as a generator.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1135424"></a><span class="fm-combinumeral">❾</span> Form absolute paths to the segmented image files.</p>

  <p class="body"><a class="calibre8" id="pgfId-1109793"></a>You can see that we’re passing a few arguments when reading the CSV files. These arguments characterize the file we’re reading. These files are extremely simple and contain just a single image filename on a single line. <span class="fm-code-in-text">index_col=None</span> means that the file does not have an index column, <span class="fm-code-in-text">header=None</span> means there is no header in the file<a class="calibre8" id="marker-1109794"></a>, and <span class="fm-code-in-text">squeeze=True</span> means that the output will be presented as a pandas <span class="fm-code-in-text">Series</span>, not a pandas <span class="fm-code-in-text">Dataframe</span>. With that, we can define a TensorFlow data set (<span class="fm-code-in-text">tf.data.Dataset</span>) as follows:</p>
  <pre class="programlisting">filename_ds = tf.data.Dataset.from_generator(
        subset_filename_gen_func, output_types=(tf.string, tf.string)
    )</pre>

  <p class="body"><a class="calibre8" id="pgfId-1109798"></a>TensorFlow has several different functions for generating data sets using different sources. As we have defined the function <span class="fm-code-in-text">get_subset_filenames()</span> to return a generator, we will use the <span class="fm-code-in-text">tf.data.Dataset.from_generator()</span> function<a class="calibre8" id="marker-1109799"></a>. Note that we need to provide the format as well as the datatypes of the returned data, by the generator, using the <span class="fm-code-in-text">output_types</span> argument. The function <span class="fm-code-in-text">subset_filename_gen_func</span> returns two strings; therefore, we define output types as a tuple of two <span class="fm-code-in-text">tf.string</span> elements<a class="calibre8" id="marker-1109801"></a>.</p>

  <p class="body"><a class="calibre8" id="pgfId-1109802"></a>One other important aspect is the different txt files we read from depending on the subset. There are three different files in the relative path: the <span class="fm-code-in-text">data\VOCtrainval_11-May-2012\VOCdevkit\VOC2012\ImageSets\Segmentation</span> folder; <span class="fm-code-in-text">train.txt</span>, <span class="fm-code-in-text">val.txt</span>, and <span class="fm-code-in-text">trainval.txt</span>. Here, train.txt contains the filenames of the training images, whereas <span class="fm-code-in-text">val.txt</span> contains the filenames of the validation/testing images. We will use these files to create different pipelines that produce different data.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1109804"></a>Where does tf.data come from?</p>

    <p class="fm-sidebar-text"><a id="pgfId-1109805"></a>TensorFlow’s tf.data pipeline can consume data from various sources. Here are some of the commonly used methods to retrieve data:</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1109806"></a><span class="fm-code-in-text1">tf.data.Dataset.from_generator(gen_fn</span>)—You have already seen this function in action. If you have a generator (i.e., <span class="fm-code-in-text1">gen_fn</span>) that produces data, you want it to be processed through a <span class="fm-code-in-text1">tf.data</span> pipeline<a id="marker-1114303"></a>. This is the easiest method to use.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1109808"></a><span class="fm-code-in-text1">tf.data.Dataset.from_tensor_slices(t)</span>—This is a very useful function if you have data already loaded as a big matrix. t can be an N-dimensional matrix, and this function will extract element by element on the first dimension. For example, assume that you have loaded a tensor t of size 3 × 4 to memory:</p>
    <pre class="programlisting">t = [ [1,2,3,4],
      [2,3,4,5],
      [6,7,8,9] ]</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1109814"></a>Then you can easily set up a <span class="fm-code-in-text1">tf.data</span> pipeline<a id="marker-1117372"></a> as follows. <span class="fm-code-in-text1">tf.data.Dataset.from_ tensor_slices(t)</span> <i class="fm-italics">will return</i> <span class="fm-code-in-text1">[1,2,3,4]</span>, <i class="fm-italics">then</i> <span class="fm-code-in-text1">[2,3,4,5]</span>, <i class="fm-italics">and finally</i> <span class="fm-code-in-text1">[6,7,8,9]</span> when you iterate this data pipeline. In other words, you are seeing one row (i.e., a slice from the batch dimension, hence the name <span class="fm-code-in-text1">from_tensor_slices</span>) at a time. You can now incorporate functions like <span class="fm-code-in-text1">tf.data.Dataset.batch</span><a id="marker-1117374"></a><span class="fm-code-in-text1">()</span> to get a batch of rows.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1109817"></a>Now it’s time to read in the images found in the file paths we obtained in the previous step. TensorFlow has support to easily load an image, where the path to a filename is <span class="fm-code-in-text">img_filename</span>, using the functions <span class="fm-code-in-text">tf.io.read_file</span> and <span class="fm-code-in-text">tf.image.decode_image</span>. Here, <span class="fm-code-in-text">img_filename</span> is a <span class="fm-code-in-text">tf.string</span> (i.e., a string in TensorFlow):</p>
  <pre class="programlisting">tf.image.decode_jpeg(tf.io.read_file(image_filename))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1109819"></a>We will use this pattern to load input images. However, we need to implement a custom image load function to load the target image. If you use the previous approach, it will automatically convert the image to an array with pixel values (instead of palette indices). But if we don’t perform that conversion, we will have a target array that is in the exact format we need because the palette indices that are in the target image are the actual class labels for each corresponding pixel in the input image. We will use <span class="fm-code-in-text">PIL.Image</span> within our TensorFlow data pipeline to load the image as a palettized image and avoid converting it to RGB:</p>
  <pre class="programlisting">from PIL import Image
 
def load_image_func(image):
    """ Load the image given a filename """
    
    img =  np.array(Image.open(image))        
    return img</pre>

  <p class="body"><a class="calibre8" id="pgfId-1109828"></a>However, you can’t yet use custom functions as part of the <span class="fm-code-in-text">tf.data</span> pipeline<a class="calibre8" id="marker-1109827"></a>. They need to be streamlined with the data-flow graph of the data pipeline by wrapping it as a TensorFlow operation. This can be easily achieved by using the <span class="fm-code-in-text">tf.numpy_function</span> operation<a class="calibre8" id="marker-1109829"></a>, which allows you to wrap a custom function that returns a NumPy array as a TensorFlow operation. If we have the target image’s file path represented by <span class="fm-code-in-text">y</span>, you can use the following code to load the image into TensorFlow with a custom image-loading function:</p>
  <pre class="programlisting">tf.numpy_function(load_image_func, inp=[y], Tout=[tf.uint8])</pre>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1109831"></a>The dark side of tf.numpy_function</p>

    <p class="fm-sidebar-text"><a id="pgfId-1109832"></a>NumPy has larger coverage for various scientific computations than TensorFlow, so you might think that <span class="fm-code-in-text1">tf.numpy_funtion</span> makes things very convenient. This is not quite true, as you can infest your TensorFlow code with terrible performance degradations. When TensorFlow executes NumPy code, it can create very inefficient data flow graphs and introduce overheads. Therefore, always try to stick to TensorFlow operations and use custom NumPy code only if you have to. In our case, since there is no alternative way for us to load a palletized image without mapping palletized values to actual RGB, we used a custom function.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1109834"></a>Notice how we’re passing both the input (i.e., <span class="fm-code-in-text">inp=[y]</span>) and its data type (i.e., <span class="fm-code-in-text">Tout=[tf.uint8]</span>) to this function. They both need to be in the form of a Python list. Finally, let’s collate everything we discussed in one place:</p>
  <pre class="programlisting">def load_image_func(image):
    """ Load the image given a filename """
    
    img =  np.array(Image.open(image))        
    return img
    
# Load the images from the filenames returned by the above step
    image_ds = filename_ds.map(lambda x,y: (
        tf.image.decode_jpeg(tf.io.read_file(x)), 
        tf.numpy_function(load_image_func, [y], [tf.uint8])
    ))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1109847"></a>The <span class="fm-code-in-text">tf.data.Dataset.map()</span> function<a class="calibre8" id="marker-1109846"></a> will be used quite heavily throughout this discussion. You can find a lengthy explanation of the <span class="fm-code-in-text">map()</span> function<a class="calibre8" id="marker-1109848"></a> in the sidebar.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1109849"></a><a id="marker-1114394"></a>A Refresher: tf.data.Dataset.map() function</p>

    <p class="fm-sidebar-text"><a id="pgfId-1109851"></a>This <span class="fm-code-in-text1">tf.data</span> pipeline<a id="marker-1114391"></a> will make extensive use of the <span class="fm-code-in-text1">tf.data.Dataset.map()</span> function<a id="marker-1114392"></a>. Therefore, it is extremely helpful for us to remind ourselves what this function accomplishes.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1109854"></a>The <span class="fm-code-in-text1">td.data.Dataset.map()</span> function<a id="marker-1136834"></a> applies a given function or functions across all the records in a data set. In other words, it transforms the data points in the data set using a specified transformation. For example, assume the <span class="fm-code-in-text1">tf.data.Dataset</span></p>
    <pre class="programlisting">  dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4])</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1109857"></a>to get the square of each element, you can use the map function as</p>
    <pre class="programlisting">  dataset = dataset.map(lambda x: x**2)</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1109860"></a>If you have multiple elements in a single record, leveraging the flexibility of <span class="fm-code-in-text1">map()</span>, you can transform them individually:</p>
    <pre class="programlisting">  dataset = tf.data.Dataset.from_tensor_slices([[1,3], [2,4], [3,5], [4,6]])
  dataset = dataset.map(lambda x, y: (x**2, y+x))
which will return,
[[1, 4], [4, 6], [9, 8], [16, 10]]</pre>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1109866"></a>As a normalization step we will bring the pixel values to [0,1] range by using</p>
  <pre class="programlisting">image_ds = image_ds.map(lambda x, y: (tf.cast(x, 'float32')/255.0, y))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1109868"></a>Note that we are keeping our target image (<span class="fm-code-in-text">y</span>) as it is. Before I continue with any more steps in our pipeline, I want to direct your attention to an important matter. This is a caveat that is quite common, and it is thus worthwhile to be aware of it. After the step we just completed, you might feel like, if you want, you can batch the data and feed it to the model. For example</p>
  <pre class="programlisting">image_ds = image_ds.batch(10)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1109870"></a>If you do that for this data set, you will get an error like the following:</p>
  <pre class="programlisting">InvalidArgumentError: Cannot batch tensors with different shapes in 
<span class="fm-code-continuation-arrow">➥</span> component 0. First element had shape [375,500,3] and element 1 had 
<span class="fm-code-continuation-arrow">➥</span> shape [333,500,3]. [Op:IteratorGetNext]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1109872"></a>This is because you ignored a crucial characteristic and a sanity check of the data set. Unless you’re using a curated data set, you are unlikely to find images with the same dimensions. If you look at images in the data set, you will notice that they are not of the same size; they have different heights and widths. In TensorFlow, unless you use a special data structure like <span class="fm-code-in-text">tf.RaggedTensor</span>, you cannot batch unequally sized images together. That is exactly what TensorFlow is complaining about in the error.</p>

  <p class="body"><a class="calibre8" id="pgfId-1109873"></a>To alleviate the problem, we need to bring all the images to a standard size (see listing 8.4). To do that, we will define the following function. It will either</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1109874"></a>Resize the image to a larger size (<span class="fm-code-in-text">resize_to_before_crop</span>) and then crop the image to the desired size (<span class="fm-code-in-text">input_size</span>) or</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1109875"></a>Resize the image to the desired size (<span class="fm-code-in-text">input_size</span>)</p>
    </li>
  </ul>

  <p class="fm-code-listing-caption"><a id="pgfId-1109877"></a>Listing 8.4 Bringing images to a fixed size using random cropping or resizing</p>
  <pre class="programlisting">def randomly_crop_or_resize(x,y):
    """ Randomly crops or resizes the images """
 
    def rand_crop(x, y):                                                  <span class="fm-combinumeral">❶</span>
        """ Randomly crop images after enlarging them """
        x = tf.image.resize(x, resize_to_before_crop, method='bilinear')  <span class="fm-combinumeral">❷</span>
        y = tf.cast(                                                      <span class="fm-combinumeral">❸</span>
                tf.image.resize(
                    tf.transpose(y,[1,2,0]),                              <span class="fm-combinumeral">❹</span>
                    resize_to_before_crop, method='nearest'
                ),
                'float32'
            )          
 
        offset_h = tf.random.uniform(
            [], 0, x.shape[0]-input_size[0], dtype='int32'
        )                                                                 <span class="fm-combinumeral">❺</span>
        offset_w = tf.random.uniform(
            [], 0, x.shape[1]-input_size[1], dtype='int32'
        )                                                                 <span class="fm-combinumeral">❻</span>
        x = tf.image.crop_to_bounding_box(
            image=x, 
            offset_height=offset_h, offset_width=offset_w,
            target_height=input_size[0], target_width=input_size[1]       <span class="fm-combinumeral">❼</span>
        )
        y = tf.image.crop_to_bounding_box(
            image=y, 
            offset_height=offset_h, offset_width=offset_w,
            target_height=input_size[0], target_width=input_size[1]       <span class="fm-combinumeral">❼</span>
        )
 
        return x, y
 
    def resize(x, y):
        """ Resize images to a desired size """
        x = tf.image.resize(x, input_size, method='bilinear')             <span class="fm-combinumeral">❽</span>
        y = tf.cast(
                tf.image.resize(
                    tf.transpose(y,[1,2,0]),                                        
                    input_size, method='nearest'                          <span class="fm-combinumeral">❽</span>
                ),
                'float32'
            )          
 
        return x, y
 
    rand = tf.random.uniform([], 0.0,1.0)                                 <span class="fm-combinumeral">❾</span>
 
    if augmentation and \                                                 <span class="fm-combinumeral">❿</span>
        (input_size[0] &lt; resize_to_before_crop[0] or \
         input_size[1] &lt; resize_to_before_crop[1]):
        x, y = tf.cond(
                rand &lt; 0.5,                                               <span class="fm-combinumeral">⓫</span>
                lambda: rand_crop(x, y),
                 lambda: resize(x, y)
                )
        else:
            x, y = resize(x, y)                                           <span class="fm-combinumeral">⓬</span>
 
        return x, y</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1134330"></a><span class="fm-combinumeral">❶</span> Define a function to randomly crop images after resizing.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1134309"></a><span class="fm-combinumeral">❷</span> Resize the input image using bilinear interpolation to a larger size.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1134347"></a><span class="fm-combinumeral">❸</span> Resize the target image using the nearest interpolation to a larger size.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1134364"></a><span class="fm-combinumeral">❹</span> To resize, we first swap the axis of y as it has the shape [1, height, width]. We convert this back to [height, width, 1] (i.e., a single channel image) using the tf.transpose() function.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1134381"></a><span class="fm-combinumeral">❺</span> Define a random variable to offset images on height during cropping.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1134398"></a><span class="fm-combinumeral">❻</span> Define a random variable to offset images on width during cropping.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1134415"></a><span class="fm-combinumeral">❼</span> Crop the input image and the target image using the same cropping parameters.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1134435"></a><span class="fm-combinumeral">❽</span> Resize both the input image and the target image to a desired size (no cropping).</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1134452"></a><span class="fm-combinumeral">❾</span> Define a random variable (used to perform augmentations).</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1134469"></a><span class="fm-combinumeral">❿</span> If augmentation is enabled and the resized image is larger than the input size we requested, perform augmentation.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1134486"></a><span class="fm-combinumeral">⓫</span> During augmentation, the rand_crop or resize function is executed randomly.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1134503"></a><span class="fm-combinumeral">⓬</span> If augmentation is disabled, only resize images.</p>

  <p class="body"><a class="calibre8" id="pgfId-1109951"></a>Here, we define a function called <span class="fm-code-in-text">randomly_crop_or_resize</span><a class="calibre8" id="marker-1109950"></a>, which has two nested functions, <span class="fm-code-in-text">rand_crop</span> and <span class="fm-code-in-text">resize</span><a class="calibre8" id="marker-1109953"></a>. The <span class="fm-code-in-text">rand_crop</span> first resizes the image to the size specified in <span class="fm-code-in-text">resize_to_before_crop</span> and creates a random crop. It is imperative to check that you applied the exact same crop to both the input and the target. For example, same-crop parameters should be used to crop both the input and the target. In order to crop images, we use</p>
  <pre class="programlisting">x = tf.image.crop_to_bounding_box(
    image=x, 
    offset_height=offset_h, offset_width=offset_w,
    target_height=input_size[0], target_width=input_size[1]                 
)
y = tf.image.crop_to_bounding_box(
    image=y, 
    offset_height=offset_h, offset_width=offset_w,
    target_height=input_size[0], target_width=input_size[1]               
)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1109966"></a>The arguments are self-explanatory: <span class="fm-code-in-text">image</span> takes an image to be cropped, <span class="fm-code-in-text">offset_height</span> and <span class="fm-code-in-text">offset_width</span> decide the starting point for the crop, and <span class="fm-code-in-text">target_height</span> and <span class="fm-code-in-text">target_width</span> specify the final size after the crop. The resize function will simply resize the input and the target to a specified size using the <span class="fm-code-in-text">tf.image.resize</span> operation<a class="calibre8" id="marker-1109967"></a>.</p>

  <p class="body"><a class="calibre8" id="pgfId-1109968"></a>When resizing, we use <i class="fm-italics">bilinear interpolation</i> for the input images and <i class="fm-italics">nearest interpolation</i> for targets. Bilinear interpolation resizes the images by computing the resulting pixels, as an average of neighboring pixels, whereas nearest interpolation computes the output pixel as the nearest most common pixel from the neighbors. Bilinear interpolation leads to a smoother result after resizing. However, you must use nearest interpolation<a class="calibre8" id="marker-1109969"></a><a class="calibre8" id="marker-1109970"></a> for the target image, as bilinear interpolation will lead to fractional outputs, corrupting the integer-based annotations. The interpolation techniques described are visualized in figure 8.6.</p>

  <p class="fm-figure"><img alt="08-06" class="calibre10" src="../../OEBPS/Images/08-06.png" width="1003" height="922"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1137532"></a>Figure 8.6 Nearest interpolation and bilinear interpolation for both up-sampling and down-sampling tasks</p>

  <p class="body"><a class="calibre8" id="pgfId-1109977"></a>Next, we will introduce an additional step to the way we’re going to use these two nested functions. If augmentation is enabled, we want the cropping or resizing to take place randomly within the pipeline. We will define a random variable (drawn from a uniform distribution between 0 and 1) and perform crop or resize depending on the value of the random variable at a given time. This conditioning can be achieved using the <span class="fm-code-in-text">tf.cond</span> function<a class="calibre8" id="marker-1109978"></a>, which takes three arguments and returns output according to these arguments:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1109979"></a><span class="fm-code-in-text">Condition</span>—This is a computation that results in a Boolean value (i.e., is the random variable <span class="fm-code-in-text">rand</span> greater than 0.5).</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1109980"></a><span class="fm-code-in-text">true_fn</span>—If the condition is true, then this function will be executed (i.e., perform <span class="fm-code-in-text">rand_crop</span> on both <span class="fm-code-in-text">x</span> and <span class="fm-code-in-text">y</span>)</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1109981"></a><span class="fm-code-in-text">false_fn</span>—If the condition is false, then this function will be executed (i.e., perform a resize on both <span class="fm-code-in-text">x</span> and <span class="fm-code-in-text">y</span>)</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1109982"></a>If augmentation is disabled (i.e., by setting the augmentation variable to <span class="fm-code-in-text">False</span>), only resizing is performed. With the details fleshed out, we can use the <span class="fm-code-in-text">randomly_crop_ or_resize</span> function<a class="calibre8" id="marker-1109983"></a> in our data pipeline as follows:</p>
  <pre class="programlisting">    image_ds = image_ds.map(lambda x,y: randomly_crop_or_resize(x,y))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1109985"></a>At this point, we have a globally fixed-sized image coming out of our pipeline. The next thing we address is very important. Factors such as variable size of images and custom NumPy functions used to load images make it impossible for TensorFlow to infer the shape of its final tensor (though it’s a fixed-sized tensor) after a few steps. If you check the shapes of the tensors produced at this point, you will probably perceive them as</p>
  <pre class="programlisting">(None, None, None)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1109987"></a>This means that TensorFlow was unable to infer the shape of the tensors. To avoid any ambiguities or problems moving forward, we will set the shape of the output we have in the pipeline. For a tensor <span class="fm-code-in-text">t</span>, if the shape is ambiguous but you know the shape, you can set the shape manually using</p>
  <pre class="programlisting">t.set_shape([&lt;shape of the tensor&gt;])</pre>

  <p class="body"><a class="calibre8" id="pgfId-1109989"></a>In our data pipeline, we can set the shape as</p>
  <pre class="programlisting">def fix_shape(x, y, size):
    """ Set the shape of the input/target tensors """
    
    x.set_shape((size[0], size[1], 3))
    y.set_shape((size[0], size[1], 1))
    
    return x, y
 
image_ds = image_ds.map(lambda x,y: fix_shape(x,y, target_size=input_size))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1109999"></a>We know that the outputs following the resize or crop are going to be</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1110000"></a><i class="fm-italics">Input image</i>—An RGB image with <span class="fm-code-in-text">input_size</span> height and width</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1110003"></a><i class="fm-italics">Target image</i>—A single-channel image with <span class="fm-code-in-text">input_size</span> height and width</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1110005"></a>We will set the shape accordingly using the <span class="fm-code-in-text">tf.data.Dataset.map()</span> function<a class="calibre8" id="marker-1110004"></a>. We cannot underestimate the power of data augmentation, so we will introduce several data augmentation steps to our data pipeline (see the next listing).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1110007"></a>Listing 8.5 Functions used for random augmentation of images</p>
  <pre class="programlisting">def randomly_flip_horizontal(x, y):
    """ Randomly flip images horizontally. """
    
    rand = tf.random.uniform([], 0.0,1.0)                                           <span class="fm-combinumeral">❶</span>
 
    def flip(x, y):
        return tf.image.flip_left_right(x), tf.image.flip_left_right(y)             <span class="fm-combinumeral">❷</span>
 
    x, y = tf.cond(rand &lt; 0.5, lambda: flip(x, y), lambda: (x, y))                  <span class="fm-combinumeral">❸</span>
 
    return x, y
 
if augmentation:    
    image_ds = image_ds.map(lambda x, y: randomly_flip_horizontal(x,y))             <span class="fm-combinumeral">❹</span>
 
    image_ds = image_ds.map(lambda x, y: (tf.image.random_hue(x, 0.1), y))          <span class="fm-combinumeral">❺</span>
 
    image_ds = image_ds.map(lambda x, y: (tf.image.random_brightness(x, 0.1), y))   <span class="fm-combinumeral">❻</span>
 
    image_ds = image_ds.map(lambda x, y: (tf.image.random_contrast(x, 0.8, 1.2), y))<span class="fm-combinumeral">❼</span></pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1133871"></a><span class="fm-combinumeral">❶</span> Define a random variable.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1133892"></a><span class="fm-combinumeral">❷</span> Define a function to flip images deterministically.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1133912"></a><span class="fm-combinumeral">❸</span> Using the same pattern as before, we use tf.cond to randomly perform horizontal flipping.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1133929"></a><span class="fm-combinumeral">❹</span> Randomly flip images in the data set.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1133946"></a><span class="fm-combinumeral">❺</span> Randomly adjust the hue (i.e., color) of the input image (target stays the same).</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1133963"></a><span class="fm-combinumeral">❻</span> Randomly adjust the brightness of the input image (target stays the same).</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1133980"></a><span class="fm-combinumeral">❼</span> Randomly adjust the contrast of the input image (target stays the same).</p>

  <p class="body"><a class="calibre8" id="pgfId-1110035"></a>In listing 8.5, we perform the following translations:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1110036"></a>Randomly flipping images horizontally</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1110037"></a>Randomly changing the hue of the images (up to 10%)</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1110038"></a>Randomly changing the brightness of the images (up to 10%)</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1110039"></a>Randomly changing the contrast of the images (up to 20%)</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1110041"></a>By using the <span class="fm-code-in-text">tf.data.Dataset.map()</span> function,<a class="calibre8" id="marker-1110040"></a> we can easily perform the specified random augmentation steps, should the user enable augmentation in the pipeline (i.e., by setting the augmentation variable to <span class="fm-code-in-text">True</span>). Note that we’re performing some augmentations (e.g., random hue, brightness, and contrast adjustments) on the input image only. We will also give the user the option to have different-sized inputs and targets (i.e., outputs). This is achieved by resizing the output to a desired size, defined by the <span class="fm-code-in-text">output_size</span> argument. The model we use for this task has different-sized input and output dimensions:</p>
  <pre class="programlisting">if output_size:
    image_ds = image_ds.map(
                   lambda x, y: (
                       x, 
                       tf.image.resize(y, output_size, method='nearest')
                   )
    )</pre>

  <p class="body"><a class="calibre8" id="pgfId-1110050"></a>Again, here we use the nearest interpolation to resize the target. Next, we will shuffle the data (if the user set the shuffle argument to <span class="fm-code-in-text">True</span>):</p>
  <pre class="programlisting">if shuffle:
    image_ds = image_ds.shuffle(buffer_size=batch_size*5)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1110053"></a>The shuffle function takes an important argument called <span class="fm-code-in-text">buffer_size</span>, which determines how many samples are loaded to memory in order to select a sample randomly. The higher the <span class="fm-code-in-text">buffer_size</span>, the more randomness you are introducing. On the other hand, a higher <span class="fm-code-in-text">buffer_size</span> implies higher memory consumption. It’s now time to batch the data, so instead of a single data point, we get a batch of data when we iterate:</p>
  <pre class="programlisting">image_ds = image_ds.batch(batch_size).repeat(epochs)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1110056"></a>This is done using the <span class="fm-code-in-text">tf.data.Dataset.batch()</span> function<a class="calibre8" id="marker-1110055"></a> and passing the desired batch size as the argument. When using the <span class="fm-code-in-text">tf.data</span> pipeline<a class="calibre8" id="marker-1110057"></a>, if you are running it for multiple epochs, you also need to use the <span class="fm-code-in-text">tf.data.Dataset.repeat()</span> function<a class="calibre8" id="marker-1110058"></a> to repeat the pipeline for a given number of epochs.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1110059"></a>Why do we need tf.data.Dataset.repeat()?</p>

    <p class="fm-sidebar-text"><a id="pgfId-1110060"></a><span class="fm-code-in-text1">tf.data.Dataset</span> is a generator. A unique characteristic of a generator is that you only can iterate it once. After the generator reaches the end of the sequence it’s iterating, it will exit by throwing an exception. Therefore, if you need to iterate through a generator multiple times, you need to redefine the generator as many times as needed. By adding <span class="fm-code-in-text1">tf.data.Dataset.repeat(epochs)</span>, the generate is redefined as many times as we would like (<span class="fm-code-in-text1">epochs</span> times in this example).</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1110063"></a>One more step is needed before our <span class="fm-code-in-text">tf.data</span> pipeline<a class="calibre8" id="marker-1110062"></a> is done and dusted. If you look at the shape of the target (<span class="fm-code-in-text">y</span>) output, you will see that it has a channel dimension of 1. However, for the loss function we will be using, we need to get rid of that dimension:</p>
  <pre class="programlisting">image_ds = image_ds.map(lambda x, y: (x, tf.squeeze(y)))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1110066"></a>For this, we will use the <span class="fm-code-in-text">tf.squeeze()</span> operation<a class="calibre8" id="marker-1110065"></a>, which removes any dimensions that are of size 1 and returns a tensor. For example, if you squeeze a tensor of size [1,3,2,1,5], you will get a [3,2,5] sized tensor. The final code is provided in listing 8.6. You might notice two steps that are highlighted. These are two popular optimization steps available: caching and prefetching.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1110068"></a>Listing 8.6 The final tf.data pipeline</p>
  <pre class="programlisting">def get_subset_tf_dataset(
    subset_filename_gen_func, batch_size, epochs, 
    input_size=(256, 256), output_size=None, resize_to_before_crop=None, 
    augmentation=False, shuffle=False
):
    
    if augmentation and not resize_to_before_crop:
        raise RuntimeError(                                                          <span class="fm-combinumeral">❶</span>
            "You must define resize_to_before_crop when augmentation is enabled."
        )
        
    filename_ds = tf.data.Dataset.from_generator(
        subset_filename_gen_func, output_types=(tf.string, tf.string)                <span class="fm-combinumeral">❷</span>
    )
 
<b class="fm-bold">    image_ds = filename_ds.map(lambda x,y: (</b>
<b class="fm-bold">        tf.image.decode_jpeg(tf.io.read_file(x)), </b>                                   <span class="fm-combinumeral">❸</span>
<b class="fm-bold">        tf.numpy_function(load_image_func, [y], [tf.uint8])</b>
<b class="fm-bold">    )).cache()</b>
            
    image_ds = image_ds.map(lambda x, y: (tf.cast(x, 'float32')/255.0, y))           <span class="fm-combinumeral">❹</span>
    
    
    def randomly_crop_or_resize(x,y):                                                <span class="fm-combinumeral">❺</span>
        """ Randomly crops or resizes the images """
        ...
 
        def rand_crop(x, y):
            """ Randomly crop images after enlarging them """
            ...
 
        def resize(x, y):
            """ Resize images to a desired size """
            ...
 
    image_ds = image_ds.map(lambda x,y: randomly_crop_or_resize(x,y))                <span class="fm-combinumeral">❻</span>
    image_ds = image_ds.map(lambda x,y: fix_shape(x,y, target_size=input_size))      <span class="fm-combinumeral">❼</span>
    
    if augmentation:    
        image_ds = image_ds.map(lambda x, y: randomly_flip_horizontal(x,y))          <span class="fm-combinumeral">❽</span>
        image_ds = image_ds.map(lambda x, y: (tf.image.random_hue(x, 0.1), y))       <span class="fm-combinumeral">❽</span>
        image_ds = image_ds.map(lambda x, y: (tf.image.random_brightness(x, 0.1), y))<span class="fm-combinumeral">❽</span>
        image_ds = image_ds.map(
            lambda x, y: (tf.image.random_contrast(x, 0.8, 1.2), y)                  <span class="fm-combinumeral">❽</span>
        )
    
    if output_size:
        image_ds = image_ds.map(
            lambda x, y: (x, tf.image.resize(y, output_size,  method='nearest'))     <span class="fm-combinumeral">❾</span>
        )
        
    if shuffle:
        image_ds = image_ds.shuffle(buffer_size=batch_size*5)                        <span class="fm-combinumeral">❿</span>
    image_ds = image_ds.batch(batch_size).repeat(epochs)                             <span class="fm-combinumeral">⓫</span>
    
    <b class="fm-bold">image_ds = image_ds.prefetch(tf.data.experimental.AUTOTUNE)   </b>                   <span class="fm-combinumeral">⓬</span>
    
    image_ds = image_ds.map(lambda x, y: (x, tf.squeeze(y)))                         <span class="fm-combinumeral">⓭</span>
    
    return image_ds                                                                  <span class="fm-combinumeral">⓮</span></pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1132818"></a><span class="fm-combinumeral">❶</span> If augmentation is enabled, resize_to_before_crop needs to be defined.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1132839"></a><span class="fm-combinumeral">❷</span> Return a list of filenames depending on the subset of data requested.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1132856"></a><span class="fm-combinumeral">❸</span> Load the images into memory. cache() is an optimization step and will be discussed in the text.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1132880"></a><span class="fm-combinumeral">❹</span> Normalize the input images.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1132900"></a><span class="fm-combinumeral">❺</span> The function that randomly crops or resizes images</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1132917"></a><span class="fm-combinumeral">❻</span> Perform random crop or resize on the images.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1132934"></a><span class="fm-combinumeral">❼</span> Set the shape of the resulting images.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1132951"></a><span class="fm-combinumeral">❽</span> Randomly perform various augmentations on the data.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1132968"></a><span class="fm-combinumeral">❾</span> Resize the output image if needed.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1132985"></a><span class="fm-combinumeral">❿</span> Shuffle the data using a buffer.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1133002"></a><span class="fm-combinumeral">⓫</span> Batch the data and repeat the process for a desired number of epochs.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1133019"></a><span class="fm-combinumeral">⓬</span> This is an optimization step discussed in detail in the text.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1133036"></a><span class="fm-combinumeral">⓭</span> Remove the unnecessary dimension from target images.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1133053"></a><span class="fm-combinumeral">⓮</span> Get the final tf.data pipeline.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110144"></a>It wasn’t an easy journey, but it was a rewarding one. We have learned some important skills in defining the data pipeline:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1110145"></a>Defining a generator that returns the filenames of the data to be fetched</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1110147"></a>Loading images within a <span class="fm-code-in-text">tf.data</span> pipeline<a class="calibre8" id="marker-1110146"></a></p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1110148"></a>Manipulating images (resizing, cropping, brightness adjustment, etc.)</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1110149"></a>Batching and repeating data</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1110150"></a>Defining multiple pipelines for different data sets with different requirements</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1110151"></a>Next, we will look at some optimization techniques to turn our mediocre data pipeline into an impressive data highway.</p>

  <h3 class="fm-head1" id="sigil_toc_id_103"><a id="pgfId-1110152"></a>8.2.1 Optimizing tf.data pipelines</h3>

  <p class="body"><a class="calibre8" id="pgfId-1110156"></a>TensorFlow<a class="calibre8" id="marker-1110153"></a><a class="calibre8" id="marker-1110154"></a><a class="calibre8" id="marker-1110155"></a> is a framework meant for consuming large data sets, where consuming data in an efficient manner is a key priority. One thing still missing from our conversation is what kind of optimization steps are available for <span class="fm-code-in-text">tf.data</span> pipelines, so let us nudge this discussion in that direction. Two steps were set in bold in listing 8.6: caching and prefetching. If you are interested in other optimization techniques, you can read more at <span class="fm-hyperlink"><a class="url" href="https://www.tensorflow.org/guide/data_performance">https://www.tensorflow.org/guide/data_performance</a></span>.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110157"></a>Caching will store the data in memory as it flows through the pipeline. This means that, when cached, that step (e.g., loading the data from the disk) happens only in the first epoch. The subsequent epochs will read from the cached data that’s held in memory. Here, you can see that we’re caching the images after we load them to memory. This way, TensorFlow loads the images in the first epoch only:</p>
  <pre class="programlisting"><b class="fm-code-bold">image_ds = filename_ds.map(lambda x,y: (</b>
<b class="fm-code-bold">        tf.image.decode_jpeg(tf.io.read_file(x)), </b>
<b class="fm-code-bold">        tf.numpy_function(load_image_func, [y], [tf.uint8])</b>
<b class="fm-code-bold">)).cache()</b></pre>

  <p class="body"><a class="calibre8" id="pgfId-1110162"></a>Prefetching is another powerful weapon you have at your disposal, and it allows you to leverage the multiprocessing power of your device:</p>
  <pre class="programlisting">image_ds = image_ds.<b class="fm-code-bold">prefetch(tf.data.experimental.AUTOTUNE)</b></pre>

  <p class="body"><a class="calibre8" id="pgfId-1110164"></a>The argument provided to the function decides how much data is prefetched. By setting it to <span class="fm-code-in-text">AUTOTUNE</span>, TensorFlow will decide the best amount of data to be fetched depending on the resources available. Assume a simple data pipeline that loads images from the disk and trains a model. Then, the data read and model training will happen in interleaved steps. This leads to significant idling time, as the model idles while the data is loading, and vice versa.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110165"></a>However, thanks to prefetching, this doesn’t need to be the case. Prefetching employs background threads and an internal buffer to load the data in advance while the model is training. When the next iteration comes, the model can seamlessly continue the training as data is already fetched into the memory. The differences between sequential execution and prefetching are shown in figure 8.7.</p>

  <p class="fm-figure"><img alt="08-07" class="calibre10" src="../../OEBPS/Images/08-07.png" width="564" height="342"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1137566"></a>Figure 8.7 Sequential execution versus pre-fetching-based execution in model training</p>

  <p class="body"><a class="calibre8" id="pgfId-1110175"></a>Next, we will look at the finished <span class="fm-code-in-text">tf.data</span> pipeline for the image segmentation<a class="calibre8" id="marker-1110172"></a><a class="calibre8" id="marker-1110173"></a><a class="calibre8" id="marker-1110174"></a> problem.</p>

  <h3 class="fm-head1" id="sigil_toc_id_104"><a id="pgfId-1110176"></a>8.2.2 The final tf.data pipeline</h3>

  <p class="body"><a class="calibre8" id="pgfId-1110180"></a>Finally<a class="calibre8" id="marker-1110177"></a><a class="calibre8" id="marker-1110178"></a><a class="calibre8" id="marker-1110179"></a>, you can define the data pipeline(s) using the functions we have defined so far. Here, we define three different data pipelines<a class="calibre8" id="marker-1110181"></a> for three different purposes: training, validation, and testing (see the following listing).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1110184"></a>Listing 8.7 Creating the train/validation/test data pipelines instances</p>
  <pre class="programlisting">orig_dir = os.path.join(
    'data', 'VOCtrainval_11-May-2012', 'VOCdevkit', 'VOC2012', 'JPEGImages'                 <span class="fm-combinumeral">❶</span>
)
seg_dir = os.path.join(
    'data', 'VOCtrainval_11-May-2012', 'VOCdevkit', 'VOC2012', 'SegmentationClass'          <span class="fm-combinumeral">❷</span>
)
subset_dir = os.path.join(
    'data', 'VOCtrainval_11-May-2012', 'VOCdevkit', 'VOC2012', 'ImageSets',                 <span class="fm-combinumeral">❸</span>
    'Segmentation'
)
 
partial_subset_fn = partial(
    get_subset_filenames, orig_dir=orig_dir, seg_dir=seg_dir, subset_dir=subset_dir         <span class="fm-combinumeral">❹</span>
)
train_subset_fn = partial(partial_subset_fn, subset='train')                                <span class="fm-combinumeral">❺</span>
val_subset_fn = partial(partial_subset_fn, subset='val')                                    <span class="fm-combinumeral">❺</span>
test_subset_fn = partial(partial_subset_fn, subset='test')                                  <span class="fm-combinumeral">❺</span>
 
input_size = (384, 384)                                                                     <span class="fm-combinumeral">❻</span>
 
tr_image_ds = get_subset_tf_dataset(                                                        <span class="fm-combinumeral">❼</span>
    train_subset_fn, batch_size, epochs, 
    input_size=input_size, resize_to_before_crop=(444,444),
    augmentation=True, shuffle=True
)
val_image_ds = get_subset_tf_dataset(                                                       <span class="fm-combinumeral">❽</span>
    val_subset_fn, batch_size, epochs, 
    input_size=input_size, 
    shuffle=False
)
test_image_ds = get_subset_tf_dataset(                                                      <span class="fm-combinumeral">❾</span>
    test_subset_fn, batch_size, 1, 
    input_size=input_size, 
    shuffle=False
)</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1132074"></a><span class="fm-combinumeral">❶</span> Directory where the input images are</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1132095"></a><span class="fm-combinumeral">❷</span> Directory where the annotated images (targets) are</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1132112"></a><span class="fm-combinumeral">❸</span> Directory where the text files containing train/validation/test filenames are</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1132129"></a><span class="fm-combinumeral">❹</span> Define a reusable partial function from get_subset_filenames.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1132146"></a><span class="fm-combinumeral">❺</span> Define three generators for train/validation/test data.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1132163"></a><span class="fm-combinumeral">❻</span> Define input image size.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1132180"></a><span class="fm-combinumeral">❼</span> Define a train data pipeline that uses data augmentation and shuffling.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1132197"></a><span class="fm-combinumeral">❽</span> Define a validation data pipeline that doesn’t use data augmentation or shuffling.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1132214"></a><span class="fm-combinumeral">❾</span> Define a test data pipeline.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110231"></a>First, we define several important paths:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1110232"></a><span class="fm-code-in-text">orig_dir</span>—Directory containing input images</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1110234"></a><span class="fm-code-in-text">seg_dir</span><a class="calibre8" id="marker-1110233"></a>—Directory containing the target images</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1110236"></a><span class="fm-code-in-text">subset_dir</span>—Directory containing text files (train.txt, val.txt) that enlist training and validation instances, respectively</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1110238"></a>Then we will define a partial function from the <span class="fm-code-in-text">get_subset_filenames()</span> function we defined earlier so that we can get a generator just by setting the subset argument of the function. Using this technique, we will define three generators: <span class="fm-code-in-text">train_subset_fn</span>, <span class="fm-code-in-text">val_subset_fn</span>, and <span class="fm-code-in-text">test_subset_fn</span>. Finally, we will define three <span class="fm-code-in-text">tf.data.Datasets</span> using the <span class="fm-code-in-text">get_subset_tf_dataset()</span> function. Our pipelines will have the following characteristics:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1110244"></a><i class="fm-italics">Training pipeline</i><a class="calibre8" id="marker-1114933"></a>—Performs data augmentation and data shuffling on every epoch</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1110247"></a><i class="fm-italics">Validation pipeline and test pipeline</i><a class="calibre8" id="marker-1114936"></a>—No augmentation or shuffling</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1110248"></a>The model we will define expects a 384 × 384-sized input and an output. In the training data pipeline, we will resize images to 444 × 444 and then randomly crop a 384 × 384-sized image. Following this, we will look at the core part of the solution: defining the image segmentation model.</p>

  <p class="fm-head2"><a id="pgfId-1110249"></a>Exercise 2</p>

  <p class="body"><a class="calibre8" id="pgfId-1110250"></a>You have been given a small set of data that contains two tensors: tensor a contains 100 64 × 64 × 3-sized images (i.e., 100 × 64 × 64 × 3 shaped), and tensor b contains 100 32 × 32 × 1-sized segmentation masks (i.e., 100 × 32 × 32 × 1 shaped). You have been asked to define a <span class="fm-code-in-text">tf.data.Dataset</span> using the functions discussed<a class="calibre8" id="marker-1110251"></a><a class="calibre8" id="marker-1110252"></a><a class="calibre8" id="marker-1110253"></a> that<a class="calibre8" id="marker-1110254"></a><a class="calibre8" id="marker-1110255"></a> will</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1110256"></a>Resize the segmentation masks to match the input image size (using nearest interpolation)</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1110257"></a>Normalize the input images using the transformation (x - 128)/255 where a single image is x</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1110258"></a>Batch the data to batches of 32 and repeat for five epochs</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1110259"></a>Prefetch the data with an auto-tuning feature</p>
    </li>
  </ul>

  <h2 class="fm-head" id="sigil_toc_id_105"><a id="pgfId-1110261"></a>8.3 DeepLabv3: Using pretrained networks to segment images</h2>

  <p class="body"><a class="calibre8" id="pgfId-1110265"></a>It’s<a class="calibre8" id="marker-1110262"></a><a class="calibre8" id="marker-1110263"></a><a class="calibre8" id="marker-1110264"></a> now time to create the brains of the pipeline: the deep learning model. Based on feedback from a colleague at a self-driving car company working on similar problems, you will implement a DeepLab v3 model. This is a model built on the back of a pretrained ResNet 50 model (trained on image classification) but with the last several layers changed to perform <i class="fm-italics">atrous convolution</i> instead of standard convolution. It uses a pyramidal aggregation module that uses atrous convolution at different scales to generate image features at different scales to produce the final output. Finally, it uses a bilinear interpolation layer to resize the final output to a desired size. You are confident that DeepLab v3 can deliver good initial results.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110266"></a>Deep neural network-based segmentation models can be broadly categorized into two types:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1110267"></a>Encoder decoder models (e.g., U-Net model)</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1110268"></a>Fully convolutional network (FCN) followed by a pyramidal aggregation module (e.g., DeepLab v3 model)</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1110269"></a>A well-known example of the encoder-decoder model is the U-Net model. In other words, U-Net has an encoder that gradually creates smaller, coarser representations of the input. This is followed by a decoder that takes the representations the encoder built and gradually up-samples (i.e., increases the size of) the output until it reaches the size of the input image. The up-sampling is achieved through an operation known as <i class="fm-italics">transpose convolution</i><a class="calibre8" id="marker-1110270"></a>. Finally, you train the whole structure end to end, where an input is the input image and the target is the segmentation mask for the corresponding image. We will not discuss this type of model in this chapter. However, I have included a detailed walkthrough in appendix B (along with an implementation of the model).</p>

  <p class="body"><a class="calibre8" id="pgfId-1110271"></a>The other type of segmentation models introduces a special model that replaces the decoder. We call this module a <i class="fm-italics">pyramidal aggregation module</i><a class="calibre8" id="marker-1110272"></a>. Its purpose is to garner spatial information at different scales (e.g., different-sized outputs from various interim convolution layers) that provides fine-grained contextual information about the objects present in the image. DeepLab v3 is a prime example of this approach. We will put the DeepLab v3 model under the microscope and use it to excel at the segmentation task.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110273"></a>Researchers and engineers gravitate toward methods that use pyramidal aggregation modules more. There could be many reasons for this. One lucrative reason is that there are less parameters in networks that use pyramidal aggregation than an encoder-decoder based counterpart. Another reason may be that, typically, introducing a novel module offers more flexibility (compared to an encoder-decoder) to engineer efficient and accurate feature extraction methods at multiple scales.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110274"></a>How important is the pyramidal aggregation module? To know that, we have to first understand what the fully convolutional part of the network looks like. Figure 8.8 illustrates the generic structure of such a segmentation model.</p>

  <p class="fm-figure"><img alt="08-08" class="calibre10" src="../../OEBPS/Images/08-08.png" width="1075" height="406"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1137600"></a>Figure 8.8 General structure and organization of a fully convolutional network that uses a pyramidal aggregation module</p>

  <p class="body"><a class="calibre8" id="pgfId-1110281"></a>The best way to understand the importance of the pyramidal aggregation module is to see what happens if we don’t have it. If that is the case, then the last convolutional layer will have the enormous and unrealistic responsibility of building the final segmentation mask (which is typically 16-32x times larger than the layer output). It is no surprise that there is a massive representational bottleneck between the final convolution layer and the final segmentation mask, leading to poor performance. The pyramidal structure typically enforced in CNNs results in a very small output width and height in the final layer.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110282"></a>The pyramidal aggregation module bridges this gap. It does so by combining several different interim outputs. This way, the network has ample fine-grained (from earlier layers) and coarser (from deeper layers) details to construct the desired segmentation mask. Fine-grained representations provide spatial/contextual information about the image, whereas the coarser representations provide high-level information about the image (e.g., what objects are present). By fusing both types of these representations, the task of generating the final output becomes more achievable.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1110283"></a>Why not a skyscraper instead of a pyramid?</p>

    <p class="fm-sidebar-text"><a id="pgfId-1110284"></a>You might be tempted to ponder, if making the outputs smaller as you go causes loss of information, “Why not keep it the same size?” (hence the term <i class="fm-italics">skyscraper</i>). This is an impractical solution for two main reasons.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1110285"></a>First, decreasing the size of the outputs through pooling or striding is an important regularization method that forces the network to learn translation-invariant features (as we discussed in chapter 6). By taking this away, we can hinder the generalizability of the network.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1110286"></a>Second, not decreasing the output size will increase the memory footprint of the model significantly. This will, in turn, restrict the depth of the network dramatically, making it more difficult to create deeper networks.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1110288"></a>DeepLab v3 is the golden child of a lineage of models that emerged from and was introduced in the paper “Rethinking Atrous Convolution for Semantic Image Segmentation” (<span class="fm-hyperlink"><a class="url" href="https://arxiv.org/pdf/1706.05587.pdf">https://arxiv.org/pdf/1706.05587.pdf</a></span>) by several researchers from Google.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110289"></a>Most segmentation models face an adverse side effect caused by a common and beneficial design principle. Vision models incorporate stride/pooling to make network translation invariant. But an ill-favored outcome of that is the compounding reduction of the size of the outputs produced. This typically leads to a final output that is 16-32 times smaller than the input. Being a dense prediction task, image segmentation tasks suffer heavily from this design idea. Therefore, most of the groundbreaking networks that have surfaced have been about solving this. The DeepLab model came into the world for exactly that purpose. Let’s now see how DeepLab v3 solves this problem.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110290"></a>DeepLab v3 uses a ResNet-50 (<span class="fm-hyperlink"><a class="url" href="https://arxiv.org/pdf/1512.03385.pdf">https://arxiv.org/pdf/1512.03385.pdf</a></span>) pretrained on an ImageNet image classification data set as its backbone for extracting features of an image. It is one of the pioneering residual networks that made waves in the computer vision community a few years ago. DeepLab v3 introduces several architectural changes to the model to alleviate this issue. Furthermore, DeepLab v3 introduces a shiny new component called <i class="fm-italics">atrous spatial pyramid pooling</i> (ASPP). We will discuss each of these in more detail in the coming sections.</p>

  <h3 class="fm-head1" id="sigil_toc_id_106"><a id="pgfId-1110291"></a>8.3.1 A quick overview of the ResNet-50 model</h3>

  <p class="body"><a class="calibre8" id="pgfId-1110296"></a>The<a class="calibre8" id="marker-1110292"></a><a class="calibre8" id="marker-1110293"></a><a class="calibre8" id="marker-1110294"></a><a class="calibre8" id="marker-1110295"></a> ResNet-50 model consists of several convolution blocks, followed by a global average pooling layer and a fully connected final prediction layer with softmax activation. The convolution block is the innovative part of the model. The original model has 16 convolution blocks organized into five groups. A single block consists of three convolution layers (1 × 1 convolution layer with stride 2, 3 × 3 convolution layer, and 1 × 1 convolution layer), batch normalization, and residual connections. We discussed residual connections in depth in<a class="calibre8" id="marker-1110297"></a><a class="calibre8" id="marker-1110298"></a><a class="calibre8" id="marker-1110299"></a><a class="calibre8" id="marker-1110300"></a> chapter 7. Next, we will discuss a core computation used throughout the model known as atrous convolution.</p>

  <h3 class="fm-head1" id="sigil_toc_id_107"><a id="pgfId-1110301"></a><a id="marker-1119473"></a><a id="marker-1119474"></a><a id="marker-1119475"></a><a id="marker-1119476"></a>8.3.2 Atrous convolution: Increasing the receptive field of convolution layers with holes</h3>

  <p class="body"><a class="calibre8" id="pgfId-1110302"></a>Compared to the standard ResNet-50, a major change that DeepLab v3 boasts is the use of atrous convolutions. Atrous (meaning “holes” in French) convolution, also known as dilated convolution, is a variant of the standard convolution. Atrous convolution works by inserting “holes” in between the convolution parameters. The increase in the receptive field is controlled by a parameter called <i class="fm-italics">dilation rate</i><a class="calibre8" id="marker-1110303"></a>. A higher dilation rate means more holes between actual parameters in the convolution. A major benefit of atrous convolution is the ability to increase the size of the receptive field without compromising the parameter efficiency of a convolution layer.</p>

  <p class="fm-figure"><img alt="08-09" class="calibre10" src="../../OEBPS/Images/08-09.png" width="1003" height="450"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1137638"></a>Figure 8.9 Atrous convolution compared<a id="marker-1137634"></a><a id="marker-1137635"></a><a id="marker-1137636"></a><a id="marker-1137637"></a> to standard convolution. Standard convolution is a special case of atrous convolution, where the rate is 1. As you increase the dilation rate, the receptive field of the layer increases.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110314"></a>Figure 8.9 shows how a large dilation rate leads to a larger receptive field. The number of shaded gray boxes represents the number of parameters, whereas the dashed, lightly shaded box represents the size of the receptive field. As you can see, the number of parameters stays constant, while the receptive field increases. Computationally, it is quite straightforward to extend standard convolution to atrous convolution. All you need to do is insert zeros for the holes in the atrous convolution operation.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1110315"></a>Wait! How does atrous convolution help segmentation models?</p>

    <p class="fm-sidebar-text"><a id="pgfId-1110316"></a>As we discussed, the main issue presented by the pyramidal structure of CNNs is that the output gets gradually smaller. The easiest solution, leaving the learned parameters untouched, is to reduce the stride of the layers. Though technically that will increase output size, conceptually there is a problem.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1110317"></a>To understand it, assume the i<sup class="fm-superscript1">th</sup> layer of a CNN has a stride of 2 and gets a h × w-sized input. Then the i+1<sup class="fm-superscript1">th</sup> layer gets a h/2 × w/2-sized input. By removing the stride of the i<sup class="fm-superscript1">th</sup> layer, it gets a h × w-sized output. However, the kernel of the i+1<sup class="fm-superscript1">th</sup> layer has been trained to see a smaller output, so by increasing the size of the input, we are disrupting (or reducing) the receptive field of the layer. By introducing atrous convolution, we compensate for that reduction of the receptive field.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1110319"></a>Let’s now see how the ResNet-50 is repurposed for image segmentation. First, we download it from the <span class="fm-code-in-text">tf.keras.applications</span> module<a class="calibre8" id="marker-1110320"></a>. The architecture of the ResNet-50 model has the following format. To start, it has a stride 2 convolution layer and a stride 2 pooling layer. After that, it has sequence of convolution blocks and finally an average pooling layer and fully connected output layer. These convolution blocks have a hierarchical organization of convolution layers. Each convolution block consists of several subblocks, which consist of three convolution layers (i.e., a 1 × 1 convolution, a 3 × 3 convolution, and a 1 × 1 convolution) along with batch<a class="calibre8" id="marker-1110321"></a><a class="calibre8" id="marker-1110322"></a><a class="calibre8" id="marker-1110323"></a><a class="calibre8" id="marker-1110324"></a> normalization.</p>

  <h3 class="fm-head1" id="sigil_toc_id_108"><a id="pgfId-1110325"></a><a id="marker-1110451"></a><a id="marker-1110452"></a><a id="marker-1110453"></a><a id="marker-1110454"></a>8.3.3 Implementing DeepLab v3 using the Keras functional API</h3>

  <p class="body"><a class="calibre8" id="pgfId-1110330"></a>The<a class="calibre8" id="marker-1110326"></a><a class="calibre8" id="marker-1110327"></a><a class="calibre8" id="marker-1110328"></a><a class="calibre8" id="marker-1110329"></a> network starting from the input up to the <span class="fm-code-in-text">conv4</span> block remains unchanged. Following the notation from the original ResNet paper, these blocks are identified as <span class="fm-code-in-text">conv2</span>, <span class="fm-code-in-text">conv3</span>, and <span class="fm-code-in-text">conv4</span> block groups. Our first task is to create a model containing the input layer up to the <span class="fm-code-in-text">conv4</span> block of the original ResNet-50 model. After that, we will focus on recreating the final convolution block (i.e., <span class="fm-code-in-text">conv5</span>) as per the DeepLab v3 paper:</p>
  <pre class="programlisting"># Pretrained model and the input
inp = layers.Input(shape=target_size+(3,))
resnet50 = tf.keras.applications.ResNet50(
    include_top=False, input_tensor=inp,pooling=None
)
    
for layer <i class="fm-italics">in</i> resnet50.layers:
    if layer.name == "conv5_block1_1_conv":
        break
    out = layer.output
            
resnet50_upto_conv4 = models.Model(resnet50.input, out)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1110343"></a>As shown here, we find the last layer in the ResNet-50 model just before the <span class="fm-code-in-text">"conv5_ block1_1_conv"</span>, which would be the last layer of the <span class="fm-code-in-text">conv4</span> block group. With that, we can define a makeshift model that contains layers from the input to the final output of the <span class="fm-code-in-text">conv4</span> block group. Later, we will focus on augmenting this model by introducing modifications and novel components from the paper. We will redefine the <span class="fm-code-in-text">conv5</span> block with dilated convolutions. To do this, we need to understand the composition of a ResNet block (figure 8.10). We can assume it has three different levels.</p>

  <p class="fm-figure"><img alt="08-10" class="calibre10" src="../../OEBPS/Images/08-10.png" width="406" height="506"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1137672"></a>Figure 8.10 Anatomy of a convolution block in ResNet-50. For this example, we show the very first convolution block of ResNet-50. The organization of a convolution block group consists of three different levels.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110350"></a>Let’s now implement a function to represent each level while using dilated convolution. In order to convert a standard convolution layer to a dilated convolution, we just have to pass in the desired rate to the <span class="fm-code-in-text">dilation_rate</span> parameter in the <span class="fm-code-in-text">tf.keras.layers.Conv2D</span> layer<a class="calibre8" id="marker-1119590"></a>. First, we will implement a function that represents a level 3 block, as shown in the following listing.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1110354"></a>Listing 8.8 A level 3 convolution block in ResNet-50</p>
  <pre class="programlisting">def block_level3(
    inp, filters, kernel_size, rate, block_id, convlayer_id, activation=True           <span class="fm-combinumeral">❶</span>
):
    """ A single convolution layer with atrous convolution and batch normalization 
    inp: 4-D tensor having shape [batch_size, height, width, channels]
    filters: number of output filters
    kernel_size: The size of the convolution kernel
    rate: dilation rate for atrous convolution
    block_id, convlayer_id - IDs to distinguish different convolution blocks and layers
    activation: If true ReLU is applied, if False no activation is applied
    """
 
    conv5_block_conv_out = layers.Conv2D(
        filters, kernel_size, dilation_rate=rate, padding='same',                      <span class="fm-combinumeral">❷</span>
        name='conv5_block{}_{}_conv'.format(block_id, convlayer_id)
    )(inp)
 
    conv5_block_bn_out = layers.BatchNormalization(
        name='conv5_block{}_{}_bn'.format(block_id, convlayer_id)                      <span class="fm-combinumeral">❸</span>
    )(conv5_block_conv_out)
 
    if activation:
        conv5_block_relu_out = layers.Activation(
            'relu', name='conv5_block{}_{}_relu'.format(block_id, convlayer_id)        <span class="fm-combinumeral">❹</span>
        )(conv5_block_bn_out)
    
        return conv5_block_relu_out
    else:
        return conv5_block_bn_out                                                      <span class="fm-combinumeral">❺</span></pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1131701"></a><span class="fm-combinumeral">❶</span> Here, inp takes a 4D input having shape [batch size, height, width, channels].</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1131718"></a><span class="fm-combinumeral">❷</span> Perform 2D convolution on the input with a given number of filters, kernel_size, and dilation rate.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1131735"></a><span class="fm-combinumeral">❸</span> Perform batch normalization on the output of the convolution layer.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1131752"></a><span class="fm-combinumeral">❹</span> Apply ReLU activation if activation is set to True.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1131769"></a><span class="fm-combinumeral">❺</span> Return the output without an activation if activation is set to False.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110389"></a>A level 3 block has a single convolution layer with a desired dilation rate and a batch normalization layer followed by a nonlinear ReLU activation layer. Next, we will write a function for the level 2 block (see the next listing).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1110391"></a>Listing 8.9 A level 2 convolution block in ResNet-50</p>
  <pre class="programlisting">def block_level2(inp, rate, block_id):
    """ A level 2 resnet block that consists of three level 3 blocks """
 
    block_1_out = block_level3(inp, 512, (1,1), rate, block_id, 1)
    block_2_out = block_level3(block_1_out, 512, (3,3), rate, block_id, 2)
    block_3_out = block_level3(
        block_2_out, 2048, (1,1), rate, block_id, 3, activation=False
    )
            
    return block_3_out</pre>

  <p class="body"><a class="calibre8" id="pgfId-1110402"></a>A level 2 block consists of three level 3 blocks with a given dilation rate that have convolution layers with the following specifications:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1110403"></a>1 × 1 convolution layer having 512 filters and a desired dilation rate</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1110404"></a>3 × 3 convolution layer having 512 filters and a desired dilation rate</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1110405"></a>1 × 1 convolution layer having 2048 filters and a desired dilation rate</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1110406"></a>Apart from using atrous convolution, this is identical to a level 2 block of the original <span class="fm-code-in-text">conv5</span> block in the ResNet-50 model. With all the building blocks ready, we can implement the fully fledged <span class="fm-code-in-text">conv5</span> block with atrous convolution (see the next listing).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1110408"></a>Listing 8.10 Implementing the final ResNet-50 convolution block group (level 1)</p>
  <pre class="programlisting">def resnet_block(inp, rate):
    """ Redefining a resnet block with atrous convolution """
        
    block0_out = block_level3(
        inp, 2048, (1,1), 1, block_id=1, convlayer_id=0, activation=False <span class="fm-combinumeral">❶</span>
    )
    block1_out = block_level2(inp, 2, block_id=1)                         <span class="fm-combinumeral">❷</span>
    block1_add = layers.Add(
        name='conv5_block{}_add'.format(1))([block0_out, block1_out]      <span class="fm-combinumeral">❸</span>
    )
    block1_relu = layers.Activation(
        'relu', name='conv5_block{}_relu'.format(1)                       <span class="fm-combinumeral">❹</span>
    )(block1_add)
    block2_out = block_level2 (block1_relu, 2, block_id=2) # no relu      <span class="fm-combinumeral">❺</span>
    block2_add = layers.Add(
        name='conv5_block{}_add'.format(2)                                <span class="fm-combinumeral">❻</span>
    )([block1_add, block2_out])
    block2_relu = layers.Activation(
        'relu', name='conv5_block{}_relu'.format(2)                       <span class="fm-combinumeral">❼</span>
    )(block2_add)
        
    block3_out = block_level2 (block2_relu, 2, block_id=3)                <span class="fm-combinumeral">❽</span>
    block3_add = layers.Add(
        name='conv5_block{}_add'.format(3)                                <span class="fm-combinumeral">❽</span>
    )([block2_add, block3_out])
    block3_relu = layers.Activation(
        'relu', name='conv5_block{}_relu'.format(3)                       <span class="fm-combinumeral">❽</span>
    )(block3_add)
        
     return block3_relu</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1131083"></a><span class="fm-combinumeral">❶</span> Create a level 3 block (block0) to create residual connections for the first block.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1131104"></a><span class="fm-combinumeral">❷</span> Define the first level 2 block, which has a dilation rate of 2 (block1).</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1131121"></a><span class="fm-combinumeral">❸</span> Create a residual connection from block0 to block1.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1131138"></a><span class="fm-combinumeral">❹</span> Apply ReLU activation to the result.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1131155"></a><span class="fm-combinumeral">❺</span> The second level 2 block with a dilation rate of 2 (block2)</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1131172"></a><span class="fm-combinumeral">❻</span> Create a residual connection from block1 to block2.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1131189"></a><span class="fm-combinumeral">❼</span> Apply ReLU activation.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1131206"></a><span class="fm-combinumeral">❽</span> Apply a similar procedure to block1 and block2 to create block3.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110449"></a>There’s no black magic here. The function <span class="fm-code-in-text">resnet_block</span> lays the outputs of the functions we already discussed to assemble the final convolution block. Particularly, it has three level 2 blocks with residual connections going from the previous block to the next. Finally, we can get the final output of the <span class="fm-code-in-text">conv5</span> block with a dilation rate of 2 by calling the <span class="fm-code-in-text">resnet_block</span> function with the output of the interim model (<span class="fm-code-in-text">resnet50_ upto_conv4</span>) we defined as the input and a dilation rate<a class="calibre8" id="marker-1131346"></a><a class="calibre8" id="marker-1131347"></a><a class="calibre8" id="marker-1131348"></a><a class="calibre8" id="marker-1131349"></a> of 2:</p>
  <pre class="programlisting">resnet_block4_out = resnet_block(resnet50_upto_conv4.output, 2)</pre>

  <h3 class="fm-head1" id="sigil_toc_id_109"><a id="pgfId-1110456"></a>8.3.4 Implementing the atrous spatial pyramid pooling module</h3>

  <p class="body"><a class="calibre8" id="pgfId-1110461"></a>Here<a class="calibre8" id="marker-1110457"></a><a class="calibre8" id="marker-1110458"></a><a class="calibre8" id="marker-1110459"></a><a class="calibre8" id="marker-1110460"></a>, we will discuss the most exciting innovation of the DeepLab v3 model. The atrous spatial pyramid pooling (ASPP) module serves two purposes:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1110462"></a>Aggregates multiscale information about an image, obtained through outputs produced using different dilation rates</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1110463"></a>Combines highly summarized information obtained through global average pooling</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1110464"></a>The ASPP module gathers multiscale information by performing different convolutions on the last ResNet-50 output. Specifically, the ASPP module performs 1 × 1 convolution, 3 × 3 convolution (r = 6), 3 × 3 convolution (r = 12), and 3 × 3 convolution (r = 18), where r is the dilation rate. All of these convolutions have 256 output channels and are implemented as level 3 blocks (provided by the function <span class="fm-code-in-text">block_level3()</span>).</p>

  <p class="body"><a class="calibre8" id="pgfId-1110466"></a>ASRP captures high-level information by performing global average pooling, followed by a 1 × 1 convolution with 256 output channels to match the output size of multiscale outputs, and finally a bilinear up-sampling layer to up-sample the height and width dimensions shrunk by the global average pooling. Remember that bilinear interpolation up-samples the images by computing the resulting pixels as an average of neighboring pixels. Figure 8.11 illustrates the ASPP module.</p>

  <p class="fm-figure"><img alt="08-11" class="calibre10" src="../../OEBPS/Images/08-11.png" width="925" height="692"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1137706"></a>Figure 8.11 The ASPP module used in the DeepLab v3 model</p>

  <p class="body"><a class="calibre8" id="pgfId-1110473"></a>The job of the ASPP module can be summarized as a concise function. We have all the tools we need to implement this function from the previous work we have done (see the following listing).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1110475"></a>Listing 8.11 Implementing ASPP</p>
  <pre class="programlisting">def atrous_spatial_pyramid_pooling(inp):
    """ Defining the ASPP (Atrous spatial pyramid pooling) module """
        
    # Part A: 1x1 and atrous convolutions
    outa_1_conv = block_level3(
        inp, 256, (1,1), 1, '_aspp_a', 1, activation='relu'
    )                                                                                 <span class="fm-combinumeral">❶</span>
    outa_2_conv = block_level3(
        inp, 256, (3,3), 6, '_aspp_a', 2, activation='relu'
    )                                                                                 <span class="fm-combinumeral">❷</span>
    outa_3_conv = block_level3(
        inp, 256, (3,3), 12, '_aspp_a', 3, activation='relu'
    )                                                                                 <span class="fm-combinumeral">❸</span>
    outa_4_conv = block_level3(
        inp, 256, (3,3), 18, '_aspp_a', 4, activation='relu'
    )                                                                                 <span class="fm-combinumeral">❹</span>
        
    # Part B: global pooling
    outb_1_avg = layers.Lambda(
        lambda x: K.mean(x, axis=[1,2], keepdims=True)
    )(inp)                                                                            <span class="fm-combinumeral">❺</span>
    outb_1_conv = block_level3(
        outb_1_avg, 256, (1,1), 1, '_aspp_b', 1, activation='relu'                    <span class="fm-combinumeral">❻</span>
    )
    outb_1_up = layers.UpSampling2D((24,24), interpolation='bilinear')(outb_1_avg)    <span class="fm-combinumeral">❼</span>
    out_aspp = layers.Concatenate()(
        [outa_1_conv, outa_2_conv, outa_3_conv, outa_4_conv, outb_1_up]               <span class="fm-combinumeral">❽</span>
    )   
    
    return out_aspp
 
out_aspp = atrous_spatial_pyramid_pooling(resnet_block4_out)                          <span class="fm-combinumeral">❾</span></pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1130258"></a><span class="fm-combinumeral">❶</span> Define a 1 × 1 convolution.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1130279"></a><span class="fm-combinumeral">❷</span> Define a 3 x 3 convolution with 256 filters and a dilation rate of 6.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1130296"></a><span class="fm-combinumeral">❸</span> Define a 3 x 3 convolution with 256 filters and a dilation rate of 12.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1130313"></a><span class="fm-combinumeral">❹</span> Define a 3 x 3 convolution with 256 filters and a dilation rate of 18.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1130330"></a><span class="fm-combinumeral">❺</span> Define a global average pooling layer.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1130347"></a><span class="fm-combinumeral">❻</span> Define a 1 × 1 convolution with 256 filters.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1130364"></a><span class="fm-combinumeral">❼</span> Up-sample the output using bilinear interpolation.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1130381"></a><span class="fm-combinumeral">❽</span> Concatenate all the outputs.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1130398"></a><span class="fm-combinumeral">❾</span> Create an instance of ASPP.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110517"></a>The ASPP module consists of four level 3 blocks, as outlined in the code. The first block comprises a 1 × 1 convolution with 256 filters without dilation (this produces <span class="fm-code-in-text">outa_1_conv</span>). The latter three blocks consist of 3 × 3 convolutions with 256 filters but with varying dilation rates (i.e., 6, 12, 18; they produce <span class="fm-code-in-text">outa_2_conv</span>, <span class="fm-code-in-text">outa_3_conv</span>, and <span class="fm-code-in-text">outa_4_conv</span>, respectively). This covers aggregating features from the image at multiple scales. However, we also need to preserve the global information about the image, similar to a global average pooling layer (<span class="fm-code-in-text">outb_1_avg</span>). This is achieved through a lambda layer that averages the input over the height and width dimensions:</p>
  <pre class="programlisting">outb_1_avg = layers.Lambda(lambda x: K.mean(x, axis=[1,2], keepdims=True))(inp)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1110519"></a>The output of the averaging is then followed by a 1 × 1 convolution filter with 256 filters. Then, to bring the output to the same size as previous outputs, an up-sampling layer that uses bilinear interpolation is used (this produces <span class="fm-code-in-text">outb_1_up</span>):</p>
  <pre class="programlisting">outb_1_up = layers.UpSampling2D((24,24), interpolation='bilinear')(outb_1_avg)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1110522"></a>Finally, all these outputs are concatenated to a single output using a <span class="fm-code-in-text">Concatenate</span> layer<a class="calibre8" id="marker-1110521"></a> to produce the final output<a class="calibre8" id="marker-1110523"></a><a class="calibre8" id="marker-1110524"></a><a class="calibre8" id="marker-1110525"></a><a class="calibre8" id="marker-1110526"></a> <span class="fm-code-in-text">out_aspp</span>.</p>

  <h3 class="fm-head1" id="sigil_toc_id_110"><a id="pgfId-1110527"></a>8.3.5 Putting everything together</h3>

  <p class="body"><a class="calibre8" id="pgfId-1110528"></a>Now it’s time to collate all the different components to create one majestic segmentation model. <a class="calibre8" id="aRef62724202"></a>The next listing outlines the steps required to build the final model.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1110531"></a>Listing 8.12 The final DeepLab v3 model</p>
  <pre class="programlisting">inp = layers.Input(shape=target_size+(3,))                               <span class="fm-combinumeral">❶</span>
 
resnet50= tf.keras.applications.ResNet50(
    include_top=False, input_tensor=inp,pooling=None                     <span class="fm-combinumeral">❷</span>
)
    
for layer <i class="fm-italics">in</i> resnet50.layers:
    if layer.name == "conv5_block1_1_conv":
        break
    out = layer.output                                                   <span class="fm-combinumeral">❸</span>
            
resnet50_upto_conv4 = models.Model(resnet50.input, out)                  <span class="fm-combinumeral">❹</span>
 
resnet_block4_out = resnet_block(resnet50_upto_conv4.output, 2)          <span class="fm-combinumeral">❺</span>
 
out_aspp = atrous_spatial_pyramid_pooling(resnet_block4_out)             <span class="fm-combinumeral">❻</span>
    
out = layers.Conv2D(21, (1,1), padding='same')(out_aspp)                 <span class="fm-combinumeral">❼</span>
final_out = layers.UpSampling2D((16,16), interpolation='bilinear')(out)  <span class="fm-combinumeral">❼</span>
 
deeplabv3 = models.Model(resnet50_upto_conv4.input, final_out)           <span class="fm-combinumeral">❽</span></pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1129716"></a><span class="fm-combinumeral">❶</span> Define the RGB input layer.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1129737"></a><span class="fm-combinumeral">❷</span> Download and define the resnet50.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1129754"></a><span class="fm-combinumeral">❸</span> Get the output of the last layer we’re interested in.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1129774"></a><span class="fm-combinumeral">❹</span> Define an interim model from the input up to the last layer of the conv4 block.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1129791"></a><span class="fm-combinumeral">❺</span> Define the removed conv5 resnet block.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1129808"></a><span class="fm-combinumeral">❻</span> Define the ASPP module.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1129825"></a><span class="fm-combinumeral">❼</span> Define the final output.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1129842"></a><span class="fm-combinumeral">❽</span> Define the final model.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110561"></a>Note how the model has a linear layer that does not have any activation present (e.g., sigmoid or softmax). This is because we are planning to use a special loss function that uses logits (unnormalized scores obtained from the last layer before applying softmax) instead of normalized probability scores. Due to that, we will keep the last layer a linear output with no activation.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110562"></a>We have one final housekeeping step to perform: copying the weights from the original <span class="fm-code-in-text">conv5</span> block to the newly created <span class="fm-code-in-text">conv5</span> block in our model. To do that, first we need to store the weights from the original model as follows:</p>
  <pre class="programlisting">w_dict = {}
for l <i class="fm-italics">in</i> ["conv5_block1_0_conv", "conv5_block1_0_bn", 
          "conv5_block1_1_conv", "conv5_block1_1_bn", 
          "conv5_block1_2_conv", "conv5_block1_2_bn", 
          "conv5_block1_3_conv", "conv5_block1_3_bn"]:
    w_dict[l] = resnet50.get_layer(l).get_weights()</pre>

  <p class="body"><a class="calibre8" id="pgfId-1110569"></a>We cannot copy the weights to the new model until we compile the model, as weights are not initialized until the model is compiled. Before we do that, we need to learn loss functions and evaluation metrics that are used in segmentation tasks. To do that, we will need to implement custom loss functions and metrics and use them to compile the model. This will be discussed in the next section.</p>

  <p class="fm-head2"><a id="pgfId-1110570"></a>Exercise 3</p>

  <p class="body"><a class="calibre8" id="pgfId-1110571"></a>You want to create a new pyramidal aggregation module called aug-ASPP. The idea is similar to the ASPP module we implemented earlier, but with a few differences. Let’s say you have been given two interim outputs from the model: <span class="fm-code-in-text">out_1</span> and <span class="fm-code-in-text">out_2</span> (same size). You have to write a function, <span class="fm-code-in-text">aug_aspp</span>, that will take these two outputs and do the following:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1110573"></a>Perform atrous convolution with r = 16, 128 filters, 3 × 3 convolution, stride 1, and ReLU activation on <span class="fm-code-in-text">out_1</span> (output will be called <span class="fm-code-in-text">atrous_out_1</span>)</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1110574"></a>Perform atrous convolution with r = 8, 128 filters, 3 × 3 convolution, stride 1, and ReLU activation on both <span class="fm-code-in-text">out_1</span> and <span class="fm-code-in-text">out_2</span> (output will be called <span class="fm-code-in-text">atrous_ out_2_1</span> and <span class="fm-code-in-text">atrous_out_2_2</span>)</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1110575"></a>Concatenate <span class="fm-code-in-text">atrous_out_2_1</span> and <span class="fm-code-in-text">atrous_out_2_2</span> (output will be called <span class="fm-code-in-text">atrous_out_2</span>)</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1110576"></a>Apply 1 × 1 convolution with 64 filters to both <span class="fm-code-in-text">atrous_out_1</span> and <span class="fm-code-in-text">atrous_out_2</span> and concatenate (output will be called <span class="fm-code-in-text">conv_out</span>)</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1110577"></a>Use bilinear up-sampling to double the size of <span class="fm-code-in-text">conv_out</span> (on height and width dimensions) and apply sigmoid<a class="calibre8" id="marker-1110578"></a><a class="calibre8" id="marker-1110579"></a><a class="calibre8" id="marker-1110580"></a> activation</p>
    </li>
  </ul>

  <h2 class="fm-head" id="sigil_toc_id_111"><a id="pgfId-1110582"></a>8.4 Compiling the model: Loss functions and evaluation metrics in image segmentation</h2>

  <p class="body"><a class="calibre8" id="pgfId-1110583"></a>In order to finalize the DeepLab v3 model (built using mostly the ResNet-50 structure and the ASPP module), we have to define a suitable loss function and metrics to measure the performance of the model. Image segmentation is quite different from image classification tasks, so the loss function and metrics don’t necessarily translate to the segmentation problem. One key difference is that there is typically a large class imbalance in segmentation data, as a “background” class typically dominates an image compared to other object-related pixels. To get started, you read a few blog posts and research papers and identify weighted categorical cross-entropy loss and dice loss as good candidates. You focus on three different metrics: pixel accuracy, mean (class-weighted) accuracy, and mean IoU.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110584"></a>Loss functions and evaluation metrics used in image segmentation models are different from what is used in image classifiers. To start, image classifiers take in a single class label for a single image, whereas a segmentation model predicts a class for every single pixel in the image. This highlights the necessity of not only reimagining existing loss functions and metrics, but also inventing new losses and evaluation metrics that are more appropriate for the output produced by segmentation models. We will first discuss loss functions and then metrics.</p>

  <h3 class="fm-head1" id="sigil_toc_id_112"><a id="pgfId-1110585"></a>8.4.1 Loss functions</h3>

  <p class="body"><a class="calibre8" id="pgfId-1110586"></a>A <i class="fm-italics">loss function</i> is what is used to optimize the model whose purpose is to find the parameters that minimize a defined loss. A<a class="calibre8" id="marker-1110587"></a><a class="calibre8" id="marker-1110588"></a> loss function used in a deep network must be <i class="fm-italics">differentiable</i>, as the minimization of the loss happens with the help of gradients. The loss functions we’ll use comprise two loss functions:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1110589"></a>Cross-entropy loss</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1110590"></a>Dice loss</p>
    </li>
  </ul>

  <p class="fm-head2"><a id="pgfId-1110591"></a><a id="marker-1121683"></a>Cross-entropy loss</p>

  <p class="body"><a class="calibre8" id="pgfId-1110592"></a>Cross-entropy loss is one of the most common losses used in segmentation tasks and can be implemented with just one line in Keras. We already used cross-entropy loss quite a few times but didn’t analyze it in detail. However, it is worthwhile to review the underpinning mechanics that govern cross-entropy loss.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110593"></a><a id="marker-1121682"></a>Cross-entropy loss takes in a predicted target and a true target. Both these tensors are of shape [batch size, height, width, object classes]. The object class dimension is a one-hot encoded representation of which object class a given pixel belongs to. The cross-entropy loss is then computed for every pixel independently using</p>

  <p class="fm-equation"><img alt="08_11a" class="calibre10" src="../../OEBPS/Images/08_11a.png" width="489" height="82"/><br class="calibre2"/>
  <a id="pgfId-1112431"></a></p>

  <p class="body"><a class="calibre8" id="pgfId-1110600"></a>where <i class="fm-timesitalic">CE</i>(<i class="fm-timesitalic">i, j</i>) represents the cross-entropy<a class="calibre8" id="marker-1110598"></a><a class="calibre8" id="marker-1110599"></a> loss for pixel at position (<i class="fm-timesitalic">i, j</i>) on the image, <i class="fm-timesitalic">c</i> is the number of classes, and <i class="fm-timesitalic">y</i><sub class="fm-subscript">k</sub> and <i class="fm-timesitalic">ŷ</i><a class="calibre8" id="aHlk54170421"></a><sub class="fm-subscript">k</sub> represent the elements in the one-hot encoded vector and the predicted probability distribution over classes of that pixel. This is then summed across all the pixels to get the final loss.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110602"></a>Beneath the simplicity of the method, a critical issue lurks. Class imbalance is almost certain to rear its ugly head in image segmentation problems. You will find hardly any real-world images where each object occupies an equal area in the image. The good news is it is not very difficult to deal with this issue. This can be mitigated by assigning a weight for each pixel in the image, depending on the dominance of the class it represents. Pixels belonging to large objects will have smaller weights, whereas pixels belonging to smaller objects will have larger weights, providing an equal say despite the size in the final loss. The next listing shows how to do this in TensorFlow.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1110604"></a>Listing 8.13 Computing the label weights for a given batch of data</p>
  <pre class="programlisting">def get_label_weights(y_true, y_pred):
    
    weights = tf.reduce_sum(tf.one_hot(y_true, num_classes), axis=[1,2])  <span class="fm-combinumeral">❶</span>
    
    tot = tf.reduce_sum(weights, axis=-1, keepdims=True)                  <span class="fm-combinumeral">❷</span>
    
    weights = (tot - weights) / tot  # [b, classes]                       <span class="fm-combinumeral">❸</span>
    
    y_true = tf.reshape(y_true, [-1, y_pred.shape[1]*y_pred.shape[2]])    <span class="fm-combinumeral">❹</span>
        
    y_weights = tf.gather(params=weights, indices=y_true, batch_dims=1)   <span class="fm-combinumeral">❺</span>
    y_weights = tf.reshape(y_weights, [-1])                               <span class="fm-combinumeral">❻</span>
    
    return y_weights</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1129361"></a><span class="fm-combinumeral">❶</span> Get the total pixels per-class in y_true.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1129382"></a><span class="fm-combinumeral">❷</span> Get the total pixels in y_true.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1129402"></a><span class="fm-combinumeral">❸</span> Compute the weights per-class. Rarer classes get more weight.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1129419"></a><span class="fm-combinumeral">❹</span> Reshape y_true to a [batch size, height*width]-sized tensor.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1129436"></a><span class="fm-combinumeral">❺</span> Create a weight vector by gathering the weights corresponding to indices in y_true.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1129453"></a><span class="fm-combinumeral">❻</span> Make y_weights a vector.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110625"></a>Here, for a given batch, we compute the weights as a sequence/vector that has a number of elements equal to <span class="fm-code-in-text">y_true</span>. First, we get the total number of pixels for each class by computing the sum over the width and height of the one-hot encoded <span class="fm-code-in-text">y_true</span> (i.e., has dimensions batch, height, width, and class). Here, a class that has a value larger than <span class="fm-code-in-text">num_classes</span> will be ignored. Next, we compute the total number of pixels per sample by taking the sum over the class dimension resulting in <i class="fm-italics">tot</i> (a <span class="fm-code-in-text">[batch size, 1]</span>-sized tensor). Now the weights can be computed per sample and per class using</p>

  <p class="fm-equation"><img alt="08_11b" class="calibre10" src="../../OEBPS/Images/08_11b.png" width="134" height="68"/><br class="calibre2"/>
  <a id="pgfId-1112465"></a></p>

  <p class="body"><a class="calibre8" id="pgfId-1110630"></a>where <i class="fm-timesitalic">n</i> is the total number of pixels and <i class="fm-timesitalic">n</i><sup class="fm-superscript">i</sup> is the total number of pixels belonging to the <i class="fm-timesitalic">i</i><sup class="fm-superscript">th</sup> class. After that, we reshape <span class="fm-code-in-text">y_true</span> to shape [batch size, -1] as preparation for an important step in weight computation. As the final output, we want to create a tensor out of weights, where we gather elements from the <span class="fm-code-in-text">y_weights</span> that correspond to elements in <span class="fm-code-in-text">y_true</span>. In other words, we fetch the value from <span class="fm-code-in-text">y_weights</span>, where the index to fetch is given by the values in <span class="fm-code-in-text">y_true</span>. At the end, the result will be of the same shape and size as <span class="fm-code-in-text">y_true</span>. This is all we need to weigh the samples: multiply weights element-wise with the loss value for each pixel. To achieve this, we will use the function <span class="fm-code-in-text">tf.gather()</span>, which gathers the elements from a given tensor (<span class="fm-code-in-text">params</span>) while taking a tensor that represents indices (<span class="fm-code-in-text">indices</span>) and returns a tensor that is of the same shape as the indices:</p>
  <pre class="programlisting">y_weights = tf.gather(params=weights, indices=y_true, batch_dims=1)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1110632"></a>Here, to ignore the batch dimension during performing the gather, we pass the argument <span class="fm-code-in-text">batch_dims</span> indicating how many batch dimensions we have. With that, we will define a function that outputs the weighted cross-entropy loss given a batch of predicted and true targets.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110633"></a>With the weights ready, we can now implement our first segmentation loss function. We will implement weighted cross-entropy loss. At a glance, the function masks irrelevant pixels (e.g., pixels belonging to unknown objects) and unwraps the predicted and true labels to get rid of the height and width dimensions. Finally, we can compute the cross-entropy loss using the built-in function in TensorFlow (see the next listing).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1110635"></a>Listing 8.14 Implementing the weighted cross-entropy loss</p>
  <pre class="programlisting">def ce_weighted_from_logits(num_classes):
    
    def loss_fn(y_true, y_pred):
        """ Defining cross entropy weighted loss """
 
        valid_mask = tf.cast(
            tf.reshape((y_true &lt;= num_classes - 1), [-1,1]), 'int32'
        )                                                                <span class="fm-combinumeral">❶</span>
 
        y_true = tf.cast(y_true, 'int32')                                <span class="fm-combinumeral">❷</span>
        y_true.set_shape([None, y_pred.shape[1], y_pred.shape[2]])       <span class="fm-combinumeral">❷</span>
 
        y_weights = get_label_weights(y_true, y_pred)                    <span class="fm-combinumeral">❸</span>
        y_pred_unwrap = tf.reshape(y_pred, [-1, num_classes])            <span class="fm-combinumeral">❹</span>
        y_true_unwrap = tf.reshape(y_true, [-1])                         <span class="fm-combinumeral">❹</span>
 
        return tf.reduce_mean(
            y_weights * tf.nn.sparse_softmax_cross_entropy_with_logits(  <span class="fm-combinumeral">❺</span>
                y_true_unwrap * tf.squeeze(valid_mask), 
                y_pred_unwrap * tf.cast(valid_mask, 'float32')) 
        )
    
    return loss_fn                                                       <span class="fm-combinumeral">❻</span></pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1128871"></a><span class="fm-combinumeral">❶</span> Define the valid mask, masking unnecessary pixels.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1128892"></a><span class="fm-combinumeral">❷</span> Some initial setup that casts y_true to int and sets the shape</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1128909"></a><span class="fm-combinumeral">❸</span> Get the label weights.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1128926"></a><span class="fm-combinumeral">❹</span> Unwrap y_pred and y_true so that batch, height, and width dimensions are squashed.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1128943"></a><span class="fm-combinumeral">❺</span> Compute the cross-entropy loss with y_true, y_pred, and the mask.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1128960"></a><span class="fm-combinumeral">❻</span> Return the function that computes the loss.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110666"></a>You might be thinking, “Why is the loss defined as a nested function?” This is a standard pattern we have to follow if we need to include extra arguments to our loss function (i.e., <span class="fm-code-in-text">num_classes</span>). All we are doing is capturing the computations of the loss function in the <span class="fm-code-in-text">loss_fn</span> function and then creating an outer function <span class="fm-code-in-text">ce_weighted_from_logits()</span> that will return the function that encapsulates the loss computations (i.e., <span class="fm-code-in-text">loss_fn</span>).</p>

  <p class="body"><a class="calibre8" id="pgfId-1110669"></a> Specifically, a valid mask is created to indicate whether the labels in <span class="fm-code-in-text">y_true</span> are less than the number of classes. Any label that has a value larger than the number of classes is ignored (e.g., unknown objects). Next, we get the weight vector and indicate a weight for each pixel using the <span class="fm-code-in-text">get_label_weights()</span> function. We will unwrap <span class="fm-code-in-text">y_pred</span> to a <span class="fm-code-in-text">[-1, num_classes]</span>-sized tensor, as <span class="fm-code-in-text">y_pred</span> contains <i class="fm-italics">logits</i> (i.e., unnormalized probability scores output by the model) across all classes in the data set. <span class="fm-code-in-text">y_true</span> will be unwrapped to a vector (i.e., a single dimension), as <span class="fm-code-in-text">y_true</span> only contains the class label. Finally, we use <span class="fm-code-in-text">tf.nn.sparse_softmax_cross_entropy_with_logits()</span> to compute the loss over masked predicted and true targets. The function takes two arguments, <span class="fm-code-in-text">labels</span> and <span class="fm-code-in-text">logits</span>, which are self-explanatory. We can make two salient observations:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1110671"></a>We are computing sparse cross-entropy loss (i.e., not standard cross-entropy loss).</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1110672"></a>We are computing cross-entropy loss from logits.</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1110673"></a>When using sparse cross entropy, we don’t have to one-hot encode the labels, so we can skip this, which leads to a more memory-efficient data pipeline. This is because one-hot encoding is handled internally by the model. By using a sparse loss, we have less to worry about.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110675"></a>Computing the loss from logits<a class="calibre8" id="marker-1110674"></a> (i.e., unnormalized scores) instead of from normalized probabilities leads to better and more stable gradients. Therefore, whenever possible, make sure to use logits<a class="calibre8" id="marker-1110676"></a> instead of normalized probabilities.</p>

  <p class="fm-head2"><a id="pgfId-1110677"></a>Dice loss</p>

  <p class="body"><a class="calibre8" id="pgfId-1110680"></a>The<a class="calibre8" id="marker-1110678"></a><a class="calibre8" id="marker-1110679"></a> second loss we will discuss is called the <i class="fm-italics">dice loss</i>, which is computed as</p>

  <p class="fm-equation"><img alt="08_11c" class="calibre10" src="../../OEBPS/Images/08_11c.png" width="378" height="69"/><br class="calibre2"/>
  <a id="pgfId-1112496"></a></p>

  <p class="body"><a class="calibre8" id="pgfId-1110685"></a>Here, the intersection between the prediction and target tensors can be computed with element-wise multiplication, whereas the union can be computed using element-wise addition between the prediction and the target tensors. You might be thinking that using element-wise operations is a strange way to compute intersection and union. To understand the reason behind this, I want to refer to a statement made earlier: a loss function used in a deep network must be <i class="fm-italics">differentiable</i>.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110686"></a>This means that we cannot use the standard conventions we use to compute intersection and union from a given set of values. Rather, we need to resort to a differentiable computation, leading to intersection and union between two tensors. Intersection can be computed by taking element-wise multiplication between the predicted and true targets. Union can be computed by taking the element-wise addition between the predicted and true targets. Figure 8.12 clarifies how these operations lead to intersection and union between two tensors.</p>

  <p class="fm-figure"><img alt="08-12" class="calibre10" src="../../OEBPS/Images/08-12.png" width="944" height="1419"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1137740"></a>Figure 8.12 Computations involved in dice loss. The intersection can be computed as a differentiable function by taking element-wise multiplication, whereas union can be computed as the element-wise sum.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110693"></a>This loss is predominantly focused on maximizing the intersection between the predicted and true targets. The multiplier of 2 is used to balance out the duplication of values that comes from the overlap between the intersection and the union, found in the denominator (see the following listing).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1110695"></a>Listing 8.15 Implementing the dice loss</p>
  <pre class="programlisting">def dice_loss_from_logits(num_classes):
    """ Defining the dice loss 1 - [(2* i + 1)/(u + i)]"""    
    
    def loss_fn(y_true, y_pred):
        
        smooth = 1.
        
        # Convert y_true to int and set shape
        y_true = tf.cast(y_true, 'int32')                                      <span class="fm-combinumeral">❶</span>
        y_true.set_shape([None, y_pred.shape[1], y_pred.shape[2]])             <span class="fm-combinumeral">❶</span>
 
        # Get pixel weights
        y_weights = tf.reshape(get_label_weights(y_true, y_pred), [-1, 1])     <span class="fm-combinumeral">❷</span>
 
        # Apply softmax to logits      
        y_pred = tf.nn.softmax(y_pred)                                         <span class="fm-combinumeral">❸</span>
 
        y_true_unwrap = tf.reshape(y_true, [-1])                               <span class="fm-combinumeral">❹</span>
        y_true_unwrap = tf.cast(
            tf.one_hot(tf.cast(y_true_unwrap, 'int32'), num_classes), 
<span class="fm-code-continuation-arrow">➥</span> 'float32'
        )                                                                      <span class="fm-combinumeral">❹</span>
        y_pred_unwrap = tf.reshape(y_pred, [-1, num_classes])                  <span class="fm-combinumeral">❹</span>
 
        intersection = tf.reduce_sum(y_true_unwrap * y_pred_unwrap * y_weights)<span class="fm-combinumeral">❺</span>
 
        union = tf.reduce_sum((y_true_unwrap + y_pred_unwrap) * y_weights)     <span class="fm-combinumeral">❻</span>
 
        score = (2. * intersection + smooth) / ( union + smooth)               <span class="fm-combinumeral">❼</span>
 
        loss = 1 - score                                                       <span class="fm-combinumeral">❽</span>
 
        return loss
    
    return loss_fn</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1128210"></a><span class="fm-combinumeral">❶</span> Initial setup for y_true</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1128231"></a><span class="fm-combinumeral">❷</span> Get the label weights and reshape it to a [-1, 1] shape.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1128261"></a><span class="fm-combinumeral">❸</span> Apply softmax on y_pred to get normalized<a id="marker-1128266"></a><a id="marker-1128267"></a> probabilities.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1128280"></a><span class="fm-combinumeral">❹</span> Unwrap y_pred and one-hot-encoded y_true to the [-1, num_classes] shape.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1128297"></a><span class="fm-combinumeral">❺</span> Compute intersection using element-wise multiplication.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1128314"></a><span class="fm-combinumeral">❻</span> Compute union using element-wise addition.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1128331"></a><span class="fm-combinumeral">❼</span> Compute the dice coefficient.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1128348"></a><span class="fm-combinumeral">❽</span> Compute the dice loss.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110741"></a>Here, <span class="fm-code-in-text">smooth</span> is a smoothing parameter that we’ll use to avoid potential NaN values resulting in division by zero. After that we do the following:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1110742"></a>Obtain weights for each <span class="fm-code-in-text">y_true</span> label</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1110743"></a>Apply a softmax activation to <span class="fm-code-in-text">y_pred</span></p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1110744"></a>Unwrap <span class="fm-code-in-text">y_pred</span> to the <span class="fm-code-in-text">[-1, num_classes]</span> tensor and <span class="fm-code-in-text">y_true</span> to a <span class="fm-code-in-text">[-1]</span>-sized vector</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1110745"></a>Then intersection and union are computed for <span class="fm-code-in-text">y_pred</span> and <span class="fm-code-in-text">y_true</span>. Specifically, intersection is computed as the result of element-wise multiplication of <span class="fm-code-in-text">y_pred</span> and <span class="fm-code-in-text">y_true</span> and the union as the result of the element-wise addition of <span class="fm-code-in-text">y_pred</span> and <span class="fm-code-in-text">y_true</span>.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1110746"></a>Focal loss</p>

    <p class="fm-sidebar-text"><a id="pgfId-1110747"></a><i class="fm-italics">Focal loss</i> is a relatively novel loss introduced in the paper “Focal Loss for Dense Object Prediction” (<span class="fm-hyperlink"><a class="url" href="https://arxiv.org/pdf/1708.02002.pdf">https://arxiv.org/pdf/1708.02002.pdf</a></span>). Focal loss was introduced to combat the severe class imbalance found in segmentation tasks. Specifically, it solves a problem in many easy examples (e.g., samples from common classes with smaller loss), over-powering small numbers of hard examples (e.g., samples from rare classes with larger loss). Focal loss solves this problem by introducing a modulating factor that will down-weight easy examples, so, naturally, the loss function focuses more on learning hard examples.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1110749"></a>The loss function we will use to optimize the segmentation model will be the loss resulting from addition of sparse cross-entropy loss and dice loss (see the next listing).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1110751"></a>Listing 8.16 Final combined loss function</p>
  <pre class="programlisting">def ce_dice_loss_from_logits(num_classes):
    
    def loss_fn(y_true, y_pred):
        # Sum of cross entropy and dice losses
        loss = ce_weighted_from_logits(num_classes)(
            tf.cast(y_true, 'int32'), y_pred
        ) + dice_loss_from_logits(num_classes)(
            y_true, y_pred
        )    
            
        return loss
    
    return loss_fn </pre>

  <p class="body"><a class="calibre8" id="pgfId-1110768"></a>Next, we will discuss<a class="calibre8" id="marker-1110766"></a><a class="calibre8" id="marker-1110767"></a> evaluation metrics.</p>

  <h3 class="fm-head1" id="sigil_toc_id_113"><a id="pgfId-1110769"></a>8.4.2 Evaluation metrics</h3>

  <p class="body"><a class="calibre8" id="pgfId-1110770"></a>Evaluation metrics play a vital role in model training as a health check for the model. This means low performance/issues can be quickly identified by making sure evaluation metrics behave in a reasonable way. Here we will discuss three different metrics:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1110771"></a>Pixel</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1110772"></a>Mean accuracy</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1110773"></a>Mean IoU</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1110774"></a>We will implement these as custom metrics by leveraging some of the existing metrics in TensorFlow, where you have to subclass from the <span class="fm-code-in-text">tf.keras.metrics.Metric</span> class<a class="calibre8" id="marker-1110775"></a> or one of the existing metrics. This means that you create a new Python class, which inherits from the base <span class="fm-code-in-text">tf.keras.metrics.Metric</span> base class<a class="calibre8" id="marker-1110776"></a> of one of the existing concrete metrics classes:</p>
  <pre class="programlisting">class MyMetric(tf.keras.metrics.Metric):
 
  def __init__(self, name='binary_true_positives', **kwargs):
    super(MyMetric, self).__init__(name=name, **kwargs)
 
    # Create state related variables
 
  def update_state(self, y_true, y_pred, sample_weight=None):
   
    # update state in this function
 
  def result(self):
    
    # We return the result computed from the state
 
  def reset_states():
    # Do what’s required to reset the maintained states
    # This function is called between epochs     </pre>

  <p class="body"><a class="calibre8" id="pgfId-1110796"></a>The first thing you need to understand about a metric is that it is a stateful object, meaning it maintains a state. For example, a single epoch has multiple iterations and assumes you’re interested in computing the accuracy. The metric needs to accumulate the values required to compute the accuracy over all the iterations so that at the end, it can compute the average accuracy for that epoch. When defining a metric, there are three functions you need to be mindful of: <span class="fm-code-in-text">__init__</span>, <span class="fm-code-in-text">update_state</span>, <span class="fm-code-in-text">result</span>, and <span class="fm-code-in-text">reset_states</span>.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110798"></a>Let’s get concrete by assuming that we are implementing an accuracy metric (i.e., the number of elements in <span class="fm-code-in-text">y_pred</span> that matched <span class="fm-code-in-text">y_true</span> as a percentage). It needs to maintain a total: the sum of all the accuracy values we passed and the count (number of accuracy values we passed). With these two state elements, we can compute the mean accuracy at any time. When implementing the accuracy metric, you implement these functions:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1110799"></a><span class="fm-code-in-text">__init__</span>—Defines two states; total and count</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1110801"></a><span class="fm-code-in-text">update_state</span><a class="calibre8" id="marker-1110800"></a>—Updates total and count based on <span class="fm-code-in-text">y_true</span> and <span class="fm-code-in-text">y_pred</span></p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1110803"></a><span class="fm-code-in-text">result</span>—Computes the mean accuracy as total/count</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1110805"></a><span class="fm-code-in-text">reset_states</span>—Resets both the total and count (this needs to happen at the beginning of an epoch)</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1110810"></a>Let’s see how this knowledge translates to the evaluation<a class="calibre8" id="marker-1110806"></a><a class="calibre8" id="marker-1110807"></a><a class="calibre8" id="marker-1110808"></a><a class="calibre8" id="marker-1110809"></a> metrics we’re interested in solving.</p>

  <p class="fm-head2"><a id="pgfId-1110811"></a>Pixel and mean accuracies</p>

  <p class="body"><a class="calibre8" id="pgfId-1110812"></a>Pixel accuracy is the simplest metric you can think of. It measures the pixel-wise accuracy between the prediction and the true target (see the next listing).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1110814"></a>Listing 8.17 Implementing the pixel accuracy metric</p>
  <pre class="programlisting">class PixelAccuracyMetric(tf.keras.metrics.Accuracy):
 
  def __init__(self, num_classes, name='pixel_accuracy', **kwargs):
    super(PixelAccuracyMetric, self).__init__(name=name, **kwargs)    
 
  def update_state(self, y_true, y_pred, sample_weight=None):
    
    y_true.set_shape([None, y_pred.shape[1], y_pred.shape[2]])      <span class="fm-combinumeral">❶</span>
    y_true = tf.reshape(y_true, [-1])                               <span class="fm-combinumeral">❷</span>
 
    y_pred = tf.reshape(tf.argmax(y_pred, axis=-1),[-1])            <span class="fm-combinumeral">❸</span>
 
    valid_mask = tf.reshape((y_true &lt;= num_classes - 1), [-1])      <span class="fm-combinumeral">❹</span>
 
    y_true = tf.boolean_mask(y_true, valid_mask)                    <span class="fm-combinumeral">❺</span>
    y_pred = tf.boolean_mask(y_pred, valid_mask)                    <span class="fm-combinumeral">❺</span>
    
    super(PixelAccuracyMetric, self).update_state(y_true, y_pred)   <span class="fm-combinumeral">❻</span></pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1127849"></a><span class="fm-combinumeral">❶</span> Set the shape of y_true (in case it is undefined).</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1127870"></a><span class="fm-combinumeral">❷</span> Reshape y_true to a vector.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1127887"></a><span class="fm-combinumeral">❸</span> Reshape y_pred after taking argmax to a vector.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1127907"></a><span class="fm-combinumeral">❹</span> Define a valid mask (mask out unnecessary pixels).</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1127924"></a><span class="fm-combinumeral">❺</span> Gather pixels/labels that satisfy the valid_mask condition.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1127941"></a><span class="fm-combinumeral">❻</span> With the processed y_true and y_pred, compute the accuracy using the update_state() function.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110839"></a>Pixel accuracy computes the one-to-one match between predicted pixels and true pixels. To compute this, we subclass from <span class="fm-code-in-text">tf.keras.metrics.Accuracy</span> as it has all the computations we need. To do this, we override the <span class="fm-code-in-text">update_state</span> function as shown. There are a few things we need to take care of:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1110840"></a>We need to set the shape of <span class="fm-code-in-text">y_true</span> as a precaution. This is because when working with <span class="fm-code-in-text">tf.data.Dataset</span>, sometimes the shape is lost.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1110841"></a>Reshape <span class="fm-code-in-text">y_true</span> to a vector.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1110842"></a>Get the class labels of <span class="fm-code-in-text">y_pred</span> by performing <span class="fm-code-in-text">tf.argmax()</span> and reshape it to a vector.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1110843"></a>Define a valid mask that ignores unwanted classes (e.g., unknown objects).</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1110845"></a>Get the pixels that satisfy only the <span class="fm-code-in-text">valid_mask</span> filter<a class="calibre8" id="marker-1110844"></a>.</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1110846"></a>Once we complete these tasks, we simply call the parent object’s (i.e<span class="fm-code-in-text">.</span>, <span class="fm-code-in-text">tf.keras .metrics.Accuracy</span><a class="calibre8" id="marker-1110847"></a>) <span class="fm-code-in-text">update_state</span> method<a class="calibre8" id="marker-1110848"></a> with the corresponding arguments. We don’t have to override <span class="fm-code-in-text">result()</span> and <span class="fm-code-in-text">reset_states()</span> functions<a class="calibre8" id="marker-1110852"></a>, as they already contain the correct computations.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110853"></a>We said that class imbalance is prevalent in image segmentation problems. Typically, background pixels will spread in a large region of the image, potentially leading to misguided conclusions. Therefore, a slightly better approach might be to compute the accuracy individually per class and then average it. Enter mean accuracy, which prevents the undesired characteristics of pixel accuracy (see the next listing).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1110855"></a>Listing 8.18 Implementing the mean accuracy metric</p>
  <pre class="programlisting">class MeanAccuracyMetric(tf.keras.metrics.Mean):
 
  def __init__(self, num_classes, name='mean_accuracy', **kwargs):
    super(MeanAccuracyMetric, self).__init__(name=name, **kwargs)    
 
  def update_state(self, y_true, y_pred, sample_weight=None):
    
    smooth = 1            
  
    y_true.set_shape([None, y_pred.shape[1], y_pred.shape[2]])             <span class="fm-combinumeral">❶</span>
 
    y_true = tf.reshape(y_true, [-1])                                      <span class="fm-combinumeral">❶</span>
    y_pred = tf.reshape(tf.argmax(y_pred, axis=-1),[-1])                   <span class="fm-combinumeral">❶</span>
 
    valid_mask = tf.reshape((y_true &lt;= num_classes - 1), [-1])             <span class="fm-combinumeral">❶</span>
 
    y_true = tf.boolean_mask(y_true, valid_mask)                           <span class="fm-combinumeral">❶</span>
    y_pred = tf.boolean_mask(y_pred, valid_mask)                           <span class="fm-combinumeral">❶</span>
 
    conf_matrix = tf.cast(                                                            
        tf.math.confusion_matrix(y_true, y_pred, num_classes=num_classes), 
<span class="fm-code-continuation-arrow">➥</span> 'float32'                                                               <span class="fm-combinumeral">❷</span>
    )
    true_pos = tf.linalg.diag_part(conf_matrix)                            <span class="fm-combinumeral">❸</span>
 
    mean_accuracy = tf.reduce_mean(
        (true_pos + smooth)/(tf.reduce_sum(conf_matrix, axis=1) + smooth)  <span class="fm-combinumeral">❹</span>
    )
    
    super(MeanAccuracyMetric, self).update_state(mean_accuracy)            <span class="fm-combinumeral">❺</span></pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1127386"></a><span class="fm-combinumeral">❶</span> Initial setup</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1127407"></a><span class="fm-combinumeral">❷</span> Compute the confusion matrix using y_true and y_pred.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1127427"></a><span class="fm-combinumeral">❸</span> Get the true positives (elements on the diagonal).</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1127444"></a><span class="fm-combinumeral">❹</span> Compute the mean accuracy using true positives and true class counts for each class.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1127461"></a><span class="fm-combinumeral">❺</span> Compute the average of mean_accuracy using the update_state() function.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110890"></a>The <span class="fm-code-in-text">MeanAccuracyMetric</span> will branch out from <span class="fm-code-in-text">tf.keras.metrics.Mean</span>, which computes the average over a given sequence of values. The plan is to compute the <span class="fm-code-in-text">mean_accuracy</span> within the <span class="fm-code-in-text">update_state()</span> function and then pass the value to the parent’s <span class="fm-code-in-text">update_state()</span> function<a class="calibre8" id="marker-1110891"></a> so that we get the average value of mean accuracy. First, we perform the initial setup and clean-up of <span class="fm-code-in-text">y_true</span> and <span class="fm-code-in-text">y_pred</span> we discussed earlier.</p>

  <p class="fm-figure"><img alt="08-13" class="calibre10" src="../../OEBPS/Images/08-13.png" width="753" height="489"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1137781"></a>Figure 8.13 Illustration of a confusion matrix for a five-class classification problem. The shaded boxes represent true positives.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110892"></a>Afterward, we compute the confusion matrix (figure 8.13) from predicted and true targets. A confusion matrix for an <i class="fm-timesitalic">n</i>-way classification problem (i.e., a classification problem with <i class="fm-timesitalic">n</i> possible classes) is defined as a <i class="fm-timesitalic">n</i> × <i class="fm-timesitalic">n</i> matrix. Here, the element at the (<i class="fm-timesitalic">i</i>, <i class="fm-timesitalic">j</i>) position indicates how many instances were predicted as belonging to the <i class="fm-timesitalic">i</i> <sup class="fm-superscript">th</sup> class but actually belong to the <i class="fm-timesitalic">j</i> <sup class="fm-superscript">th</sup> class. Figure 8.13 portrays this type of confusion matrix. We can get the true positives by extracting the diagonal (i.e., (<i class="fm-timesitalic">i</i>, <i class="fm-timesitalic">i</i>) elements in the matrix for all 1 &lt; = <i class="fm-timesitalic">i</i> &lt; = <i class="fm-timesitalic">n</i>). We can now compute the mean accuracy in two steps:</p>

  <ol class="calibre11">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1110893"></a>Perform element-wise division on the true positive count by actual counts for all the classes. This produces a vector whose elements represents per-class accuracy.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1110894"></a>Compute the vector mean that resulted from step 1.</p>
    </li>
  </ol>

  <p class="body"><a class="calibre8" id="pgfId-1110896"></a>Finally, we pass the mean accuracy to its parent’s <span class="fm-code-in-text">update_state()</span> function<a class="calibre8" id="marker-1122651"></a>.</p>

  <p class="fm-head2"><a id="pgfId-1110903"></a>Mean IoU</p>

  <p class="body"><a class="calibre8" id="pgfId-1110905"></a><i class="fm-italics">Mean IoU</i> (mean intersection over union<a class="calibre8" id="marker-1110904"></a>) is a popular evaluation metric pick for segmentation tasks and has close ties to the dice loss we discussed earlier, as they both use the concept of intersection and union to compute the final result (see the next listing).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1110907"></a>Listing 8.19 Implementing the mean IoU metric</p>
  <pre class="programlisting">class MeanIoUMetric(tf.keras.metrics.MeanIoU):
 
  def __init__(self, num_classes, name='mean_iou', **kwargs):
    super(MeanIoUMetric, self).__init__(num_classes=num_classes, name=name, **kwargs)    
 
  def update_state(self, y_true, y_pred, sample_weight=None):
    
    y_true.set_shape([None, y_pred.shape[1], y_pred.shape[2]])
    y_true = tf.reshape(y_true, [-1])
           
    y_pred = tf.reshape(tf.argmax(y_pred, axis=-1),[-1])
 
    valid_mask = tf.reshape((y_true &lt;= num_classes - 1), [-1])
 
    # Get pixels corresponding to valid mask
    y_true = tf.boolean_mask(y_true, valid_mask)
    y_pred = tf.boolean_mask(y_pred, valid_mask)
    
    super(MeanIoUMetric, self).update_state(y_true, y_pred)    <span class="fm-combinumeral">❶</span></pre>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1127319"></a><span class="fm-combinumeral">❶</span> After the initial setup of y_true and y_pred, all we need to do is call the parent’s update_state() function.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110930"></a>The mean IoU computations are already found in <span class="fm-code-in-text">tf.keras.metrics.MeanIoU</span>. Therefore, we will use that as our parent class. All we need to do is perform the aforementioned setup for <span class="fm-code-in-text">y_true</span> and <span class="fm-code-in-text">y_pred</span> and then call the parent’s <span class="fm-code-in-text">update_state()</span> function<a class="calibre8" id="marker-1112558"></a>. Mean IoU is computed as</p>

  <p class="fm-equation"><img alt="08_13a" class="calibre10" src="../../OEBPS/Images/08_13a.png" width="588" height="72"/><br class="calibre2"/>
  <a id="pgfId-1112539"></a></p>

  <p class="body"><a class="calibre8" id="pgfId-1110936"></a>Various elements used in this computation are depicted in figure 8.14.</p>

  <p class="fm-figure"><img alt="08-14" class="calibre10" src="../../OEBPS/Images/08-14.png" width="781" height="544"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1137813"></a>Figure 8.14 Confusion matrix and how it can be used to compute false positives, false negatives, and true positives</p>

  <p class="body"><a class="calibre8" id="pgfId-1110944"></a>We now understand the loss functions<a class="calibre8" id="marker-1110943"></a> and evaluation metrics that are available to us and have already implemented them. We can incorporate these losses to compile the model:</p>
  <pre class="programlisting">deeplabv3.compile(
    loss=ce_dice_loss_from_logits(num_classes), 
    optimizer=optimizer, 
    metrics=[
        MeanIoUMetric(num_classes), 
        MeanAccuracyMetric(num_classes), 
        PixelAccuracyMetric(num_classes)
    ])</pre>

  <p class="body"><a class="calibre8" id="pgfId-1110953"></a>Remember that we stored the weights from a convolution block we removed earlier. Now that we have compiled the model, we can copy the weights to the new model using the following syntax:</p>
  <pre class="programlisting"># Setting weights for newly added layers
for k, w <i class="fm-italics">in</i> w_dict.items():    
    deeplabv3.get_layer(k).set_weights(w)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1110957"></a>We now move on to training the model with the data pipeline and the model we defined.</p>

  <p class="fm-head2"><a id="pgfId-1110958"></a>Exercise 4</p>

  <p class="body"><a class="calibre8" id="pgfId-1110959"></a>You are coming up with a new loss function that computes the disjunctive union between <span class="fm-code-in-text">y_true</span> and <span class="fm-code-in-text">y_pred</span>. The disjunctive union between two sets A and B is the set of elements that are in either A or B but not in the intersection. You know you can compute the intersection with element-wise multiplication and union with element-wise addition of <span class="fm-code-in-text">y_true</span> and <span class="fm-code-in-text">y_pred</span>. Write the equation to compute the disjunctive union as a function of <span class="fm-code-in-text">y_true</span><a class="calibre8" id="marker-1110960"></a><a class="calibre8" id="marker-1110961"></a> and <span class="fm-code-in-text">y_pred</span>.</p>

  <h2 class="fm-head" id="sigil_toc_id_114"><a id="pgfId-1110962"></a>8.5 Training the model</h2>

  <p class="body"><a class="calibre8" id="pgfId-1110965"></a>You’re<a class="calibre8" id="marker-1110963"></a><a class="calibre8" id="marker-1110964"></a> coming to the final stages of the first iteration of your product. Now it’s time to put the data and knowledge you garnered to good use (i.e., train the model). We will train the model for 25 epochs and monitor the pixel accuracy, mean accuracy, and mean IoU metrics. During the training, we will measure the performance on validation data set.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110966"></a>Training the model is the easiest part, as we have done the hard work that leads up to training. It is now just a matter of calling <span class="fm-code-in-text">fit()</span> with the correct parameters on the DeepLab v3 we just defined, as the following listing shows.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1110968"></a>Listing 8.20 Training the model</p>
  <pre class="programlisting">if <i class="fm-italics">not</i> os.path.exists('eval'):
    os.mkdir('eval')
    
csv_logger = tf.keras.callbacks.CSVLogger(
    os.path.join('eval','1_pretrained_deeplabv3.log')                      <span class="fm-combinumeral">❶</span>
 
monitor_metric = 'val_loss'
mode = 'min' if 'loss' <i class="fm-italics">in</i> monitor_metric else 'max'                        <span class="fm-combinumeral">❷</span>
print("Using metric={} and mode={} for EarlyStopping".format(monitor_metric, mode))
lr_callback = tf.keras.callbacks.ReduceLROnPlateau(
    monitor=monitor_metric, factor=0.1, patience=3, mode=mode, min_lr=1e-8 <span class="fm-combinumeral">❸</span>
)
es_callback = tf.keras.callbacks.EarlyStopping(
    monitor=monitor_metric, patience=6, mode=mode                          <span class="fm-combinumeral">❹</span>
)
 
# Train the model
deeplabv3.fit(                                                             <span class="fm-combinumeral">❺</span>
    x=tr_image_ds, steps_per_epoch=n_train,
    validation_data=val_image_ds, validation_steps=n_valid, 
    epochs=epochs, callbacks=[lr_callback, csv_logger, es_callback])</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1127003"></a><span class="fm-combinumeral">❶</span> Train logger</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1127039"></a><span class="fm-combinumeral">❷</span> Set the mode for the following callbacks automatically by looking at the metric name.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1127056"></a><span class="fm-combinumeral">❸</span> Learning rate scheduler</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1127073"></a><span class="fm-combinumeral">❹</span> Early stopping callback</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1127004"></a><span class="fm-combinumeral">❺</span> Train the model while using the validation set for learning rate adaptation and early stopping.</p>

  <p class="body"><a class="calibre8" id="pgfId-1110996"></a>First, we will define a directory called eval if it does not exist. The training logs will be saved in this directory. Next, we define three different callbacks to be used during the training:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1110998"></a><span class="fm-code-in-text">csv_logger</span>—Logs the training loss/metrics and validation loss/metrics</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1111000"></a><span class="fm-code-in-text">lr_callback</span><a class="calibre8" id="marker-1110999"></a>—Reduces the learning rate by a factor of 10, if the validation loss does not decrease within three epochs</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1111002"></a><span class="fm-code-in-text">es_callback</span>—Performs early stopping if the validation loss does not decrease within six epochs</p>
    </li>
  </ul>

  <p class="fm-callout"><a id="pgfId-1111003"></a><span class="fm-callout-head">NOTE</span> On an Intel Core i5 machine with an NVIDIA GeForce RTX 2070 8GB, the training took approximately 45 minutes to run 25 epochs.</p>

  <p class="body"><a class="calibre8" id="pgfId-1111004"></a>With that, we call <span class="fm-code-in-text">deeplabv3.fit()</span> with the following parameters:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1111005"></a><span class="fm-code-in-text">x</span>—The tf.data pipeline producing training instances (set to <span class="fm-code-in-text">tr_image_ds</span>).</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1111007"></a><span class="fm-code-in-text">steps_per_epoch</span><a class="calibre8" id="marker-1111006"></a>—Number of steps per epoch. This is obtained by computing the number of training instances and dividing it by the batch size (set to <span class="fm-code-in-text">n_train</span>).</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1111009"></a><span class="fm-code-in-text">validation_data</span><a class="calibre8" id="marker-1111008"></a>—The <span class="fm-code-in-text">tf.data</span> pipeline producing validation instances. This is obtained by computing the number of validation instances and dividing it by the batch size (set to <span class="fm-code-in-text">val_image_ds</span>).</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1111011"></a><span class="fm-code-in-text">epochs</span>—Number of epochs (set to epochs).</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1111013"></a><span class="fm-code-in-text">callbacks</span>—The callbacks we set up earlier (set to <span class="fm-code-in-text">[lr_callback, csv_logger, es_callback]</span>).</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1111014"></a>After the model is trained, we will evaluate it on the test set. We will also visualize segmentations generated by the model.</p>

  <p class="fm-head2"><a id="pgfId-1111015"></a>Exercise 5</p>

  <p class="body"><a class="calibre8" id="pgfId-1111016"></a>You have a data set of 10,000 samples and have split it into 90% training data and 10% validation data. You use a batch size of 10 for training and a batch size of 20 for validation. How many training and validation steps will be there in a single<a class="calibre8" id="marker-1111017"></a><a class="calibre8" id="marker-1111018"></a> epoch?</p>

  <h2 class="fm-head" id="sigil_toc_id_115"><a id="pgfId-1111019"></a>8.6 Evaluating the model</h2>

  <p class="body"><a class="calibre8" id="pgfId-1111022"></a>Let’s<a class="calibre8" id="marker-1111020"></a><a class="calibre8" id="marker-1111021"></a> take a moment to reflect on what we have done so far. We defined a data pipeline to read images and prepare them as inputs and targets for the model. Then we defined a model known as DeepLab v3 that uses a pretrained ResNet-50 as its backbone and a special module called atrous spatial pyramid pooling to predict the final segmentation mask. Then we defined task-specific losses and metrics to make sure we could evaluate the model with a variety of metrics. Afterward, we trained the model. Now it’s time for the ultimate reveal. We will measure the performance on an unseen test data set to see how well the model does. We will also visualize the model outputs and compare them against the real targets by plotting them side by side.</p>

  <p class="body"><a class="calibre8" id="pgfId-1111023"></a>We can run the model over the unseen test images and gauge how well it is performing. To do that, we execute the following:</p>
  <pre class="programlisting">deeplabv3.evaluate(test_image_ds, steps=n_valid)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1111025"></a>The size of the test set is the same as the validation set, as we split the images listed in val.txt into two equal validation and test sets. This will return around</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1111026"></a>62% mean IoU</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1111027"></a>87% mean accuracy</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1111028"></a>91% pixel accuracy</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1111029"></a>These are very respectable scores given our circumstances. Our training data set consists of less than 1,500 segmented images. Using this data, we were able to train a model that achieves around 62% mean IoU on a test data set of approximately size 725.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1111030"></a>What does state of the art look like?</p>

    <p class="fm-sidebar-text"><a id="pgfId-1111031"></a>The state-of-the-art performance on Pascal VOC 2012 reports around 90% mean IoU (<span class="fm-hyperlink"><a class="url" href="http://mng.bz/o2m2">http://mng.bz/o2m2</a></span>). However, these are models that are much larger and complex than what we used here. Furthermore, they are typically trained with significantly more data by using an auxiliary data set known as the semantic boundary data set (SBD<a id="marker-1115259"></a>) (introduced in the paper <span class="fm-hyperlink"><a class="url" href="http://mng.bz/nNve">http://mng.bz/nNve</a></span>). This will push the training datapoint count to over 10,000 (close to seven times the size of our current training set).</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1111034"></a>You can further investigate the model by visually inspecting some of the results our module produces. After all, it is a vision model that we are developing. Therefore, we should not rely solely on numbers to make decisions and conclusions. We should also visually analyze the results before settling on a conclusion.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1111035"></a>What would the results from a U-Net based network look like?</p>

    <p class="fm-sidebar-text"><a id="pgfId-1111036"></a>Under similar conditions provided for the DeepLab v3 model, the U-Net model built with a pretrained ResNet-50 model as the encoder was only able to achieve approximately 32.5% mean IoU, 78.5% mean accuracy, and 81.5% pixel accuracy. The implementation is provided in the Jupyter notebook in the ch08 folder.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1111037"></a>On an Intel Core i5 machine with an NVIDIA GeForce RTX 2070 8GB, the training took approximately 55 minutes to run 25 epochs.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1111038"></a>There is a detailed explanation of the U-Net model in appendix B.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1111040"></a>To complete this investigation, we will get a random sample from the test set and ask the model to predict the segmentation map for each of those images. Then we will plot the results side by side to ensure that our model is doing a good job (figure 8.15).</p>

  <p class="fm-figure"><img alt="08-15" class="calibre10" src="../../OEBPS/Images/08-15.png" width="1103" height="1094"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1137847"></a>Figure 8.15 Comparing the true annotated targets to model predictions. You can see that the model is quite good at separating objects from different backgrounds.</p>

  <p class="body"><a class="calibre8" id="pgfId-1111047"></a>We can see that unless it is an extremely difficult image (e.g., the top-left image, where there’s a car obscured by a gate), our model does a very good job. It can identify almost all the images found in the sample we analyzed with high accuracy. The code for visualizing the images is provided in the notebook.</p>

  <p class="body"><a class="calibre8" id="pgfId-1111048"></a>This concludes our discussion of <a id="marker-1111119"></a>image segmentation. In the next few chapters, we will discuss several natural language processing problems.</p>

  <p class="fm-head2"><a id="pgfId-1111049"></a>Exercise 6</p>

  <p class="body"><a class="calibre8" id="pgfId-1111050"></a>You are given</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1111051"></a>A model (called model)</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1111053"></a>A batch of images called <span class="fm-code-in-text">batch_image</span> (already preprocessed and ready to be fed to a model)</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1111055"></a>A corresponding batch of targets, <span class="fm-code-in-text">batch_targets</span> (the true segmentation mask in one-hot encoded format)</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1111056"></a>Write a function called <span class="fm-code-in-text">get_top_bad_examples(model, batch_images, batch_targets, n)</span> that will return the top <span class="fm-code-in-text">n</span> indices of the hardest (highest loss) images in <span class="fm-code-in-text">batch_ images</span>. Given a predicted mask and a target mask, you can use the sum over element-wise multiplication as the loss of a given image.</p>

  <p class="body"><a class="calibre8" id="pgfId-1111057"></a>You can use the <span class="fm-code-in-text">model.predict()</span> function to make a prediction on <span class="fm-code-in-text">batch_ images</span>, and it will return a predicted mask as the same size as <span class="fm-code-in-text">batch_targets</span>. Once you compute the losses for the batch (<span class="fm-code-in-text">batch_loss</span>), you can use the <span class="fm-code-in-text">tf.math.top_k(batch_ loss, n)</span> function<a class="calibre8" id="marker-1123087"></a><a class="calibre8" id="marker-1123088"></a> to get the indices of elements with the highest value. <span class="fm-code-in-text">tf.math .top_k()</span> returns a tuple containing the top values and indices of a given vector, in that<a class="calibre8" id="marker-1123089"></a><a class="calibre8" id="marker-1123090"></a> order.</p>

  <h2 class="fm-head" id="sigil_toc_id_116"><a id="pgfId-1111063"></a>Summary</h2>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1111064"></a>Segmentation models fall into two broad categories: semantic segmentation and instance segmentation.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1111065"></a>The <span class="fm-code-in-text">tf.data</span> API provides various functionality to implement complex data pipelines, such as using custom NumPy functions, performing quick transformations using <span class="fm-code-in-text">tf.data.Dataset.map()</span>, and I/O optimization techniques like prefetch and cache.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1111066"></a>DeepLab v3 is a popular segmentation model that uses a pretrained ResNet-50 model as its backbone and atrous convolutions to increase the receptive field by inserting holes (i.e., zeros) between the kernel weights.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1111067"></a>The DeepLab v3 model uses a module known as atrous spatial pyramid pooling to aggregate information at multiple scales, which helps to create a fine-grained segmented output.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1111068"></a>In segmentation tasks, cross entropy and dice loss are two popular losses, whereas pixel accuracy, mean accuracy, and mean IoU are popular evaluation metrics.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1111069"></a>In TensorFlow, loss functions can be implemented as stateless functions. But metrics must be implemented as stateful objects by subclassing from the <span class="fm-code-in-text">tf.keras.metrics.Metric</span> base class or a suitable class.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1111070"></a>The DeepLab v3 model achieved a very good accuracy of 62% mean IoU on the Pascal VOC 2010 data set.</p>
    </li>
  </ul>

  <h2 class="fm-head" id="sigil_toc_id_117"><a id="pgfId-1111071"></a>Answers to exercises</h2>

  <p class="body"><a class="calibre8" id="pgfId-1111072"></a><b class="fm-bold">Exercise 1</b></p>
  <pre class="programlisting">palettized_image = np.zeros(shape=rgb_image.shape[:-1])
for i in range(rgb_image.shape[0]):
    for j in range(rgb_image.shape[1]):
        for k in range(palette.shape[0]):
            if (palette[k] == rgb_image[i,j]).all():
                palettized_image[i,j] = k
                break</pre>

  <p class="body"><a class="calibre8" id="pgfId-1111080"></a><b class="fm-bold">Exercise 2</b></p>
  <pre class="programlisting">dataset_a = tf.data.Dataset.from_tensor_slices(a)
dataset_b = tf.data.Dataset.from_tensor_slices(b)
 
image_ds = tf.data.Dataset.zip((dataset_a, dataset_b))
 
image_ds = image_ds.map(
            lambda x, y: (x, tf.image.resize(y, (64,64),  method='nearest'))
        )
 
image_ds = image_ds.map(
            lambda x, y: ((x-128.0)/255.0, tf.image.resize(y, (64,64),  method='nearest'))
        )
 
image_ds = image_ds.batch(32).repeat(5).prefetch(tf.data.experimental.AUTOTUNE)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1111096"></a><b class="fm-bold">Exercise 3</b></p>
  <pre class="programlisting">import tensorflow.keras.layers as layers
 
def aug_aspp(out_1, out_2):
    
    atrous_out_1 = layers.Conv2D(128, (3,3), dilation_rate=16, 
<span class="fm-code-continuation-arrow">➥</span> padding='same', activation='relu')(out_1)
    
    atrous_out_2_1 = layers.Conv2D(128, (3,3), dilation_rate=8, 
<span class="fm-code-continuation-arrow">➥</span> padding='same', activation='relu')(out_1)
    atrous_out_2_2 = layers.Conv2D(128, (3,3), dilation_rate=8, 
<span class="fm-code-continuation-arrow">➥</span> padding='same', activation='relu')(out_2)
    atrous_out_2 = layers.Concatenate()([atrous_out_2_1, atrous_out_2_2])
    
    tmp1 = layers.Conv2D(64, (1,1), padding='same', activation='relu')(atrous_out_1)
    tmp2 = layers.Conv2D(64, (1,1), padding='same', activation='relu')(atrous_out_2)
    conv_out = layers.Concatenate()([tmp1,tmp2])
    
    out = layers.UpSampling2D((2,2), interpolation='bilinear')(conv_out)
    out = layers.Activation('sigmoid')(out)
    
    return out</pre>

  <p class="body"><a class="calibre8" id="pgfId-1111116"></a><b class="fm-bold">Exercise 4</b></p>
  <pre class="programlisting">out = (y_pred - (y_pred * y_true)) + (y_true - (y_pred * y_true))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1111118"></a><b class="fm-bold">Exercise 5</b></p>

  <div class="programlisting"></div>

  <p class="body"><a class="calibre8" id="pgfId-1111121"></a><b class="fm-bold">Exercise 6</b></p>
  <pre class="programlisting">def get_top_n_bad_examples(model, batch_images, batch_targets, n):
    
    batch_pred = model.predict(batch_images)
    
    batch_loss = tf.reduce_sum(batch_pred*batch_targets, axis=[1,2,3])
    
    _, hard_inds = tf.math.top_k(batch_loss, n)
    
    return hard_inds</pre>
</div>
</div>
</body>
</html>