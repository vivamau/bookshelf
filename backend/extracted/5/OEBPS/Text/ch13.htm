<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xml:lang="en"
      lang="en"
      xmlns="http://www.w3.org/1999/xhtml"
      xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<title>TensorFlow in Action</title>
<link rel="stylesheet" type="text/css" href="../../override_v1.css"/>
<link rel="stylesheet" type="text/css" href="../../stylesheet.css"/><link rel="stylesheet" type="text/css" href="../../page_styles.css"/>
</head>
<body>
<div id="book-content">
<div id="sbo-rt-content" class="calibre"><h1 class="tochead"><a id="pgfId-1153810"></a>13 Transformers</h1>

  <p class="co-summary-head"><a id="pgfId-1153812"></a>This chapter<a id="marker-1156255"></a> covers</p>

  <ul class="calibre9">
    <li class="co-summary-bullet"><a class="calibre8" id="pgfId-1153813"></a>Implementing a full Transformer model with all the components</li>

    <li class="co-summary-bullet"><a class="calibre8" id="pgfId-1153814"></a>Implementing a spam classifier using a pretrained BERT model from TFHub</li>

    <li class="co-summary-bullet"><a class="calibre8" id="pgfId-1153815"></a>Implementing a question-answering model using Hugging Face’s Transformer library</li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1153816"></a>In chapters 11 and 12, you learned about sequence-to-sequence models, a powerful family of models that allows us to map an arbitrary-length sequence to another arbitrary-length sequence. We exemplified this ability through a machine translation task. Sequence-to-sequence models consist of an encoder and a decoder. The encoder takes in the input sequence (a sentence in the source language) and creates a compact representation of that (known as the context vector). The decoder takes in the context vector to produce the final target (i.e., a sentence in the target language). But we saw how limiting the context vector makes the model and looked at two techniques to improve model performance. First, teacher forcing allows the decoder to see not only the context vector but also the previous words in the target at a given time. This provides the decoder much more information to produce the final prediction accurately. Second, the attention mechanism allows the decoder to peek into any part of the encoder history of outputs and use that information to produce the output as well. However, LSTM and GRU models are still quite restrictive as they can only see one output in the sequence at a given time and need to rely on a limited state vector (i.e., memory) to remember what they have seen.</p>

  <p class="body"><a class="calibre8" id="pgfId-1153818"></a>But there’s a new rival in town. If there’s one word that most recent state-of-the-art NLP and computer vision research have been brimming with, it’s Transformer. Transformer models are the latest type of deep learning models that made an unforgettable entrance by being crowned as state of the art for many NLP tasks, beating previous leaders such as LSTM- and GRU-based models. Inspired by their unprecedented success in NLP, they are now being introduced to solve various computer vision problems.</p>

  <p class="body"><a class="calibre8" id="pgfId-1153820"></a>By design, Transformer models make remembering or using information present in long sequences of data (e.g., a sequence of words) trivial. Unlike LSTM models, which have to look at one time step at a time, Transformer models can see the full sequence at once. This enables Transformer models to understand language better than other models. Furthermore, Transformer models enjoy high parallelizability due to the minimization of longitudinal (i.e., temporal) computations that require sequential processing of text.</p>

  <p class="body"><a class="calibre8" id="pgfId-1153822"></a>In this chapter, continuing our conversation from chapter 5, we will discuss some more details of the Transformer model so that our understanding of it is holistic. We will see how the Transformer model employs several embeddings to represent tokens as well as the position of those tokens in the sequence. Then we will learn BERT, a variant of the Transformer model that has been trained on a large corpus of text, ready to be used as a base layer to solve downstream NLP tasks easily without the need for complex models. BERT is essentially the encoder section of the Transformer model pretrained on large amounts of text using two techniques: masked language modeling (i.e., words in the sequence are randomly masked where BERT must predict the masked words) and next-sentence prediction (i.e., given two sentences, A and B, predict whether B entails A). We will see BERT in action when we use it to implement a spam classifier in TensorFlow. Next, Hugging Face’s <span class="fm-code-in-text">transformers</span> library<a class="calibre8" id="marker-1153823"></a> (<span class="fm-hyperlink"><a class="url" href="https://huggingface.co/transformers/">https://huggingface.co/transformers/</a></span>) is a popular choice for implementing state-of-the-art Transformer models with ease. It is one of the most valuable libraries to look at if you are planning to implement Transformer models in TensorFlow. For this reason, we will implement a question-answering model using the Transformers library by Hugging Face. Finally, we will end the chapter with a discussion about how Transformer models are used in computer vision.</p>

  <p class="body"><a class="calibre8" id="pgfId-1153831"></a>As the first step, let’s revisit what we have already learned about Transformer models (see figure 13.1) and expand our understanding a bit further.</p>

  <p class="fm-figure"><img alt="13-01" class="calibre10" src="../../OEBPS/Images/13-01.png" width="1075" height="689"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1180085"></a>Figure 13.1 How Transformer models are used to solve NLP problems</p>

  <h2 class="fm-head" id="sigil_toc_id_166"><a id="pgfId-1153832"></a>13.1 Transformers in more detail</h2>

  <p class="body"><a class="calibre8" id="pgfId-1153833"></a>You are working as a data scientist, and it’s been suggested that you use a Transformer model in your NLP workflow. You look at the Transformer models provided in TensorFlow. However, you are struggling to understand the model from the documentation. You think implementing a Transformer network from scratch would be a great way to grok the concepts that give life to the Transformer models. Therefore, you decide to implement a Transformer model with all the components specified in the paper “Attention Is All You Need” (<span class="fm-hyperlink"><a class="url" href="https://arxiv.org/pdf/1706.03762.pdf">https://arxiv.org/pdf/1706.03762.pdf</a></span>). This model will have components like embedding layers (token and positional embeddings), self-attention layers, normalization layers, and so on.</p>

  <h3 class="fm-head1" id="sigil_toc_id_167"><a id="pgfId-1153838"></a>13.1.1 Revisiting the basic components of the Transformer</h3>

  <p class="body"><a class="calibre8" id="pgfId-1153840"></a>In<a class="calibre8" id="marker-1153839"></a> chapter 5, we talked about the basics of the Transformer model to implement a simplified Transformer. Now let’s deepen our discussion and look at all the components that coexist in a Transformer model. The Transformer model is an encoder-decoder-based model. The encoder takes a sequence of inputs (e.g., Dogs are great) to create a hidden (or latent) representation of those tokens. Next, the decoder consumes the encoder’s generated representation of input tokens and generates an output (e.g., the French translation of the input).</p>

  <p class="body"><a class="calibre8" id="pgfId-1153843"></a>The encoder consists of a stack of layers, where each layer consists of two sublayers:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1153846"></a><i class="fm-italics">A self-attention layer</i><a class="calibre8" id="marker-1153844"></a>—Generates a latent representation for each input token in the sequence. For each input token, this layer looks at the entire input sequence and selects other tokens in the sequence that enrich the semantics of the generated hidden output for that token (i.e., <i class="fm-italics">attended representation</i>).</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1153849"></a><i class="fm-italics">A fully connected layer</i><a class="calibre8" id="marker-1153848"></a>—Generates an element-wise hidden representation of the attended representation.</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1153850"></a>The decoder consists of three sublayers:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1153853"></a><i class="fm-italics">A masked self-attention layer</i><a class="calibre8" id="marker-1153851"></a>—For each input token, it looks at all the tokens to the left of it. The decoder needs to mask words to the right to prevent the model from seeing words in the future, making the prediction task trivial for the decoder.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1153855"></a><i class="fm-italics">An encoder-decoder attention layer</i>—For each input token in the decoder, it looks at both the encoder’s outputs as well as the decoder’s masked attended output to generate a semantically rich hidden output</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1153856"></a><i class="fm-italics">A fully connected layer</i>—Generates an element-wise hidden representation of the attended representation of the decoder.</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1153858"></a>The hardest thing to navigate in our previous discussions was understanding the self-attention layer. Therefore, it’s worthwhile revisiting the computations transpiring in the self-attention layer. Computations in the self-attention layer revolve around three weight matrices:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1153866"></a>Query weight matrix (<i class="fm-timesitalic1">W</i><sub class="fm-subscript">q</sub>)</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1153873"></a>Key weight matrix (<i class="fm-timesitalic1">W</i><sub class="fm-subscript">k</sub>)</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1153880"></a>Value weight matrix (<i class="fm-timesitalic1">W</i><sub class="fm-subscript">v</sub>)</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1153888"></a>Each of these weight matrices produces three outputs for a given token (at position <i class="fm-timesitalic">i</i>) in a given input sequence: a query, a key, and a value, respectively. Let’s refresh our memory on what we said about these entities in chapter 5:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1153896"></a><i class="fm-timesitalic1">Query</i> (<i class="fm-italics">q</i><sub class="fm-subscript">i</sub>)—Helps to build a probability matrix that is eventually used for indexing values (<i class="fm-timesitalic1">v</i>). Query affects the rows of the matrix and represents the index of the current word that’s being processed.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1153903"></a><i class="fm-italics">Key</i> (<i class="fm-timesitalic1">k</i><sub class="fm-subscript">i</sub>)—Helps to build a probability matrix that is eventually used for indexing values (<i class="fm-timesitalic1">v</i>). Key affects the columns of the matrix and represents the candidate words that need to be mixed depending on the query word.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1153911"></a><i class="fm-italics">Value</i> (<i class="fm-timesitalic1">v</i><sub class="fm-subscript">i</sub>)—Hidden (i.e., latent) representation of the inputs that are used to compute the final output by indexing from the probability matrix created using query and key. As explained earlier, the final output at position <i class="fm-timesitalic1">i</i> is generated not only by using the <i class="fm-timesitalic1">i</i><sup class="fm-superscript">th</sup> token, but also by using other tokens in the input sequence, which enhances the semantics captured in the final representation.</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1153924"></a>The high-level purpose of these elements is to generate an attended representation (i.e., a latent or hidden representation for a given token that is enriched by the information in other tokens of the input sequence) of a given input token. To do that, the model</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1153925"></a>Generates a query for each position in the input sequence</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1153926"></a>For each query, determines how much each key should contribute (the key also represents individual tokens)</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1153927"></a>Based on the contributions of the keys for a given query, mixes the values corresponding to those keys to generate the final attended representation</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1153928"></a>All three of the query, key, and values are generated by multiplying a trainable weight matrix with an input token’s numerical representation. All this needs to happen in a differentiable way to ensure the gradients can be backpropagated through the model. The paper proposes the following computation to compute the final representation of the self-attention layer for the input tokens:</p>

  <p class="fm-equation"><img alt="13_01a" class="calibre10" src="../../OEBPS/Images/13_01a.png" width="257" height="85"/><br class="calibre2"/>
  <a id="pgfId-1156857"></a></p>

  <p class="body"><a class="calibre8" id="pgfId-1153934"></a>Here, Q represents the queries, K represents the keys, and V represents values for all the inputs and all the tokens in each input in a batch of data. This is what makes Transformer models so powerful: unlike LSTM models, Transformer models aggregate looking at all tokens in a sequence to a single matrix multiplication, making these models highly<a class="calibre8" id="marker-1153936"></a> parallelizable.</p>

  <h3 class="fm-head1" id="sigil_toc_id_168"><a id="pgfId-1153937"></a>13.1.2 Embeddings in the Transformer</h3>

  <p class="body"><a class="calibre8" id="pgfId-1153941"></a>One<a class="calibre8" id="marker-1153938"></a> thing we overlooked when discussing the Transformer model is the embeddings used in it. We briefly touched on the word embeddings used. Let’s discuss this topic in more detail here. Word embeddings provide a semantic-preserving representation of words based on the context in which words are used. In other words, if two words are used in the same context, they will have similar word vectors. For example, the words “cat” and “dog” will have similar representations, whereas “cat” and “volcano” will have vastly different representations.</p>

  <p class="body"><a class="calibre8" id="pgfId-1153942"></a>Word vectors were initially introduced in the paper titled “Efficient Estimation of Word Representations in Vector Space” by Mikolov et al. (<span class="fm-hyperlink"><a class="url" href="https://arxiv.org/pdf/1301.3781.pdf">https://arxiv.org/pdf/1301.3781.pdf</a></span>). It came in two variants, skip-gram and continuous bag-of-words<a class="calibre8" id="marker-1162517"></a><a class="calibre8" id="marker-1162518"></a> (CBOW). Because skip-gram is somewhat more widely accepted than CBOW, let’s discuss the crux of the skip-gram algorithm.</p>

  <p class="body"><a class="calibre8" id="pgfId-1153947"></a>The first step is to define a large matrix of size <i class="fm-timesitalic">V</i> × <i class="fm-timesitalic">E</i>, where <i class="fm-timesitalic">V</i> is the size of the vocabulary and <i class="fm-timesitalic">E</i> is the size of the embeddings. The size of the embeddings (E) is a user-defined hyperparameter, where a larger E typically leads to more powerful word embeddings. In practice, you do not need to increase the size of embeddings beyond 300.</p>

  <p class="body"><a class="calibre8" id="pgfId-1153948"></a>Next, you create inputs and targets in a completely unsupervised manner. Given a large corpus of text, you select a word form as the input (probe word) and the words surrounding the probe word as targets. The surrounding words are captured by defining a fixed-sized window around the probe word. For example, for a window size of 2 (on each side of the probe word), you can generate the following input-target pairs from the sentence “<span class="fm-code-in-text">angry John threw a pizza at me</span>.”</p>
  <pre class="programlisting">(John, angry), (John, threw), (John, a), (threw, angry), (threw, John), (threw, a), (threw, pizza), ..., (at, a), (at, pizza), (at, me)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1153951"></a>With the labeled data, you can frame the problem of learning word embeddings as a classification problem. In other words, you train a model (i.e., a function of the word-embedding matrix) to predict the target word, given the input word. The model consists of two components, the embedding matrix and a fully connected layer with a softmax activation to output the predictions. Once the embeddings are learned, you can discard the other things around it (e.g., the fully connected layer) and use the embedding matrix for a downstream NLP task such as sentiment analysis, machine translation, and so on. You just have to look up the embedding vector corresponding to a word in order to obtain a numerical representation for this word.</p>

  <p class="body"><a class="calibre8" id="pgfId-1153953"></a>Motivated by the original word vector algorithms, modern deep learning models combine learning word embeddings and the actual decision-support NLP problem to a single model training task. In other words, the following general approach is taken to incorporate word embeddings to a machine learning model:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1153954"></a>Define a randomly initialized word-embedding matrix (or pretrained embeddings available to download for free).</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1153955"></a>Define the model (randomly initialized) that uses word embeddings as the inputs and produces an output (e.g., sentiment, a language translation, etc.).</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1153956"></a>Train the whole model (embeddings + the model) end to end on the task.</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1153957"></a>The same technique is used in Transformer models. However, in Transformer models, there are two different embeddings:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1153958"></a>Token embeddings (which provide a unique representation for each token seen by the model in an input sequence)</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1153959"></a>Positional embeddings (which provide a unique representation for each position in the input sequence)</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1153960"></a>The token embeddings have a unique embedding vector for each token (e.g., character, word, sub-word), depending on the model’s tokenizing mechanism.</p>

  <p class="body"><a class="calibre8" id="pgfId-1153961"></a>The positional embeddings are used to signal the model where a token is appearing. The primary purpose is for the positional embeddings server to tell the Transformer model where a word is appearing. This is because, unlike LSTMs/GRUs, Transformer models don’t have a notion of sequence, as they process the whole text at once. Furthermore, a change to the position in word can alter the meaning of a sentence or a word. For example, in the two versions of</p>
  <pre class="programlisting">Ralph loves his tennis ball. <b class="fm-code-bold">It</b> likes to chase the ball
Ralph loves his tennis ball. Ralph likes to chase <b class="fm-code-bold">it</b></pre>

  <p class="body"><a class="calibre8" id="pgfId-1153965"></a>the word “it” refers to different things, and the position of the word “it” can be used as a cue to identify this difference. The original Transformer paper uses the following equations to generate positional embeddings:</p>

  <p class="fm-equation"><a id="pgfId-1159206"></a><i class="fm-italics">PE</i>(<i class="fm-italics">pos</i>,2<i class="fm-italics">i</i>) = sin(<i class="fm-italics">pos</i>/10000<sup class="fm-superscript">21/d<sub class="fm-subscript2">model</sub></sup>)</p>

  <p class="fm-equation"><a id="pgfId-1159225"></a><i class="fm-italics">PE</i>(<i class="fm-italics">pos</i>,2<i class="fm-italics">i +</i> 1) = cos(<i class="fm-italics">pos</i>/10000<sup class="fm-superscript">21/d<sub class="fm-subscript2">model</sub></sup>)</p>

  <p class="body"><a class="calibre8" id="pgfId-1153986"></a>where pos denotes the position in the sequence and <i class="fm-timesitalic">i</i> denotes the <i class="fm-timesitalic">i</i><sup class="fm-superscript">th</sup> feature dimension (0 ≤ <i class="fm-timesitalic">i</i> &lt; <i class="fm-timesitalic">d</i><sub class="fm-subscript">model</sub>). Even-numbered features use a sine function, whereas odd-numbered features use a cosine function. Figure 13.2 presents how positional embeddings change as the time step and the feature position change. It can be seen that feature positions with higher indices have lower frequency sinusoidal waves. It is not entirely clear how the authors came up with the exact equation. However, they do mention that they did not see a significant performance difference between the previous equation and letting the model learn positional embeddings jointly during the training.</p>

  <p class="fm-figure"><img alt="13-02" class="calibre10" src="../../OEBPS/Images/13-02.png" width="1033" height="781"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1180126"></a>Figure 13.2 How positional embeddings change with the time step and the feature position. Even-numbered feature positions use the sine function, whereas odd-numbered positions use the cosine function. Additionally, the frequency of the signals decreases as the feature position increases.</p>

  <p class="body"><a class="calibre8" id="pgfId-1154000"></a>It is important to note that both token and positional embeddings will have the same dimensionality (i.e., <i class="fm-timesitalic">d</i><sub class="fm-subscript">model</sub>). Finally, as the input to the model, the token embeddings and the positional embeddings are summed to form a single hybrid embedding vector<a class="calibre8" id="marker-1154007"></a> (figure 13.3).</p>

  <p class="fm-figure"><img alt="13-03" class="calibre10" src="../../OEBPS/Images/13-03.png" width="961" height="517"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1180160"></a>Figure 13.3 The embeddings generated in a Transformer model and how the final embeddings are computed</p>

  <h3 class="fm-head1" id="sigil_toc_id_169"><a id="pgfId-1154015"></a>13.1.3 Residuals and normalization</h3>

  <p class="body"><a class="calibre8" id="pgfId-1154019"></a>Another<a class="calibre8" id="marker-1154016"></a><a class="calibre8" id="marker-1154017"></a><a class="calibre8" id="marker-1154018"></a> important characteristic of the Transformer models is the existence of the residual connections and the normalization layers in between the individual layers. We discussed residual connections in depth in chapter 7 when we discussed advance techniques for image classification. Let’s briefly revisit the mechanics and the motivation for residual connections.</p>

  <p class="body"><a class="calibre8" id="pgfId-1154020"></a>Residual connections are formed by adding a given layer’s output to the output of one or more layers ahead. This, in turn, forms “shortcut connections” through the model and provides a stronger gradient flow by reducing the changes of the phenomenon known as the <i class="fm-italics">vanishing gradients</i> (figure 13.4). Vanishing gradients cause the gradients in the layers closest to the inputs to be very small so that the training in those layers is hindered.</p>

  <p class="fm-figure"><img alt="13-04" class="calibre10" src="../../OEBPS/Images/13-04.png" width="642" height="511"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1180194"></a>Figure 13.4 Mathematical view of residual connections</p>

  <p class="body"><a class="calibre8" id="pgfId-1154027"></a>In Transformer models, in each layer, residual connections are created in the following way:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1154028"></a>The input to the multi-head self-attention sublayer is added to the output of the multi-head self-attention sublayer.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1154029"></a>The input to the fully connected sublayer is added to the output of the fully connected sublayer.</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1154030"></a>Next, the output reinforced by residual connections goes through a layer normalization layer. <i class="fm-italics">Layer normalization</i>, similar to batch normalization, is a way to reduce the “covariate shift” in neural networks, allowing them to be trained faster and achieve better performance. Covariate shift refers to changes in the distribution of neural network activations (caused by changes in the data distribution) that transpire as the model goes through model training. Such changes in the distribution hurt consistency during model training and negatively impact the model. Layer normalization was introduced in the paper “Layer Normalization” by Ba et al. (<span class="fm-hyperlink"><a class="url" href="https://arxiv.org/pdf/1607.06450.pdf">https://arxiv.org/pdf/1607.06450.pdf</a></span>).</p>

  <p class="body"><a class="calibre8" id="pgfId-1154032"></a>Batch normalization computes the mean and variance of activations as an average over the samples in the batch, causing its performance to rely on mini batch sizes, which are used to train the model.</p>

  <p class="body"><a class="calibre8" id="pgfId-1154033"></a>However, layer normalization computes the mean and variance (i.e., the normalization terms) of the activations in such a way that the normalization terms are the same for every hidden unit. In other words, layer normalization has a single mean and a variance value for all the hidden units in a layer. This is in contrast to batch normalization, which maintains individual mean and variance values for each hidden unit in a layer. Moreover, unlike batch normalization, layer normalization does not average over the samples in the batch, but rather leaves the averaging out and has different normalization terms for different inputs. By having a mean and variance per sample, layer normalization gets rid of the dependency on the mini batch size. For more details about this method, please refer to the original paper.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1154034"></a>Layer normalization in TensorFlow/Keras</p>

    <p class="fm-sidebar-text"><a id="pgfId-1154036"></a>TensorFlow provides a convenient implementation of the layer normalization algorithm at <span class="fm-hyperlink"><a class="url" href="http://mng.bz/YGRB">http://mng.bz/YGRB</a></span>. You can use this layer with any model you define using TensorFlow Keras APIs.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1154038"></a>Figure 13.5 depicts how residual connections and layer normalization are used in Transformer models.</p>

  <p class="fm-figure"><img alt="13-05" class="calibre10" src="../../OEBPS/Images/13-05.png" width="1092" height="739"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1180228"></a>Figure 13.5 How residual connections and layer normalization layers are used in the Transformer model</p>

  <p class="body"><a class="calibre8" id="pgfId-1154045"></a>With that, we end the discussion about the components in the Transformer model. We have discussed all the bells and whistles of the Transformer model, namely self-attention layers, fully connected layers, embeddings (token and positional), layer normalization, and residual connections. In the next section, we will discuss how we can use a pretrained Transformer model known as BERT to solve a spam classification task.</p>

  <p class="fm-head2"><a id="pgfId-1154046"></a>Exercise 1</p>

  <p class="body"><a class="calibre8" id="pgfId-1154047"></a>You are given the following code for the Transformer encoder</p>
  <pre class="programlisting">import tensorflow as tf
 
# Defining some hyperparameters
n_steps = 25 # Sequence length
n_en_vocab = 300 # Encoder's vocabulary size
n_heads = 8 # Number of attention heads
d = 512 # The feature dimensionality of each layer
 
# Encoder input layer
en_inp = tf.keras.layers.Input(shape=(n_steps,))
# Encoder input embedddings
en_emb = tf.keras.layers.Embedding(
    n_en_vocab, d, input_length=n_steps
)(en_inp)
 
# Two encoder layers
en_out1 = EncoderLayer(d, n_heads)(en_emb)
en_out2 = EncoderLayer(d, n_heads)(en_out1)
 
model = tf.keras.models.Model(inputs=en_inp, output=en_out2)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154068"></a>where the <span class="fm-code-in-text">EncoderLayer</span> defines a typical Transformer encoder layer that has a self-attention sublayer and a fully connected sublayer. You are asked to integrate positional encoding using the equation</p>

  <p class="fm-equation"><a id="pgfId-1160738"></a><i class="fm-italics">PE</i>(<i class="fm-italics">pos</i>, 2<i class="fm-italics">i</i>) = sin(<i class="fm-italics">pos</i>/10000<sup class="fm-superscript">2<i class="fm-italics">i</i>/d<sub class="fm-subscript2">model</sub></sup>)</p>

  <p class="body"><a class="calibre8" id="pgfId-1154085"></a><i class="fm-italics">pos</i> goes from 0 to 511 (<span class="fm-code-in-text">d=512</span> features) and <i class="fm-italics">i</i> goes from 0 to 24 (<span class="fm-code-in-text">n_steps=25</span> time steps) and denotes the time step. In other words, our positional encoding will be a tensor of shape [n_steps, d]. You can use <span class="fm-code-in-text">tf.math.sin()</span> to generate the sin value element-wise for a tensor. You can define the positional embeddings as a tensor and not the product of a <span class="fm-code-in-text">tf.keras.layers.Layer</span>. The final embedding should be generated by summing the token embeddings and the positional embeddings. How would you<a class="calibre8" id="marker-1154086"></a><a class="calibre8" id="marker-1154087"></a><a class="calibre8" id="marker-1154088"></a> do that?</p>

  <h2 class="fm-head" id="sigil_toc_id_170"><a id="pgfId-1154089"></a>13.2 Using pretrained BERT for spam classification</h2>

  <p class="body"><a class="calibre8" id="pgfId-1154093"></a>You<a class="calibre8" id="marker-1154090"></a><a class="calibre8" id="marker-1154091"></a><a class="calibre8" id="marker-1154092"></a> are working as a data scientist for a mail service company, and the company is dying to implement a spam classification functionality. They want to implement this functionality in house and save dollars. Having read about BERT and how powerful it is for solving NLP tasks, you explain to the team that all you need to do is download the BERT model, fit a classification layer on top of BERT, and train the whole model end to end on labeled data. The labeled data consists of a spam message and a label indicating whether the message is spam or ham (i.e., not spam). You have been put in charge of implementing this model.</p>

  <p class="body"><a class="calibre8" id="pgfId-1154094"></a>Now that we have discussed all the moving elements of the Transformer architecture, it puts us in a very strong position to understand BERT. BERT is a Transformer-based model introduced in the paper “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding” by Devlin et al. (<span class="fm-hyperlink"><a class="url" href="https://arxiv.org/pdf/1810.04805.pdf">https://arxiv.org/pdf/1810.04805.pdf</a></span>), and it represents a very important milestone in the history of NLP as it’s a pioneering model that proved the ability to apply “transfer learning” in the domain of NLP.</p>

  <p class="body"><a class="calibre8" id="pgfId-1154095"></a>BERT is a Transformer model that is pretrained on large amounts of textual data in an unsupervised fashion. Therefore, you can use BERT as the basis to get rich, semantically sound numerical representations for textual input sequences that can be readily fed to your downstream NLP models. Because of the rich textual representations provided by BERT, you’re relieving your decision support model of the need to understand the language and instead can directly focus on the problem at hand. From a technical perspective, if you’re solving a classification problem with BERT, all you need to do is</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1154096"></a>Fit a classifier(s) (e.g., a logistic regression layer) on top of BERT, which takes BERT’s output(s) as the input(s)</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1154097"></a>Train the model (i.e., BERT + classifier(s)) end to end on the discriminative task</p>
    </li>
  </ul>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1154098"></a>History of BERT</p>

    <p class="fm-sidebar-text"><a id="pgfId-1154099"></a>Before models like BERT, solving natural language processing (NLP) tasks was both repetitive and time-consuming. Every time, you had to train a model from scratch. And to exacerbate this suboptimal way of tackling problems, most models struggled with long sequences of text, limiting their ability to understand language.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1154100"></a>In 2017, Transformer models for NLP tasks were proposed in the paper “Attention Is All You Need” (<span class="fm-hyperlink"><a class="url" href="https://arxiv.org/pdf/1706.03762.pdf">https://arxiv.org/pdf/1706.03762.pdf</a></span>). Transformer models beat previous dominants like LSTMs and GRUs across the board on a collection of NLP tasks. Transformer models, unlike recurrent models that look at one word at a time and maintain a state (i.e., memory), look at the whole sequence at once.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1154101"></a>Then, in 2018, the “ImageNet moment” arrived for NLP (i.e., transfer learning in NLP). The ImageNet moment refers to the moment when ML practitioners realized that using a computer vision model that is already trained on the large ImageNet image classification data set on other tasks (e.g., object detection, image segmentation) yields better performance faster. This essentially gave rise to the concept of transfer learning that is heavily used in the computer vision domain. So, until 2018, the NLP domain did not have a very good way to employ transfer learning to uplift the performance on tasks. The paper “Universal Language Model Fine-Tuning for Text Classification” (<span class="fm-hyperlink"><a class="url" href="https://arxiv.org/pdf/1801.06146.pdf">https://arxiv.org/pdf/1801.06146.pdf</a></span>) introduced the idea of using pretraining on a language modeling task and then training the model on a discriminative task (e.g., a classification problem). The advantage of this approach was that you did not need as many samples as if you were to train the model from scratch.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1154102"></a>In 2018, BERT was introduced. It was a marriage of two of the finest moments in the history of NLP. In other words, BERT is a Transformer model that is pretrained on large amounts of textual data in an unsupervised fashion.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1154104"></a>We will now examine the BERT model in more detail.</p>

  <h3 class="fm-head1" id="sigil_toc_id_171"><a id="pgfId-1154105"></a>13.2.1 Understanding BERT</h3>

  <p class="body"><a class="calibre8" id="pgfId-1154109"></a>Let’s<a class="calibre8" id="marker-1154106"></a><a class="calibre8" id="marker-1154107"></a><a class="calibre8" id="marker-1154108"></a> now inspect BERT more microscopically. As I alluded to earlier, BERT is a Transformer model. To be exact, it’s the encoder part of the Transformer model. This means that BERT takes an input sequence (a collection of tokens) and produces an encoded output sequence. Figure 13.6 depicts the high-level architecture of BERT.</p>

  <p class="fm-figure"><img alt="13-06" class="calibre10" src="../../OEBPS/Images/13-06.png" width="1008" height="883"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1180262"></a>Figure 13.6 The high-level architecture of BERT. It takes a set of input tokens and produces a sequence of hidden representations generated using several hidden layers.</p>

  <p class="body"><a class="calibre8" id="pgfId-1154116"></a>When BERT takes an input, it inserts some special tokens into the input. First, at the beginning, it inserts a [CLS] (an abbreviated form of the term classification) token that is used to generate the final hidden representation for certain types of tasks (e.g., sequence classification). It represents the output after attending to all the tokens in the sequence. Next, it also inserts an [SEP] (i.e., “separation”) token depending on the type of input. The [SEP] token marks the end and beginning of different sequences in the input. For example, in question answering, the model takes a question and a context (e.g., paragraph) that may have the answer as an input, and [SEP] is used in between the question and the context.</p>

  <p class="body"><a class="calibre8" id="pgfId-1154117"></a>Next, the final embedding of the tokens is generated using three different embedding spaces. The token embedding has a unique vector for each token in the vocabulary. The positional embeddings encode the position of each token, as discussed earlier. Finally, the segment embedding provides a distinct representation for each subcomponent in the input when the input consists of multiple components. For example, in question answering, the question will have a unique vector as its segment embedding vector, whereas the context will have a different embedding vector. This is done by having <i class="fm-italics">n</i> different embedding vectors for the <i class="fm-italics">n</i> different components in the input sequence. Depending on the component index specified for each token in the input, the corresponding segment-embedding vector is retrieved. <i class="fm-italics">n</i> needs to be specified in advance<a class="calibre8" id="marker-1174800"></a><a class="calibre8" id="marker-1174789"></a>.</p>

  <p class="body"><a class="calibre8" id="pgfId-1154124"></a>The real value of BERT comes from the fact that it has been pretrained on a large corpus of data in a self-supervised fashion. In the pretraining stage, BERT is trained on two different tasks:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1154126"></a>Masked language modeling<a class="calibre8" id="marker-1154125"></a> (MLM)</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1154127"></a>Next-sentence prediction (NSP<a class="calibre8" id="marker-1174811"></a>)</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1154128"></a>The masked language modeling (MLM) task is inspired by the <i class="fm-italics">Cloze task</i> or the <i class="fm-italics">Cloze test</i>, where a student is given a sentence with one or more blanks and is asked to fill the blanks. Similarly, given a text corpus, words are masked from sentences and then the model is asked to predict the masked tokens. For example, the sentence</p>
  <pre class="programlisting">I went to the bakery to buy bread</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154132"></a>might become</p>
  <pre class="programlisting">I went to the [MASK] to buy bread</pre>

  <p class="fm-callout"><a id="pgfId-1154134"></a><span class="fm-callout-head">NOTE</span> There has been a plethora of Transformer-based models, each building on the previous. You can read more about these models in appendix C.</p>

  <p class="body"><a class="calibre8" id="pgfId-1154135"></a>Figure 13.7 illustrates the main components of BERT during the training of the masked language modeling task. BERT uses a special token ([MASK]) to represent masked words. Then the target for the model will be the word “bakery.” But this introduces a practical issue to the model. The special token [MASK] does not appear in the actual text. This means that the text the model will see during the fine-tuning phase (i.e., when training on a classification problem) will be different than what it will see during pretraining. This is sometimes referred to as the <i class="fm-italics">pretraining-fine-tuning discrepancy</i>. Therefore, the authors of BERT suggest the following approach to cope with the issue. When masking a word, do one of the following:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1154136"></a>Use the [MASK] token as it is (with 80% probability).</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1154137"></a>Use a random word (with 10% probability).</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1154138"></a>Use the true word (with 10% probability).</p>
    </li>
  </ul>

  <p class="fm-figure"><img alt="13-07" class="calibre10" src="../../OEBPS/Images/13-07.png" width="1064" height="1144"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1180296"></a>Figure 13.7 The methodology used for pretraining BERT. BERT is pretrained on two tasks: a masked language modeling task and the next sentence prediction task. In the masked language modeling task, the tokens in the input are masked and the model is asked to predict masked tokens. In the next sentence prediction task, the model is asked to predict if given two sentences are next to each other.</p>

  <p class="body"><a class="calibre8" id="pgfId-1154139"></a>Next, in the next sentence prediction task, the model is given a pair of sentences, A and B (in that order), and is asked to predict whether B is the next sentence after A. This can be done by fitting a binary classifier on top of BERT and training the whole model end to end on selected pairs of sentences. Generating pairs of sentences as inputs to the model is not hard and can be done in an unsupervised manner:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1154140"></a>A sample with the label TRUE is generated by picking two sentences that are adjacent to each other.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1154141"></a>A sample with the label FALSE is generated by picking two sentences randomly that are not adjacent to each other.</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1154142"></a>Following this approach, a labeled data set is generated for the next sentence prediction task. Then BERT, along with the binary classifier, is trained end to end using the labeled data set. Figure 13.7 highlights the data and the model architecture in the next sentence prediction task.</p>

  <p class="body"><a class="calibre8" id="pgfId-1154149"></a>You might have noticed in figure 13.6 that the input to BERT has special tokens. There are two special tokens (in addition to the [MASK] token we already discussed) that serve special purposes.</p>

  <p class="body"><a class="calibre8" id="pgfId-1154150"></a>The [CLS] token is appended to any input sequence fed to BERT. This denotes the beginning of the input. It also forms the basis for the input fed into the classification head used on top of BERT to solve your NLP task. As you know, BERT produces a hidden representation for each input token in the sequence. As a convention, the hidden representation corresponding to the [CLS] token is used as the input to the classification model that sits on top of BERT.</p>

  <p class="body"><a class="calibre8" id="pgfId-1154151"></a>The task-specific NLP tasks solved by BERT can be classified into four different categories. These are based on the tasks found in the General Language Understanding Evaluation<a class="calibre8" id="marker-1154152"></a> (GLUE) benchmark task suite (<span class="fm-hyperlink"><a class="url" href="https://gluebenchmark.com">https://gluebenchmark.com</a></span>):</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1154153"></a><i class="fm-italics">Sequence classification</i>—Here, a single input sequence is given and the model is asked to predict a label for the whole sequence (e.g., sentiment analysis, spam identification).</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1154154"></a><i class="fm-italics">Token classification</i>—Here, a single input sequence is given and the model is asked to predict a label for each token in the sequence (e.g., named entity recognition, part-of-speech tagging).</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1154155"></a><i class="fm-italics">Question answering</i>—Here, the input consists of two sequences: a question and a context. The question and the context are separated by an [SEP] token. The model is trained to predict the starting and ending indices of the span of tokens belonging to the answer.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1154156"></a><i class="fm-italics">Multiple choice</i>—Here the input consists of multiple sequences: a question followed by multiple candidates that may or may not be the answer to the question. These multiple sequences are separated by the token [SEP] and provided as a single input sequence to the model. The model is trained to predict the correct answer (i.e., the class label) for that question.</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1154157"></a>BERT is designed in such a way that it can be used to solve these tasks without any modifications to the base model. In tasks that involve multiple sequences (e.g., question answering, multiple-choice questions), you need to tell the model different inputs separately (e.g., which tokens are the question and which tokens are the context in the question-answering task). In order to make that distinction, the [SEP] token is used. An [SEP] token is inserted between the different sequences. For example, if you are solving a question-answering task, you might have an input as follows:</p>
  <pre class="programlisting">Question: What color is the ball?
Paragraph: Tippy is a dog. She loves to play with her red ball.</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154161"></a>Then the input to BERT might look like</p>
  <pre class="programlisting">[CLS] What color is the ball [SEP] Tippy is a dog She loves to play with her red ball [SEP]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154164"></a>BERT also uses a segment-embedding space to denote which sequence a token belongs to. For example, inputs having only one sequence have the same segment-embedding vector for all the tokens (e.g., spam classification task). Inputs having two or more sequences use the first or second space depending on which sequence the token belongs to. For example, in question answering, the model will use a unique segment-embedding vector to encode tokens of the question, where it will use a different segment-embedding vector to encode the tokens of the context. Now we have discussed all the elements of BERT needed to use it successfully to solve a downstream NLP task. Let’s reiterate the key points about BERT:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1154165"></a>BERT is an encoder-based Transformer that is pretrained on large amounts of text.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1154166"></a>BERT uses masked language modeling and next-sentence prediction tasks to pretrain the model.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1154167"></a>BERT outputs a hidden representation for every token in the input sequence.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1154168"></a>BERT has three embedding spaces: token embedding, positional embedding, and segment embedding.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1154169"></a>BERT uses a special token [CLS] to denote the beginning of an input and is used as the input to a downstream classification model.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1154170"></a>BERT is designed to solve four types of NLP tasks: sequence classification, token classification, free-text question answering, and multiple-choice question answering.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1154171"></a>BERT uses the special token [SEP] to separate sequence A and sequence B.</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1154175"></a>Next, we will learn how we can classify spam messages with<a class="calibre8" id="marker-1154172"></a><a class="calibre8" id="marker-1154173"></a><a class="calibre8" id="marker-1154174"></a> BERT.</p>

  <h3 class="fm-head1" id="sigil_toc_id_172"><a id="pgfId-1154176"></a>13.2.2 Classifying spam with BERT in TensorFlow</h3>

  <p class="body"><a class="calibre8" id="pgfId-1154181"></a>It’s<a class="calibre8" id="marker-1154177"></a><a class="calibre8" id="marker-1154178"></a><a class="calibre8" id="marker-1154179"></a><a class="calibre8" id="marker-1154180"></a> now time to show off your skills by implementing a spam classifier with minimal effort. First, let’s download data. The data we will use for this exercise is a collection of spam and ham (non-spam) SMS messages available at <span class="fm-hyperlink"><a class="url" href="http://mng.bz/GE9v">http://mng.bz/GE9v</a></span>. The Python code for downloading the data has been provided in the <span class="fm-code-in-text">Ch13-Transormers-with-TF2-and-Huggingface/13.1_Spam_Classification_with_BERT.ipynb</span> notebook.</p>

  <p class="fm-head2"><a id="pgfId-1154182"></a>Understanding the data</p>

  <p class="body"><a class="calibre8" id="pgfId-1154186"></a>Once<a class="calibre8" id="marker-1154184"></a><a class="calibre8" id="marker-1154185"></a> you download the data and extract it, we can quickly look at what’s in the data. It will be a single tab-separated text file. The first three entries of the file are as follows:</p>
  <pre class="programlisting">ham    Go until jurong point, crazy.. Available only in bugis n great 
<span class="fm-code-continuation-arrow">➥</span> world la e buffet... Cine there got amore wat...
ham    Ok lar... Joking wif u oni...
spam        Free entry in 2 a wkly comp to win FA Cup final tkts 21st 
<span class="fm-code-continuation-arrow">➥</span> May 2005 ...</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154191"></a>As shown, each line starts with the word <span class="fm-code-in-text">ham</span> or <span class="fm-code-in-text">spam</span>, indicating whether it’s safe or spam. Then the text in the message is given, followed by a tab. Our next task is to load this data into memory and store the inputs and labels in NumPy arrays. The following listing shows the steps to do so.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1154193"></a>Listing 13.1 Loading the data from the text file into NumPy arrays</p>
  <pre class="programlisting">inputs = []                                 <span class="fm-combinumeral">❶</span>
labels = []                                 <span class="fm-combinumeral">❷</span>
 
n_ham, n_spam = 0,0                         <span class="fm-combinumeral">❸</span>
with open(os.path.join('data', 'SMSSpamCollection'), 'r') as f:
    for r in f:                             <span class="fm-combinumeral">❹</span>
 
        if r.startswith('ham'):             <span class="fm-combinumeral">❺</span>
            label = 0                       <span class="fm-combinumeral">❻</span>
            txt = r[4:]                     <span class="fm-combinumeral">❼</span>
            n_ham += 1                      <span class="fm-combinumeral">❽</span>
        # Spam input
        elif r.startswith('spam'):          <span class="fm-combinumeral">❾</span>
            label = 1                       <span class="fm-combinumeral">❿</span>
            txt = r[5:]                     <span class="fm-combinumeral">⓫</span>
            n_spam += 1
        inputs.append(txt)                  <span class="fm-combinumeral">⓬</span>
        labels.append(label)                <span class="fm-combinumeral">⓭</span>
        
# Convert them to arrays
inputs = np.array(inputs).reshape(-1,1)     <span class="fm-combinumeral">⓮</span>
labels = np.array(labels)                   <span class="fm-combinumeral">⓯</span></pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1178245"></a><span class="fm-combinumeral">❶</span> Inputs (messages) are stored here.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1178266"></a><span class="fm-combinumeral">❷</span> The label (0/1) is stored here.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1178283"></a><span class="fm-combinumeral">❸</span> Counts the total number of ham/spam examples</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1178303"></a><span class="fm-combinumeral">❹</span> Read every row in the file.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1178320"></a><span class="fm-combinumeral">❺</span> If the line starts with ham, it’s a ham example.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1178337"></a><span class="fm-combinumeral">❻</span> Assign it a label of 0.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1178354"></a><span class="fm-combinumeral">❼</span> Input is the text in the line (except for the word starting ham).</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1178371"></a><span class="fm-combinumeral">❽</span> Increase the count n_ham.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1178388"></a><span class="fm-combinumeral">❾</span> If the line starts with spam, it’s spam.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1178405"></a><span class="fm-combinumeral">❿</span> Assign it a label of 1.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1178422"></a><span class="fm-combinumeral">⓫</span> Input is the text in the line (except for the word that starts spam).</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1178439"></a><span class="fm-combinumeral">⓬</span> Append the input text to inputs.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1178456"></a><span class="fm-combinumeral">⓭</span> Append the labels to labels.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1178473"></a><span class="fm-combinumeral">⓮</span> Convert inputs to a NumPy array (and reshape it to a matrix with one column).</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1178490"></a><span class="fm-combinumeral">⓯</span> Convert the labels list to a NumPy array.</p>

  <p class="body"><a class="calibre8" id="pgfId-1154232"></a>You can print the <span class="fm-code-in-text">n_ham</span> and <span class="fm-code-in-text">n_spam</span> variables and verify that there are 4,827 ham examples and 747 spam examples. In other words, there are fewer spam examples than ham examples. Therefore, we have to make sure to account for this imbalance when training the<a class="calibre8" id="marker-1154234"></a><a class="calibre8" id="marker-1154235"></a> model.</p>

  <p class="fm-head2"><a id="pgfId-1154236"></a>Treating class imbalance in the data</p>

  <p class="body"><a class="calibre8" id="pgfId-1154241"></a>To<a class="calibre8" id="marker-1154237"></a><a class="calibre8" id="marker-1154239"></a><a class="calibre8" id="marker-1154240"></a> counteract class imbalance, let’s create balanced training/validation and testing data sets. To do that, we will use the <span class="fm-code-in-text">imbalanced-learn</span> library<a class="calibre8" id="marker-1154242"></a>, which is a great library for manipulating imbalanced data sets (e.g., sampling various amounts of data from different classes). There are two primary strategies for restoring balance in a data set:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1154243"></a>Undersampling the majority class (fewer samples from that class are selected for the final data set)</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1154244"></a>Oversampling the minority class (more samples from that class are generated for the final data set)</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1154245"></a>We will use the first strategy here (i.e., undersampling the majority class). More specifically, we will first</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1154246"></a>Create balanced testing and validation data sets by randomly sampling data from the data set (n examples per each class)</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1154247"></a>Assign the rest of the data to a training set and undersample the majority class in the training set using an algorithm known as the <i class="fm-italics">near-miss</i> algorithm</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1154248"></a>The creation of the training validation and testing data is illustrated in figure 13.8.</p>

  <p class="fm-figure"><img alt="13-08" class="calibre10" src="../../OEBPS/Images/13-08.png" width="975" height="836"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1180330"></a>Figure 13.8 How the training, validation, and testing data sets are created from the original data set</p>

  <p class="body"><a class="calibre8" id="pgfId-1154255"></a>First, let’s import a few under-samplers from the library and the NumPy library:</p>
  <pre class="programlisting">from imblearn.under_sampling import  NearMiss, RandomUnderSampler
import numpy as np</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154258"></a>Next, we will define a variable, <span class="fm-code-in-text">n</span>, which denotes how many examples per class we will keep in the validation and testing data sets:</p>
  <pre class="programlisting">n=100 # Number of instances for each class for test/validation sets
random_seed = 4321</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154261"></a>Then we will define a random under-sampler. The most important parameter is the <span class="fm-code-in-text">sampling_strategy</span> parameter, which takes a dictionary with the label as the key and the number of samples required for that label as the value. We will also pass <span class="fm-code-in-text">random_seed</span> to the <span class="fm-code-in-text">random_state</span> argument to make sure we get the same result every time we run the code:</p>
  <pre class="programlisting">rus = RandomUnderSampler(
    sampling_strategy={0:n, 1:n}, random_state=random_seed
)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154267"></a>Then we call the <span class="fm-code-in-text">fit_resample()</span> function<a class="calibre8" id="marker-1154266"></a> of the under-sampler, with the inputs and labels arrays we created, to sample data:</p>
  <pre class="programlisting">rus.fit_resample(inputs, labels)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154269"></a>Once you fit the under-sampler, you can get the indices of the selected samples using the under-sampler’s <span class="fm-code-in-text">sample_indices_</span> attribute. Using those indices, we will create a new pair of arrays, <span class="fm-code-in-text">test_x</span> and <span class="fm-code-in-text">test_y</span>, to hold the test data:</p>
  <pre class="programlisting">test_inds = rus.sample_indices_
test_x, test_y = inputs[test_inds], np.array(labels)[test_inds]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154273"></a>The indices that are not in the test data set are assigned to separate arrays: <span class="fm-code-in-text">rest_x</span> and <span class="fm-code-in-text">rest_y</span>. These will be used to create the validation data set and the training data set:</p>
  <pre class="programlisting">rest_inds = [i for i in range(inputs.shape[0]) if i not in test_inds]
rest_x, rest_y = inputs[rest_inds], labels[rest_inds]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154276"></a>Following the same approach, we undersample data from <span class="fm-code-in-text">rest_x</span> and <span class="fm-code-in-text">rest_y</span> to create the validation data set (<span class="fm-code-in-text">valid_x</span> and <span class="fm-code-in-text">valid_y</span>). Note that we are not using the inputs and labels arrays, but rather the remaining data of those arrays after separating the test data:</p>
  <pre class="programlisting">rus.fit_resample(rest_x, rest_y)
valid_inds = rus.sample_indices_
valid_x, valid_y = rest_x[valid_inds], rest_y[valid_inds]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154280"></a>Finally, we create the training data set, which will hold all the remaining elements after creating the test and validation data sets:</p>
  <pre class="programlisting">train_inds = [i for i in range(rest_x.shape[0]) if i not in valid_inds]
train_x, train_y = rest_x[train_inds], rest_y[train_inds]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154283"></a>We also have to make sure that the training data set is balanced. In order to do that, let’s use a smarter way to undersample data than randomly selecting elements. The undersampling algorithm we will use here is called the near-miss algorithm. The near-miss algorithm removes samples from the majority class that are too close to the samples in the minority class. This helps to increase the distance between minority and majority class examples. Here, the minority class refers to the class having less data and the majority class refers to the class that has more data. To use the near-miss algorithm, it needs to able to compute the distance between two samples. Therefore, we need to convert our text into some numerical representation. We will represent each message as a bag-of-words representation. By using scikit-learn’s <span class="fm-code-in-text">CountVectorizer</span>, we can easily do that:</p>
  <pre class="programlisting">from sklearn.feature_extraction.text import CountVectorizer
 
countvec = CountVectorizer()
train_bow = countvec.fit_transform(train_x.reshape(-1).tolist())</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154288"></a><span class="fm-code-in-text">train_bow</span> will contain the bag-of-word representations of our data. Then we can pass this to a <span class="fm-code-in-text">NearMiss</span> instance. The way to obtain the data is the same as before:</p>
  <pre class="programlisting">from imblearn.under_sampling import  NearMiss
 
oss = NearMiss()
x_res, y_res = oss.fit_resample(train_bow, train_y)
train_inds = oss.sample_indices_
 
train_x, train_y = train_x[train_inds], train_y[train_inds]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154296"></a>Let’s print the sizes of our data sets and see if they have the sizes we wanted in the first place:</p>
  <pre class="programlisting">Test dataset size
1    100
0    100
dtype: int64
 
Valid dataset size
1    100
0    100
dtype: int64
Train dataset size
1    547
0    547
dtype: int64</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154312"></a>Excellent! Our data sets are all balanced, and we are ready to press ahead with the rest of the<a class="calibre8" id="marker-1154313"></a><a class="calibre8" id="marker-1154315"></a><a class="calibre8" id="marker-1154316"></a> workflow.</p>

  <p class="fm-head2"><a id="pgfId-1171535"></a>Defining the model</p>

  <p class="body"><a class="calibre8" id="pgfId-1171538"></a>With<a class="calibre8" id="marker-1171536"></a><a class="calibre8" id="marker-1171537"></a> the data prepared and ready, we will download the model. The BERT model we will use is from the TensorFlow hub (<span class="fm-hyperlink"><a class="url" href="https://www.tensorflow.org/hub">https://www.tensorflow.org/hub</a></span>). The TensorFlow hub is a model repository for various models trained on various tasks. You can get models for a multitude of tasks, including image classification, object detection, language modeling, question answering, and so on. To see the full list of available models, check out <span class="fm-hyperlink"><a class="url" href="https://tfhub.dev/">https://tfhub.dev/</a></span>.</p>

  <p class="body"><a class="calibre8" id="pgfId-1171541"></a>In order for us to successfully use BERT for an NLP task, we need three important things to work:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1171542"></a><i class="fm-italics">A tokenizer</i>—Determines how to split the provided input sequence to tokens</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1171543"></a><i class="fm-italics">An encoder</i>—Takes in the tokens, computes numerical representations, and finally generates a hidden representation for each token as well as a pooled representation (a single representation of the whole sequence)</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1171544"></a><i class="fm-italics">A classification head</i>—Takes in the pooled representation and generates a label for the input</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1171545"></a>First, let’s examine the tokenizer. The tokenizer takes a single input string or a list of strings and converts it to a list of strings or a list of lists of strings, respectively. It does this by breaking each string into smaller elements. For example, a sentence can be broken into words by splitting it on the space character. The tokenizer in BERT uses an algorithm known as the <i class="fm-italics">WordPiece</i> algorithm (<span class="fm-hyperlink"><a class="url" href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf">http://mng.bz/z40B</a></span>). It uses an iterative approach to find sub-words (i.e., parts of words) that appear commonly in a data set. The details of the WordPiece algorithm are out of scope for this book. Feel free to consult the original paper to learn more details. The most important characteristic of the tokenizer for this discussion is that it will break a given input string (e.g., a sentence) into a list of smaller tokens (e.g., sub-words).</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1171547"></a>Advantage of sub-word approaches like the WordPiece algorithm</p>

    <p class="fm-sidebar-text"><a id="pgfId-1171548"></a>Sub-word approaches like the WordPiece algorithm learn smaller, commonly occurring parts of words and use them to define the vocabulary. There are two main advantages to this approach compared to having whole words in our vocabulary.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1171549"></a>Using sub-words often reduces the size of the vocabulary. Assume the words [“walk”, “act”, “walked”, “acted”, “walking”, “acting”]. If individual words are used, each word needs to be a single item in the vocabulary. However, if a sub-word approach is used, the vocabulary can be reduced to [“walk”, “act”, “##ed”, “##ing”], which only has four words. Here, the ## means that it needs to be prefixed with another sub-word.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1171550"></a>Secondly, a sub-word approach can handle out-of-vocabulary words. This means the sub-word approach can represent words that appear in the test data set, but not in the training set. A word-based approach will not be able to do so and will simply replace the unseen word with a special token. Assume the sub-word vocabulary [“walk”, “act”, “##ed”, “##ing”, “develop”]. Even if the words “developed” or “developing” don’t appear in the training data, the vocabulary can still represent these words by combining two sub-words from the vocabulary (e.g., developed = develop + ##ed).</p>
  </div>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1171553"></a>To set up the tokenizer, let’s first import the <span class="fm-code-in-text1">tf-models-official</span> library<a class="calibre8" id="marker-1171552"></a>:</p>
  <pre class="programlisting">import tensorflow_models as tfm</pre>

  <p class="body"><a class="calibre8" id="pgfId-1171557"></a>Then you can define a tokenizer as follows:</p>
  <pre class="programlisting">vocab_file = os.path.join("data", "vocab.txt")
 
do_lower_case = True
 
tokenizer = tfm.nlp.layers.FastWordpieceBertTokenizer(
    vocab_file=vocab_file, lower_case=do_lower_case
)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1171566"></a>Here, you first get the location of the vocabulary file and define some configurations, such as whether the text should be converted to lowercase before tokenizing it. The vocabulary file is a text file with one sub-word in each line. This tokenizer uses Fast WordPiece Tokenization (<span class="fm-hyperlink"><a class="url" href="https://arxiv.org/abs/2012.15524.pdf">https://arxiv.org/abs/2012.15524.pdf</a></span>), an efficient implementation of the original WordPiece algorithm. Note that the <span class="fm-code-in-text">vocab_file</span> and <span class="fm-code-in-text">do_lower_case</span> are settings found in the model artifacts we will be downloading from TensorFlow hub in the next step. But for ease of understanding, we define them as constants here. You will find the code to automatically extract them from the model in the notebook. Next, we can use the tokenizer as follows</p>
  <pre class="programlisting">tokens = tf.reshape(
    tokenizer(["She sells seashells by the seashore"]), [-1])
print("Tokens IDs generated by BERT: {}".format(tokens))
ids = [tokenizer._vocab[tid] for tid in tokens] 
print("Tokens generated by BERT: {}".format(ids))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1171572"></a>which returns</p>
  <pre class="programlisting">Tokens IDs generated by BERT: [ 2016 15187 11915 18223  2015  2011  1996 11915 16892]
Tokens generated by BERT: ['she', 'sells', 'seas', '##hell', '##s', 'by', 'the', 'seas', '##hore']</pre>

  <p class="body"><a class="calibre8" id="pgfId-1171575"></a>You can see here how BERT’s tokenizer is tokenizing the sentence. Some words are kept as they are, whereas some words are split into sub-words (e.g., <span class="fm-code-in-text">seas + ##hell + ##s</span>). As discussed earlier, the <span class="fm-code-in-text">##</span> means that it does not mark the beginning of a word. In other words, <span class="fm-code-in-text">##</span> shows that this sub-word needs to be prefixed with another sub-word to get an actual word. Now, let’s look at the special tokens that are used by the BERT model and what IDs are assigned to them. This also validates that these tokens exist in the tokenizer:</p>
  <pre class="programlisting">special_tokens = ['[CLS]', '[SEP]', '[MASK]', '[PAD]']
ids = [tokenizer._vocab.index(tok) for tok in special_tokens]
for t, i in zip(special_tokens, ids):
    print("Token: {} has ID: {}".format(t, i))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1171580"></a>This returns</p>
  <pre class="programlisting">Token: [CLS] has ID: 101
Token: [SEP] has ID: 102
Token: [MASK] has ID: 103
Token: [PAD] has ID: 0</pre>

  <p class="body"><a class="calibre8" id="pgfId-1171588"></a>Here, the <span class="fm-code-in-text">[PAD]</span> is another special token used by BERT to denote padded tokens (0s). Padding is used very commonly in NLP to bring sentences with different lengths to the same length by padding the sentences with zeros. Here, the <span class="fm-code-in-text">[PAD]</span> token corresponds to zero.</p>

  <p class="body"><a class="calibre8" id="pgfId-1171589"></a>With the basic functionality of the tokenizer understood, we can define a function called <span class="fm-code-in-text">encode_sentence()</span> that will encode a given sentence to an input understood by the BERT model (see the next listing).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1171592"></a>Listing 13.2 Encoding a given input string using BERT’s tokenizer</p>
  <pre class="programlisting">def encode_sentence(s):
    """ Encode a given sentence by tokenizing it and adding special tokens """
    
    tokens = list(
        tf.reshape(tokenizer(["CLS" + s + "[SEP]"]), [-1])
    )                   <span class="fm-combinumeral">❶</span>
    return tokens       <span class="fm-combinumeral">❷</span></pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1178121"></a><span class="fm-combinumeral">❶</span> Add the special [CLS] and [SEP] tokens to the sequence and get the token IDs.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1178142"></a><span class="fm-combinumeral">❷</span> Return the token IDs.</p>

  <p class="body"><a class="calibre8" id="pgfId-1171603"></a>In this function, we return the tokenized output, first adding the [CLS] token, then tokenizing the given string to a list of sub-words, and finally adding the [SEP] token to mark the end of the sentence/sequence. For example, the sentence “I like ice cream”</p>
  <pre class="programlisting">encode_sentence("I like ice cream")</pre>

  <p class="body"><a class="calibre8" id="pgfId-1171607"></a>will return</p>
  <pre class="programlisting">[101, 1045, 2066, 3256, 6949, 102]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1171611"></a>As we can see, token ID 101 (i.e., [CLS]) is at the beginning, and 102 (i.e., [SEP]) is at the end. The rest of the token IDs correspond to the actual string we input. It is not enough to tokenize inputs for BERT; we also have to define some extra inputs to the model. For example, the sentence</p>
  <pre class="programlisting">"I like ice cream"</pre>

  <p class="body"><a class="calibre8" id="pgfId-1171613"></a>should return a data structure like the following:</p>
  <pre class="programlisting">{
    'input_word_ids': [[ 101, 1045, 2066, 3256, 6949,  102,    0,    0]], 
    'input_mask': [[1., 1., 1., 1., 1., 1., 0., 0.]], 
    'input_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0]]
}</pre>

  <p class="body"><a class="calibre8" id="pgfId-1171619"></a>Let’s discuss the various elements in this data structure. BERT takes in an input in the form of a dictionary where</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1171621"></a>The key <span class="fm-code-in-text">input_ids</span> represents the <span class="fm-code-in-text">token_ids</span> obtained from the <span class="fm-code-in-text">encode_sentence</span> function previously defined</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1171622"></a>The key <span class="fm-code-in-text">input_mask</span> represents a mask the same size as the <span class="fm-code-in-text">input_ids</span> of 1s and 0s, where ones indicate values that should not be masked (e.g., actual tokens in the input sequence and special tokens like [CLS] token ID 101 and [SEP] token ID 102), and where zero indicates tokens that should be masked (e.g., the [PAD] token ID 0).</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1171623"></a>The key <span class="fm-code-in-text">input_type_ids</span> is a matrix/vector of 1s and 0s having the same size as <span class="fm-code-in-text">input_ids</span>. This indicates which sentence each token belongs to. Remember that BERT can take two types of inputs: inputs with one sequence and inputs with two sequences, A and B. The <span class="fm-code-in-text">input_type_ids</span> matrix denotes which sequence (A or B) each token belongs to. Since we only have one sequence in our inputs, we simply create a matrix of zeros having the same size as <span class="fm-code-in-text">input_ids</span>.</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1171626"></a>The function <span class="fm-code-in-text">get_bert_inputs</span><a class="calibre8" id="marker-1171625"></a><span class="fm-code-in-text">()</span> will generate the input in this format using a set of documents (i.e., a list of strings, where each string is an input; see the next listing).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1171628"></a>Listing 13.3 Formatting a given input to the format BERT accepts</p>
  <pre class="programlisting">def get_bert_inputs(tokenizer, docs,max_seq_len=None):
    """ Generate inputs for BERT using a set of documents """
    
    packer = tfm.nlp.layers.BertPackInputs(                      <span class="fm-combinumeral">❶</span>
        seq_length=max_seq_length,
        special_tokens_dict = tokenizer.get_special_tokens_dict()
    )
    
    packed = packer(tokenizer(docs))                             <span class="fm-combinumeral">❷</span>
    
    packed_numpy = dict(
        [(k, v.numpy()) for k,v in packed.items()]               <span class="fm-combinumeral">❸</span>
    )
    # Final output
    return packed_numpy                                          <span class="fm-combinumeral">❹</span></pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1177810"></a><span class="fm-combinumeral">❶</span> Use BertPackInputs to generate token IDs, mask and segment IDs.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1177838"></a><span class="fm-combinumeral">❷</span> Generate outputs for all the messages in docs.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1177855"></a><span class="fm-combinumeral">❸</span> Convert the output of BertPackInputs to a dictionary with keys as string and value as a numpy array.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1177872"></a><span class="fm-combinumeral">❹</span> Return the result.</p>

  <p class="body"><a class="calibre8" id="pgfId-1171651"></a>Here we are using the <span class="fm-code-in-text">BertPackInputs</span> object that takes in an array where each item is a string containing the message. Then <span class="fm-code-in-text">BertPackInputs</span> generates a dictionary that contains the following processed outputs:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1171652"></a><span class="fm-code-in-text">input_word_ids</span>—Token IDs with [CLS] and [SEP] token IDs automatically added.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1171653"></a><span class="fm-code-in-text">input_mask</span>—An integer array with each element representing whether it is a real token (1) or a padded token (0) at that position.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1171654"></a><span class="fm-code-in-text">input_type_ids</span>—An integer array with each element representing which segment each token belongs to. It will be an array of all zeros in this case.</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1171655"></a><span class="fm-code-in-text">BertPackInputs</span> does a lot of different preprocessing required by the BERT model. You can read about various inputs accepted by this layer at <span class="fm-hyperlink"><a class="url" href="https://www.tensorflow.org/api_docs/python/tfm/nlp/layers/BertPackInputs">https://www.tensorflow.org/api_docs/python/tfm/nlp/layers/BertPackInputs</a></span>.</p>

  <p class="body"><a class="calibre8" id="pgfId-1171657"></a>To generate the prepared training, validation, and testing data for the model, simply call the <span class="fm-code-in-text">get_bert_inputs()</span> function:</p>
  <pre class="programlisting">train_inputs = get_bert_inputs(train_x, max_seq_len=80)
valid_inputs = get_bert_inputs(valid_x, max_seq_len=80)
test_inputs = get_bert_inputs(test_x, max_seq_len=80)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1171661"></a>After this, let’s shuffle the data in <span class="fm-code-in-text">train_inputs</span> as a precaution. Currently, the data is ordered such that <span class="fm-code-in-text">spam</span> messages appear after <span class="fm-code-in-text">ham</span> messages:</p>
  <pre class="programlisting">train_inds = np.random.permutation(len(train_inputs["input_word_ids"]))
train_inputs = dict(
    [(k, v[train_inds]) for k, v in train_inputs.items()]
)
train_y = train_y[train_inds]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1171667"></a>Remember to use the same shuffling for both inputs and labels to maintain their association. We have done everything to prepare the inputs for the model. Now it’s time for the grand unveiling of the model. We need to define BERT with a classification head so that the model can be trained end to end on the classification data set we have. We will do this in two steps. First, we will download the encoder part of BERT from TensorFlow hub and then use the <span class="fm-code-in-text">tfm.nlp.models.BertClassifier</span> object in the <span class="fm-code-in-text">tensorflow-models-official</span> library to generate the final BERT model with the classifier head. Let’s examine how we do the first part:</p>
  <pre class="programlisting">import tensorflow_hub as hub
 
hub_bert_url = "https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4"
max_seq_length = 60
# Contains input token ids
input_word_ids = tf.keras.layers.Input(
    shape=(max_seq_length,), dtype=tf.int32, name="input_word_ids"
)
# Contains input mask values
input_mask = tf.keras.layers.Input(
    shape=(max_seq_length,), dtype=tf.int32, name="input_mask"
)
input_type_ids = tf.keras.layers.Input(
    shape=(max_seq_length,), dtype=tf.int32, name="input_type_ids"
)
 
# BERT encoder downloaded from TF hub
bert_layer = hub.KerasLayer(hub_bert_url, trainable=True)
 
# get the output of the encoder
output = bert_layer({
    "input_word_ids":input_word_ids, 
    "input_mask": input_mask, 
    "input_type_ids": input_type_ids
})
 
# Define the final encoder as with the Functional API
hub_encoder = tf.keras.models.Model(
    inputs={
        "input_word_ids": input_word_ids, 
        "input_mask": input_mask, 
        "input_type_ids": input_type_ids
    }, 
    outputs={
        "sequence_output": output["sequence_output"], 
        "pooled_output": output["pooled_output"]
    }
)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1171709"></a>Here, we first define three input layers, where each input layer has one of the outputs of the <span class="fm-code-in-text">BertPackInputs</span> mapped to. For example, the <span class="fm-code-in-text">input_word_ids</span> input layer will receive the output found at the key <span class="fm-code-in-text">input_word_ids</span> in the dictionary produced by the function <span class="fm-code-in-text">get_bert_inputs()</span>. Next, we download the pretrained BERT encoder by passing a URL to the <span class="fm-code-in-text">hub.KerasLayer</span> object. This layer produces two outputs: <span class="fm-code-in-text">sequence_output</span>, which contains hidden representations of all the time steps, and <span class="fm-code-in-text">pooled_output</span>, which contains the hidden representation corresponding to the position of the <span class="fm-code-in-text">[CLS]</span> token. For this problem, we need the latter to pass it to a classification head sitting on top of the encoder. We will finally define a Keras model using the Functional API. This model needs to have a specific input and output signature defined via dictionaries as shown previously. We will use this model to define a classifier model based on this encoder:</p>
  <pre class="programlisting"># Generating a classifier and the encoder
bert_classifier = tfm.nlp.models.BertClassifier(
    network=hub_encoder, num_classes=2
)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1171717"></a>As you can see, it’s quite straightforward to define the classifier. We simply pass our <span class="fm-code-in-text">hub_encoder</span> to the <span class="fm-code-in-text">BertClassifier</span> and say we have two classes, <span class="fm-code-in-text">spam</span> and <span class="fm-code-in-text">ham</span> (i.e. <span class="fm-code-in-text">num_classes=2</span>).</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1171719"></a>Alternative way to get the BERT encoder</p>

    <p class="fm-sidebar-text"><a id="pgfId-1171720"></a>There’s another method you can use to get a BERT encoder. However, it requires manually loading pretrained weights; therefore, we will keep this method as an alternative. Firstly, you start with a configuration file containing the encoder model’s various hyperparameters. I have provided you with the original configuration used for the model as a YAML file (<span class="fm-code-in-text1">Ch12/data/</span><span class="fm-code-in-text1">bert_en_uncased_base.yaml</span>). It contains various hyperparameters used by BERT (e.g., hidden dimension size, nonlinear activation, etc.). Feel free to look at them to understand the different parameters used for the model. We will load these configurations using the <span class="fm-code-in-text1">yaml</span> library as a dictionary and store it in <span class="fm-code-in-text1">config_dict</span>. Next, we generate <span class="fm-code-in-text1">encoder_config</span>, an <span class="fm-code-in-text1">EncoderConfig</span> object initiated with the configuration we loaded. With <span class="fm-code-in-text1">encoder_config</span> defined, we will build an encoder BERT model that is able to generate feature representations of tokens, and then call <span class="fm-code-in-text1">bert.bert_models.classifier_model()</span> with this encoder as the network. Note that this method gets a randomly initialized BERT model<a id="marker-1174687"></a>:</p>
    <pre class="programlisting">import yaml
 
with open(os.path.join("data", "bert_en_uncased_base.yaml"), 'r') as stream:
    config_dict = yaml.safe_load(stream)['task']['model']['encoder']['bert']
 
encoder_config = tfm.nlp.encoders.EncoderConfig({
    'type':'bert',
    'bert': config_dict
})
 
bert_encoder = tfm.nlp.encoders.build_encoder(encoder_config)
 
bert_classifier = tfm.nlp.models.BertClassifier(
    network=bert_encoder, num_classes=2
)</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1171739"></a>If you want a pretrained version of BERT like this, then you need to download the TensorFlow checkpoint. You can do this by going to the link found in <span class="fm-code-in-text1">bert_url</span> and then clicking download. Finally, you load the weights with</p>
    <pre class="programlisting">checkpoint = tf.train.Checkpoint(encoder=bert_encoder)
checkpoint.read(&lt;path to .ckpt&gt;).assert_consumed()</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1171744"></a>Now you have a pretrained BERT encoder.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1171745"></a>Next, let’s discuss how we can compile the built model<a class="calibre8" id="marker-1174873"></a><a class="calibre8" id="marker-1174862"></a>.</p>

  <p class="fm-head2"><a id="pgfId-1171747"></a>Compiling the model</p>

  <p class="body"><a class="calibre8" id="pgfId-1171750"></a>Here<a class="calibre8" id="marker-1171748"></a><a class="calibre8" id="marker-1171749"></a> we will define the optimizer to train the model. So far, we have not changed much from the default optimizer options provided in TensorFlow/Keras. This time, let’s use the optimizer provided in the <span class="fm-code-in-text">tf-models-official</span> library. The optimizer can be instantiated by calling the <span class="fm-code-in-text">nlp.optimization.create_optimizer()</span> function<a class="calibre8" id="marker-1171751"></a>. This is outlined in the next listing.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1171753"></a>Listing 13.4 Optimizing BERT on the spam classification task</p>
  <pre class="programlisting">epochs = 3
batch_size = 56
eval_batch_size = 56
 
train_data_size = train_x.shape[0]
steps_per_epoch = int(train_data_size / batch_size)
num_train_steps = steps_per_epoch * epochs
warmup_steps = int(num_train_steps * 0.1)
 
init_lr = 3e-6
end_lr = 0.0
 
linear_decay = tf.keras.optimizers.schedules.PolynomialDecay(
    initial_learning_rate=init_lr,
    end_learning_rate=end_lr,
    decay_steps=num_train_steps)
 
warmup_schedule = tfm.optimization.lr_schedule.LinearWarmup(
    warmup_learning_rate = 1e-10,
    after_warmup_lr_sched = linear_decay,
    warmup_steps = warmup_steps
)
 
optimizer = tf.keras.optimizers.experimental.Adam(
    learning_rate = warmup_schedule
)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1171782"></a>As the default optimizer, <i class="fm-italics">Adam with weight decay</i> (<span class="fm-hyperlink"><a class="url" href="https://arxiv.org/pdf/1711.05101.pdf">https://arxiv.org/pdf/1711.05101.pdf</a></span>) is used. Adam with weight decay is a variant of the original Adam optimizer we used, but with better generalization properties. The <span class="fm-code-in-text">num_warmup_steps</span> denotes the duration of the learning rate warm-up. During the warm-up, the learning rate is brought up linearly from a small value to <span class="fm-code-in-text">init_lr</span> (defined in <span class="fm-code-in-text">linear_decay</span>) within <span class="fm-code-in-text">num_ warmup_steps</span>. After that, the learning rate is decayed all the way to <span class="fm-code-in-text">end_lr</span> (defined in <span class="fm-code-in-text">linear_decay</span>) during <span class="fm-code-in-text">num_train_steps</span> using a polynomial decay (<span class="fm-hyperlink"><a class="url" href="http://mng.bz/06lN">http://mng.bz/06lN</a></span>). This is depicted in figure 13.9.</p>

  <p class="fm-figure"><img alt="13-09" class="calibre10" src="../../OEBPS/Images/13-09.png" width="703" height="378"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1180371"></a>Figure 13.9 The behavior of the learning rate as training progresses through iterations (i.e., steps)</p>

  <p class="body"><a class="calibre8" id="pgfId-1171790"></a>Now we can compile the model just like we did before. We will define a loss (sparse categorical cross-entropy loss) and a metric (accuracy computed using labels rather than one-hot vectors) and then pass the optimizer, loss, and metric to the <span class="fm-code-in-text">hub_ classifier.compile</span><a class="calibre8" id="marker-1174502"></a><a class="calibre8" id="marker-1174503"></a><span class="fm-code-in-text">()</span> function:</p>
  <pre class="programlisting">metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', 
<span class="fm-code-continuation-arrow">➥</span> dtype=tf.float32)]
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
 
hub_classifier.compile(
    optimizer=optimizer,
    loss=loss,
    metrics=metrics)</pre>

  <p class="fm-head2"><a id="pgfId-1171802"></a>Training the model</p>

  <p class="body"><a class="calibre8" id="pgfId-1171806"></a>We’ve<a class="calibre8" id="marker-1171803"></a><a class="calibre8" id="marker-1171804"></a><a class="calibre8" id="marker-1171805"></a> come a long way, and now all that’s left is training the model. Model training is quite simple and similar to how we trained models using the <span class="fm-code-in-text">tf.keras.Model.fit()</span> function<a class="calibre8" id="marker-1171807"></a>:</p>
  <pre class="programlisting">hub_classifier.fit(
      x=train_inputs, 
      y=train_y,
      validation_data=(valid_inputs, valid_y),
      validation_batch_size=eval_batch_size,
      batch_size=batch_size,
      epochs=epochs)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1171815"></a>We are passing the <span class="fm-code-in-text">train_inputs</span> we prepared using the <span class="fm-code-in-text">get_bert_inputs()</span> function to the argument <span class="fm-code-in-text">x</span> and <span class="fm-code-in-text">train_y</span> (i.e., a vector of 1s and 0s indicating whether the input is a spam or a ham, respectively) to <span class="fm-code-in-text">y</span>. Similarly, we define <span class="fm-code-in-text">validation_data</span> as a tuple containing <span class="fm-code-in-text">valid_inputs</span> and <span class="fm-code-in-text">valid_y</span>. We also pass in the <span class="fm-code-in-text">batch_size</span> (training batch size), <span class="fm-code-in-text">validation_batch_size</span>, and the number of epochs to train<a class="calibre8" id="marker-1171816"></a><a class="calibre8" id="marker-1171817"></a><a class="calibre8" id="marker-1171818"></a> for.</p>

  <p class="fm-head2"><a id="pgfId-1171819"></a>Evaluating and interpreting the results</p>

  <p class="body"><a class="calibre8" id="pgfId-1171822"></a>When<a class="calibre8" id="marker-1171820"></a><a class="calibre8" id="marker-1171821"></a> you run the training, you should get a result close to the following. Here you can see the training loss and the accuracy as well as the validation loss and the accuracy:</p>
  <pre class="programlisting">Epoch 1/3
18/18 [==============================] - 544s 29s/step - loss: 0.7082 - 
<span class="fm-code-continuation-arrow">➥</span> accuracy: 0.4555 - val_loss: 0.6764 - val_accuracy: 0.5150
Epoch 2/3
18/18 [==============================] - 518s 29s/step - loss: 0.6645 - 
<span class="fm-code-continuation-arrow">➥</span> accuracy: 0.6589 - val_loss: 0.6480 - val_accuracy: 0.8150
Epoch 3/3
18/18 [==============================] - 518s 29s/step - loss: 0.6414 - 
<span class="fm-code-continuation-arrow">➥</span> accuracy: 0.7608 - val_loss: 0.6391 - val_accuracy: 0.8550</pre>

  <p class="body"><a class="calibre8" id="pgfId-1171829"></a>The console output clearly shows that the training loss went down steadily, whereas the accuracy rose from 45% to 76%. The model has reached a validation accuracy of 85%. This clearly shows the power of a model like BERT. If you were to train an NLP model from scratch, it would be impossible to reach a validation accuracy of 85% in such a short time. Since BERT has very strong language understanding, the model can focus on learning the task at hand. Note that you may get a different level of accuracy from what’s shown here as our validation and testing sets are quite small (each having only 200 records).</p>

  <p class="fm-callout"><a id="pgfId-1171830"></a><span class="fm-callout-head">NOTE</span> On an Intel Core i5 machine with an NVIDIA GeForce RTX 2070 8 GB, the training took approximately 40 seconds to run 3 epochs.</p>

  <p class="body"><a class="calibre8" id="pgfId-1171832"></a>Finally, let’s test the model on the test data by calling the <span class="fm-code-in-text">evaluate()</span> function<a class="calibre8" id="marker-1171831"></a></p>
  <pre class="programlisting">hub_classifier.evaluate(test_inputs, test_y)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1171836"></a>which will return</p>
  <pre class="programlisting">7/7 [==============================] - 22s 3s/step - loss: 0.6432 - accuracy: 0.7950</pre>

  <p class="body"><a class="calibre8" id="pgfId-1171840"></a>Again, it’s a great result. After just three epochs and no fancy parameter tweaking, we have achieved 79.5% accuracy on the test data. And all we did was fit a logistic regression layer on top of BERT.</p>

  <p class="body"><a class="calibre8" id="pgfId-1171841"></a>The next section will discuss how we can define a model that can find answers to a given question from a paragraph. To do this, we will use one of the most popular libraries for Transformer models to date: Hugging Face’s <span class="fm-code-in-text">transformers</span><a class="calibre8" id="marker-1171842"></a><a class="calibre8" id="marker-1171843"></a> library<a class="calibre8" id="marker-1174673"></a><a class="calibre8" id="marker-1174678"></a>.</p>

  <p class="fm-head2"><a id="pgfId-1171844"></a>Exercise 2</p>

  <p class="body"><a class="calibre8" id="pgfId-1171845"></a>You have a classification problem that has five classes, and you would like to modify the <span class="fm-code-in-text">bert_classifier</span>. Given the data in the correct format, how would you change the <span class="fm-code-in-text">bert_classifier</span><a class="calibre8" id="marker-1171846"></a><a class="calibre8" id="marker-1171847"></a><a class="calibre8" id="marker-1171848"></a><a class="calibre8" id="marker-1171849"></a> object<a class="calibre8" id="marker-1171850"></a><a class="calibre8" id="marker-1171851"></a><a class="calibre8" id="marker-1171852"></a> defined?</p>

  <h2 class="fm-head" id="sigil_toc_id_173"><a id="pgfId-1154552"></a><a id="marker-1174825"></a><a id="marker-1174910"></a>13.3 Question answering with Hugging Face’s Transformers</h2>

  <p class="body"><a class="calibre8" id="pgfId-1154556"></a>Your<a class="calibre8" id="marker-1164529"></a><a class="calibre8" id="marker-1164530"></a><a class="calibre8" id="marker-1164531"></a> friend is planning to kick off a start-up that uses ML to find answers to open-domain questions. Lacking the ML background, he turns to you to ask whether it is feasible to do so using ML. Knowing that question answering is machine learnable given labeled data, you decide to create a question-answering prototype using a BERT variant and demonstrate it. You will be using the SQUAD v1 question-answering data set and training a DistilBERT (a variant of BERT) on the data set. For this, you are going to use the Hugging Face’s <span class="fm-code-in-text">transformers</span> library (<span class="fm-hyperlink"><a class="url" href="https://huggingface.co/transformers/">https://huggingface.co/transformers/</a></span>). Hugging Face’s <span class="fm-code-in-text">transformers</span> library provides implementations of different Transformer models and easy-to-use APIs to train/evaluate models on data sets.</p>

  <p class="body"><a class="calibre8" id="pgfId-1154557"></a>BERT is designed to solve two different types of tasks:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1154558"></a>Tasks having a single sequence of text as the input</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1154559"></a>Tasks having two sequences of text (A and B) as the input</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1154560"></a>Spam classification falls under the first type. Question answering is a type of task that has two input sequences. In question answering, you have a question and a context (a paragraph, a sentence, etc.), which may contain the answer to the question. Then a model is trained to predict the answer for a given question and a context. Let’s get to know the process a bit better. Each record in the data set will consist of following elements:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1154561"></a>A question (a sequence of text)</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1154562"></a>A context (a sequence of text)</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1154563"></a>A starting index of the answer from the context (an integer)</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1154564"></a>An ending index of the answer from the context (an integer)</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1154565"></a>First, we need to combine the question and the context and add several special tokens. At the beginning, we need to add a [CLS] token and then a [SEP] to separate the question and the context, as well as an [SEP] to mark the end of the input. Furthermore, the question and the context are broken down into tokens (i.e., sub-words) using the model’s tokenizer. For the input having</p>

  <p class="body"><a class="calibre8" id="pgfId-1154566"></a>Question: <span class="fm-code-in-text">What did the dog barked at</span></p>

  <p class="body"><a class="calibre8" id="pgfId-1154567"></a>Answer: <span class="fm-code-in-text">The dog barked at the mail man</span></p>

  <p class="body"><a class="calibre8" id="pgfId-1154568"></a>if we assume individual words as tokens, the input will look as follows:</p>
  <pre class="programlisting">[CLS], What, did, the, dog, barked, at, [SEP], The, dog, barked, at, the, 
<span class="fm-code-continuation-arrow">➥</span> mailman, [SEP]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154572"></a>Next, these tokens are converted to IDs and fed to the BERT model. The output of the BERT model is connected to two downstream classification layers: one layer predicts the starting index of the answer, whereas the other layer predicts the ending index of the answer. Each of these two classification layers has its own weights and biases.</p>

  <p class="body"><a class="calibre8" id="pgfId-1154573"></a>The BERT model will output a hidden representation for each token in the input sequence. The token output representations across the entire span of the context are fed to the downstream models. Each classification layer then predicts the probability of each token being the starting/ending token of the answer. The weights of these layers are shared across the time dimension. In other words, the same weight matrix is applied to every output representation to predict the probabilities.</p>

  <p class="body"><a class="calibre8" id="pgfId-1154574"></a>For this section, we are going to use Hugging Face’s <span class="fm-code-in-text">transformers</span> library. Please refer to the sidebar for more details.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1154575"></a>Hugging Face’s <span class="fm-code-in-text2">transformers</span> library</p>

    <p class="fm-sidebar-text"><a id="pgfId-1154576"></a>Hugging Face is a company that focuses on solving NLP problems. The company provides libraries to train NLP models as well as host data sets and train models in a publicly accessible repository. We will use two Python libraries provided by Hugging Face: <span class="fm-code-in-text1">transformers</span> and <span class="fm-code-in-text1">datasets</span>.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1154577"></a>At the time of writing this book, the <span class="fm-code-in-text1">transformers</span> library (<span class="fm-hyperlink"><a class="url" href="https://huggingface.co/transformers/">https://huggingface.co/transformers/</a></span>) is the most versatile Python library that provides instant access to many Transformer models that have been released (e.g., BERT, XLNet, DistilBERT, Albert, RoBERT, etc.) as well as to community-released NLP models (<span class="fm-hyperlink"><a class="url" href="https://huggingface.co/models">https://huggingface.co/models</a></span>). The <span class="fm-code-in-text1">transformers</span> library supports both TensorFlow and PyTorch deep learning frameworks. PyTorch (<span class="fm-hyperlink"><a class="url" href="https://pytorch.org/">https://pytorch.org/</a></span>) is another deep learning framework like TensorFlow that offers a comprehensive suite of functionality to implement and productionize deep learning models. The following are the key advantages I see in the <span class="fm-code-in-text1">transformers</span> library:</p>

    <ul class="calibre9">
      <li class="fm-sidebar-number">
        <p class="list1"><a id="pgfId-1154578"></a>An easy-to-understand API to pretrain and fine-tune models that is consistent across all the models</p>
      </li>

      <li class="fm-sidebar-number">
        <p class="list1"><a id="pgfId-1154579"></a>The ability to download TensorFlow versions of various Transformer models and use them as Keras models</p>
      </li>

      <li class="fm-sidebar-number">
        <p class="list1"><a id="pgfId-1154580"></a>The ability to convert TensorFlow and PyTorch models</p>
      </li>

      <li class="fm-sidebar-number">
        <p class="list1"><a id="pgfId-1154581"></a>Powerful features such as the Trainer (<span class="fm-hyperlink"><a class="url" href="http://mng.bz/Kx9j">http://mng.bz/Kx9j</a></span>), which allows users to create and train models with lot of flexibility</p>
      </li>

      <li class="fm-sidebar-number">
        <p class="list1"><a id="pgfId-1154582"></a>Highly active community-contributing models and data sets</p>
      </li>
    </ul>
  </div>

  <h3 class="fm-head1" id="sigil_toc_id_174"><a id="pgfId-1154584"></a>13.3.1 Understanding the data</h3>

  <p class="body"><a class="calibre8" id="pgfId-1154589"></a>As<a class="calibre8" id="marker-1154587"></a><a class="calibre8" id="marker-1154588"></a> mentioned, we will use the SQuAD v1 data set (<span class="fm-hyperlink"><a class="url" href="https://rajpurkar.github.io/SQuAD-explorer/">https://rajpurkar.github.io/SQuAD-explorer/</a></span>). It’s a question-answering data set created by Stanford University. You can easily download the data set using Hugging Face’s <span class="fm-code-in-text">datasets</span> library<a class="calibre8" id="marker-1154590"></a> as follows:</p>
  <pre class="programlisting">from datasets import load_dataset
dataset = load_dataset("squad")</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154595"></a>Let’s print the data set and see the available attributes:</p>
  <pre class="programlisting">print(dataset)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154599"></a>This will return</p>
  <pre class="programlisting">DatasetDict({
    train: Dataset({
        features: ['id', 'title', 'context', 'question', 'answers'],
        num_rows: 87599
    })
    validation: Dataset({
        features: ['id', 'title', 'context', 'question', 'answers'],
        num_rows: 10570
    })
})</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154610"></a>There are 87,599 training examples and 10,570 validation samples. We will use these examples to create the train/validation/test split. We are interested in the last three columns in the features section only (i.e., <span class="fm-code-in-text">context</span>, <span class="fm-code-in-text">question</span>, and <span class="fm-code-in-text">answers</span>). From these, <span class="fm-code-in-text">context</span> and <span class="fm-code-in-text">question</span> are simply strings, whereas <span class="fm-code-in-text">answers</span> is a dictionary. Let’s analyze <span class="fm-code-in-text">answers</span> a bit further. You can print a few answers with</p>
  <pre class="programlisting">dataset["train"]["answers"][:5]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154614"></a>which gives</p>
  <pre class="programlisting">[{'answer_start': [515], 'text': ['Saint Bernadette Soubirous']},
 {'answer_start': [188], 'text': ['a copper statue of Christ']},
 {'answer_start': [279], 'text': ['the Main Building']},
 {'answer_start': [381], 'text': ['a Marian place of prayer and reflection']},
 {'answer_start': [92], 'text': ['a golden statue of the Virgin Mary']}]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154620"></a>We can see that each answer has a starting index (character based) and a text containing the answer. With this information, we can easily calculate the answer’s ending index<a class="calibre8" id="marker-1159806"></a><a class="calibre8" id="marker-1159807"></a> (i.e., <span class="fm-code-in-text">end index = start index + len(text)</span>).</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1154625"></a>GLUE benchmark task suite</p>

    <p class="fm-sidebar-text"><a id="pgfId-1154626"></a>The GLUE benchmark (<span class="fm-hyperlink"><a class="url" href="https://gluebenchmark.com/tasks">https://gluebenchmark.com/tasks</a></span>) is a popular collection of tasks for evaluating NLP models. It tests the natural language understanding of models on a variety of tasks. The following tasks are included in the GLUE tasks collection.</p>

    <p class="fm-figure"><img alt="13_table1" class="calibre10" src="../../OEBPS/Images/13_table1.png" width="740" height="795"/><br class="calibre2"/></p>
  </div>

  <h3 class="fm-head1" id="sigil_toc_id_175"><a id="pgfId-1154698"></a>13.3.2 Processing data</h3>

  <p class="body"><a class="calibre8" id="pgfId-1154702"></a>This<a class="calibre8" id="marker-1154699"></a><a class="calibre8" id="marker-1154700"></a><a class="calibre8" id="marker-1154701"></a> data set has several integrity issues that need to be taken care of. We will fix those issues and then create a <span class="fm-code-in-text">tf.data</span> pipeline<a class="calibre8" id="marker-1154703"></a> to pump the data. The first issues that need to be fixed are alignment issues between the given <span class="fm-code-in-text">answer_start</span> and the actual <span class="fm-code-in-text">answer_start</span>. Some examples tend to have an offset of around two characters between the given and the actual <span class="fm-code-in-text">answer_start</span> positions. We will write a function to correct for this offset, as well as add the end index. The next listing outlines the code to perform this operation.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1154705"></a>Listing 13.5 Fixing the unwanted offsets in the answer start/end indices</p>
  <pre class="programlisting">def correct_indices_add_end_idx(answers, contexts):
    """ Correct the answer index of the samples (if wrong) """
    
    n_correct, n_fix = 0, 0                                       <span class="fm-combinumeral">❶</span>
    fixed_answers = []                                            <span class="fm-combinumeral">❷</span>
    for answer, context in zip(answers, contexts):                <span class="fm-combinumeral">❸</span>
 
        gold_text = answer['text'][0]                             <span class="fm-combinumeral">❹</span>
        answer['text'] = gold_text                                <span class="fm-combinumeral">❹</span>
        start_idx = answer['answer_start'][0]                     <span class="fm-combinumeral">❺</span>
        answer['answer_start'] = start_idx                        <span class="fm-combinumeral">❺</span>
        if start_idx &lt;0 or len(gold_text.strip())==0:
            print(answer)
        end_idx = start_idx + len(gold_text)                      <span class="fm-combinumeral">❻</span>
        
        # sometimes squad answers are off by a character or two - fix this
        if context[start_idx:end_idx] == gold_text:               <span class="fm-combinumeral">❼</span>
            answer['answer_end'] = end_idx
            n_correct += 1
        elif context[start_idx-1:end_idx-1] == gold_text:         <span class="fm-combinumeral">❽</span>
            answer['answer_start'] = start_idx - 1
            answer['answer_end'] = end_idx - 1     
            n_fix += 1
        elif context[start_idx-2:end_idx-2] == gold_text:         <span class="fm-combinumeral">❾</span>
            answer['answer_start'] = start_idx - 2
            answer['answer_end'] = end_idx - 2 
            n_fix +=1
        
        fixed_answers.append(answer)
        
    print(                                                        <span class="fm-combinumeral">❿</span>
        "\t{}/{} examples had the correct answer indices".format(
            n_correct, len(answers)
        )
    )
    print(                                                        <span class="fm-combinumeral">❿</span>
        "\t{}/{} examples had the wrong answer indices".format(
            n_fix, len(answers)
        )
    )
    return fixed_answers, contexts                                <span class="fm-combinumeral">⓫</span></pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1176927"></a><span class="fm-combinumeral">❶</span> Track how many were correct and fixed.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1176948"></a><span class="fm-combinumeral">❷</span> New fixed answers will be held in this variable.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1176965"></a><span class="fm-combinumeral">❸</span> Iterate through each answer context pair.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1176982"></a><span class="fm-combinumeral">❹</span> Convert the answer from a list of strings to a string.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1176999"></a><span class="fm-combinumeral">❺</span> Convert the start of the answer from a list of integers to an integer.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1177016"></a><span class="fm-combinumeral">❻</span> Compute the end index by adding the answer’s length to the start_idx.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1177033"></a><span class="fm-combinumeral">❼</span> If the slice from start_idx to end_idx exactly matches the answer text, no changes are required.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1177050"></a><span class="fm-combinumeral">❽</span> If the slice from start_idx to end_idx needs to be offset by 1 to match the answer, offset accordingly.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1177067"></a><span class="fm-combinumeral">❾</span> If the slice from start_idx to end_idx needs to be offset by 2 to match the answer, offset accordingly.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1177091"></a><span class="fm-combinumeral">❿</span> Print the number of correct answers (requires no change).</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1177108"></a><span class="fm-combinumeral">⓫</span> Print the number of answers that required fixing.</p>

  <p class="body"><a class="calibre8" id="pgfId-1154759"></a>Now we can call this function on the two subsets (train and validation) of the data set. We will use the validation subset as our testing set (unseen examples). A proportion of the training examples will be set aside as validation samples:</p>
  <pre class="programlisting">train_questions = dataset["train"]["question"]
train_answers, train_contexts = correct_indices_add_end_idx(
    dataset["train"]["answers"], dataset["train"]["context"]
)
 
test_questions = dataset["validation"]["question"]
test_answers, test_contexts = correct_indices_add_end_idx(
    dataset["validation"]["answers"], dataset["validation"]["context"]
)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154770"></a>When you run this code, you will see the following statistics:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1154771"></a>Training data corrections</p>

      <ul class="calibre18">
        <li class="fm-sidebar-bullet-sub">
          <p class="list"><a class="calibre8" id="pgfId-1154772"></a>87,341/87,599 examples had the correct answer indices.</p>
        </li>

        <li class="fm-sidebar-bullet-sub">
          <p class="list"><a class="calibre8" id="pgfId-1154773"></a>258/87,599 examples had the wrong answer indices.</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1154774"></a>Validation data correction</p>

      <ul class="calibre18">
        <li class="fm-sidebar-bullet-sub">
          <p class="list"><a class="calibre8" id="pgfId-1154775"></a>10,565/10,570 examples had the correct answer indices.</p>
        </li>

        <li class="fm-sidebar-bullet-sub">
          <p class="list"><a class="calibre8" id="pgfId-1154776"></a>5/10,570 examples had the wrong answer indices.</p>
        </li>
      </ul>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1154777"></a>We have to make sure that the required number of corrections is not drastically high. If the number of corrections is significantly high, typically it could mean a bug in the code or problems with the data-loading logic. Here, we can clearly see that the number of examples that required corrections is low.</p>

  <p class="fm-head2"><a id="pgfId-1154778"></a>Defining and using the tokenizer</p>

  <p class="body"><a class="calibre8" id="pgfId-1154783"></a>All<a class="calibre8" id="marker-1154780"></a><a class="calibre8" id="marker-1154781"></a><a class="calibre8" id="marker-1154782"></a> the data we need to solve the problem is available to us. Now it’s time to tokenize the data like we did before. Remember that these pretrained NLP models come in two parts: the tokenizer and the model itself. The tokenizer tokenizes the text into smaller parts (e.g., sub-words) and presents those to the model in the form of a sequence of IDs. Then the model takes in those IDs and performs embedding lookups on them, as well as various computations, to come up with the final token representation that will be used as inputs to the downstream classification model (i.e., the question-answering classification layer). In a similar fashion, in the <span class="fm-code-in-text">transformers</span> library, you have the tokenizer object and the model object:</p>
  <pre class="programlisting">from transformers import DistilBertTokenizerFast
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154786"></a>You can see that we are using a tokenizer called <span class="fm-code-in-text">DistilBertTokenizerFast</span>. This tokenizer comes from a model known as the DistilBERT. DistilBERT is a smaller version of BERT that shows similar performance as BERT but smaller in size. It has been trained using a transfer learning technique known as <i class="fm-italics">knowledge distillation</i> (<span class="fm-hyperlink"><a class="url" href="https://devopedia.org/knowledge-distillation">https://devopedia.org/knowledge-distillation</a></span>). To get the tokenizer, all we need to do is call the <span class="fm-code-in-text">DistilBertTokenizerFast.from_pretrained()</span> function<a class="calibre8" id="marker-1154787"></a> with the model tag (i.e., <span class="fm-code-in-text">distilbert-base-uncased</span>). This tag says the model is a DistilBERT-type model with <span class="fm-code-in-text">base</span> size (there are different-sized models available) and ignores the case of the characters (denoted by <span class="fm-code-in-text">uncased</span>). The model tag points to a model available in Hugging Face’s model repository (<span class="fm-hyperlink"><a class="url" href="https://huggingface.co/models">https://huggingface.co/models</a></span>) and downloads it for us. It will be stored on your computer.</p>

  <p class="body"><a class="calibre8" id="pgfId-1154788"></a>Hugging Face provides two different variants of tokenizers: standard tokenizers (<span class="fm-code-in-text">PreTrainedTokenizer</span> objects; <span class="fm-hyperlink"><a class="url" href="http://mng.bz/95d7">http://mng.bz/95d7</a></span>) and fast tokenizers (<span class="fm-code-in-text">PreTrainedTokenizerFast</span> objects; <span class="fm-hyperlink"><a class="url" href="http://mng.bz/j2Xr">http://mng.bz/j2Xr</a></span>). You can read about their differences on <span class="fm-hyperlink"><a class="url" href="http://mng.bz/WxEa">http://mng.bz/WxEa</a></span>). They are significantly faster when encoding (i.e., converting strings to token sequences) in a batch-wise fashion. Furthermore, fast tokenizers have additional methods that will help us to process inputs easily for the question-answering model<a class="calibre8" id="marker-1174716"></a>.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1154789"></a>What is DistilBERT?</p>

    <p class="fm-sidebar-text"><a id="pgfId-1154790"></a>Following BERT, DistilBERT is a model introduced by Hugging Face in the paper “DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter” by Sanh et al. (<span class="fm-hyperlink"><a class="url" href="https://arxiv.org/pdf/1910.01108v4.pdf">https://arxiv.org/pdf/1910.01108v4.pdf</a></span>) in 2019. It is trained using a transfer learning technique known as knowledge distillation. The idea is to have a teacher model (i.e., BERT), where a smaller model (i.e., DistilBERT) is trying to mimic the teacher’s output, which becomes the learning objective for DistilBERT. DistilBERT is smaller compared to BERT and has only six layers, as opposed to BERT’s 12. Another key difference of DistilBERT is that it is only trained on the masked language modeling task and not on the next-sentence prediction task. This decision is backed up by several studies that question the contribution made by next-sentence prediction tasks (compared to the masked language modeling task) toward natural language understanding.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1154792"></a>With the tokenizer downloaded, let’s examine the tokenizer and how it transforms the inputs by providing some sample text:</p>
  <pre class="programlisting">context = "This is the context"
question = "This is the question"
 
token_ids = tokenizer(context, question, return_tensors='tf')
print(token_ids)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154798"></a>This returns</p>
  <pre class="programlisting">{'input_ids': &lt;tf.Tensor: shape=(1, 11), dtype=int32, numpy=
array([[ 101, 2023, 2003, 1996, 6123,  102, 2023, 2003, 1996, 3160,  102]],
      dtype=int32)&gt;, 
 'attention_mask': &lt;tf.Tensor: shape=(1, 11), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)&gt;
}</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154805"></a>Next, print the tokens corresponding to the IDs with</p>
  <pre class="programlisting">print(tokenizer.convert_ids_to_tokens(token_ids['input_ids'].numpy()[0]))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154809"></a>This will give</p>
  <pre class="programlisting">['[CLS]', 'this', 'is', 'the', 'context', '[SEP]', 'this', 'is', 'the', 'question', '[SEP]']</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154812"></a>We can now encode all the training and test data we have with the code in the next listing.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1154814"></a>Listing 13.6 Encoding training and test data</p>
  <pre class="programlisting">train_encodings = tokenizer(                                          <span class="fm-combinumeral">❶</span>
    train_contexts, train_questions, truncation=True, padding=True, 
<span class="fm-code-continuation-arrow">➥</span> return_tensors='tf' 
)
print(
    "train_encodings.shape: {}".format(train_encodings["input_ids"].shape)
)
 
test_encodings = tokenizer(
    test_contexts, test_questions, truncation=True, padding=True, 
<span class="fm-code-continuation-arrow">➥</span> return_tensors='tf'                                                <span class="fm-combinumeral">❷</span>
)
print("test_encodings.shape: {}".format(test_encodings["input_ids"].shape))</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1176739"></a><span class="fm-combinumeral">❶</span> Encode train data.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1176760"></a><span class="fm-combinumeral">❷</span> Encode test data.</p>

  <p class="body"><a class="calibre8" id="pgfId-1154829"></a>Note that we are using several arguments when calling the tokenizer. When truncation and padding are enabled (i.e., set to <span class="fm-code-in-text">True</span>), the tokenizer will pad/truncate input sequences as necessary. You can pass an argument to the tokenizer (<span class="fm-code-in-text">model_max_length</span>) during the creation to pad or truncate text to a certain length. If this argument is not given, it will use the default length available to the tokenizer as one of the configurations set when it was pretrained. When padding/truncation is enabled, your inputs will go through one of the following changes:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1154830"></a>If the sequence is shorter than the length, add the special token [PAD] to the end of the sequence until it reaches the length.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1154831"></a>If the sequence is longer than the length, it will be truncated.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1154832"></a>If the sequence has exactly the same length, no change will be introduced.</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1154833"></a>When you run the code in listing 13.6, it prints</p>
  <pre class="programlisting">train_encodings.shape: (87599, 512)
test_encodings.shape: (10570, 512)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154838"></a>We can see that all sequences are either padded or truncated until they reach a length of 512, the sequence length set during the model pretraining. Let’s look at some of the important arguments you need to be aware of when defining a tokenizer with the <span class="fm-code-in-text">transformers</span> library:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1154839"></a><span class="fm-code-in-text">model_max_length</span> (<span class="fm-code-in-text">int</span>, <i class="fm-italics">optional</i>)—The maximum length (in number of tokens) for the inputs to be padded to.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1154840"></a><span class="fm-code-in-text">padding_side</span> (<span class="fm-code-in-text">str</span>, <i class="fm-italics">optional</i>)—To which side the model should apply padding. Can be [<span class="fm-code-in-text">'right'</span>, <span class="fm-code-in-text">'left'</span>]. The default value is picked from the class attribute of the same name.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1154841"></a><span class="fm-code-in-text">model_input_names</span> (<span class="fm-code-in-text">List[string]</span>, <i class="fm-italics">optional</i>)—The list of inputs accepted by the forward pass of the model (e.g., <span class="fm-code-in-text">"token_type_ids"</span> or <span class="fm-code-in-text">"attention_mask"</span>). The default value is picked from the class attribute of the same name.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1154842"></a><span class="fm-code-in-text">bos_token</span> (<span class="fm-code-in-text">str</span> or <span class="fm-code-in-text">tokenizers.AddedToken</span>, <i class="fm-italics">optional</i>)—A special token representing the beginning of a sentence.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1154843"></a><span class="fm-code-in-text">eos_token</span> (<span class="fm-code-in-text">str</span> or <span class="fm-code-in-text">tokenizers.AddedToken</span>, <i class="fm-italics">optional</i>)—A special token representing the end of a sentence.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1154844"></a><span class="fm-code-in-text">unk_token</span> (<span class="fm-code-in-text">str</span> or <span class="fm-code-in-text">tokenizers.AddedToken</span>, <i class="fm-italics">optional</i>)—A special token representing an out-of-vocabulary token. Out-of-vocabulary tokens are important if the model encounters words it has not seen before.</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1154845"></a>Most of these arguments can be safely ignored as we are using a pretrained tokenizer model, which already had these attributes set before the training.</p>

  <p class="body"><a class="calibre8" id="pgfId-1154846"></a>Unfortunately for us, we still have to do a few more things. One important transformation we have to do is to how we represent the start and end of the answer to the model. As I said earlier, we are given the starting and ending character positions of the answer. But our model understands only token-level decomposition, not character-level decomposition. Therefore, we have to find the token positions from the given character positions. Fortunately, the fast tokenizers provide a convenient function: <span class="fm-code-in-text">char_to_token()</span>. Note that this function is only available for fast tokenizers (i.e., <span class="fm-code-in-text">PreTrainedTokenizerFast</span> objects), not the standard tokenizers (i.e., <span class="fm-code-in-text">PreTrainedTokenizer</span> objects). The <span class="fm-code-in-text">char_to_token()</span> function takes in the following arguments:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1154848"></a><span class="fm-code-in-text">batch_or_char_index (int)</span>—The example index in the batch. If the batch has one example, it will be used as the character index we’re interested in converting to a token index.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1154849"></a><span class="fm-code-in-text">char_index (int, optional</span>—If the batch index is provided, this will denote the character index we want to convert to a token index.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1154850"></a><span class="fm-code-in-text">sequence_index (int, optional</span>—If the input has multiple sequences, this denotes which sequence the character/token belongs to.</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1154851"></a>Using this function, we will write the <span class="fm-code-in-text">update_char_to_token_positions_inplace()</span> function<a class="calibre8" id="marker-1154852"></a> to convert the char-based indices to token-based indices (see the next listing)..</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1154854"></a>Listing 13.7 Converting the char indices to token-based indices</p>
  <pre class="programlisting">def update_char_to_token_positions_inplace(encodings, answers):
    start_positions = []
    end_positions = []
    n_updates = 0
    
    for i in range(len(answers)):                                     <span class="fm-combinumeral">❶</span>
        start_positions.append(
            encodings.char_to_token(i, answers[i]['answer_start'])    <span class="fm-combinumeral">❷</span>
        )
        end_positions.append(
            encodings.char_to_token(i, answers[i]['answer_end'] - 1)  <span class="fm-combinumeral">❷</span>
        )
        
        if start_positions[-1] is None or end_positions[-1] is None:
            n_updates += 1                                            <span class="fm-combinumeral">❸</span>
 
        # if start position is None, the answer passage has been truncated
        # In the guide, 
<span class="fm-code-continuation-arrow">➥</span> https:/ /huggingface.co/transformers/custom_datasets.html#qa-squad
        # they set it to model_max_length, but this will result in NaN 
<span class="fm-code-continuation-arrow">➥</span> losses as the last
        # available label is model_max_length-1 (zero-indexed)
        if start_positions[-1] is None:        
            start_positions[-1] = tokenizer.model_max_length -1       <span class="fm-combinumeral">❹</span>
            
        if end_positions[-1] is None:
            end_positions[-1] = tokenizer.model_max_length -1         <span class="fm-combinumeral">❺</span>
            
    print("{}/{} had answers truncated".format(n_updates, len(answers)))
    encodings.update({
        'start_positions': start_positions, 'end_positions': end_positions
    })                                                                <span class="fm-combinumeral">❻</span>
    
 
update_char_to_token_positions_inplace(train_encodings, train_answers)
update_char_to_token_positions_inplace(test_encodings, test_answers)</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1176280"></a><span class="fm-combinumeral">❶</span> Go through all the answers.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1176308"></a><span class="fm-combinumeral">❷</span> Get the token position for both start and end char positions.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1176325"></a><span class="fm-combinumeral">❸</span> Keep track of how many samples were missing answers.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1176342"></a><span class="fm-combinumeral">❹</span> If a starting position was not found, set it to the last available index.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1176359"></a><span class="fm-combinumeral">❺</span> If an ending position was not found, set it to the last available index.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1176376"></a><span class="fm-combinumeral">❻</span> Update the encodings in place.</p>

  <p class="body"><a class="calibre8" id="pgfId-1154897"></a>This will print</p>
  <pre class="programlisting">10/87599 had answers truncated
8/10570 had answers truncated</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154900"></a>In the code in listing 13.7, we go through every answer in the data set and call the <span class="fm-code-in-text">char_to_token()</span> method on every <span class="fm-code-in-text">answer_start</span> and <span class="fm-code-in-text">answer_end</span> element that marks the start and end (char index) of answers, respectively. The new starting and ending token indices of the answers are assigned to new keys, <span class="fm-code-in-text">start_positions</span> and <span class="fm-code-in-text">end_positions</span>. Furthermore, you can see that there is a validation step that checks if the starting or ending indices are <span class="fm-code-in-text">None</span> (i.e., a suitable token ID was not found). This can happen if the answer has been truncated from the context when preprocessing. If that’s the case, we assign the very last index of the sequence as the positions.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1154902"></a>How many rotten eggs?</p>

    <p class="fm-sidebar-text"><a id="pgfId-1154903"></a>You can see that we are printing the number of examples that needed modifications (e.g., needed correction) or were corrupted (truncated answers). This is an important sanity check, as a very high number can indicate problems with data quality or a bug in your data processing workflow. Therefore, always print these numbers and make sure they are low enough to be safely ignored.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1154909"></a>We will now see how we can define a <span class="fm-code-in-text">tf.data</span><a class="calibre8" id="marker-1154906"></a><a class="calibre8" id="marker-1154907"></a> pipeline.</p>

  <p class="fm-head2"><a id="pgfId-1154910"></a>From tokens to the tf.data pipeline</p>

  <p class="body"><a class="calibre8" id="pgfId-1154915"></a>After<a class="calibre8" id="marker-1154912"></a><a class="calibre8" id="marker-1154913"></a><a class="calibre8" id="marker-1154914"></a> all the cleaning and required transformations, our data set is as good as new. All that’s left for us is to create a <span class="fm-code-in-text">tf.data.Dataset</span> from the data. Our data pipeline will be very simple. It will create a training data set, which will be broken into two subsets, training and validation, and batch the data set. Then the test data set will be created and batched. First let’s import TensorFlow:</p>
  <pre class="programlisting">import tensorflow as tf</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154917"></a>Then we will define a generator that will produce the inputs and outputs necessary for our model to be trained. As you can see, our input is a tuple of two items. It has</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1154918"></a>The padded input token IDs (having the shape [&lt;dataset size&gt;, 512])</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1154919"></a>The attention mask (having the shape [&lt;dataset size&gt;, 512])</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1154920"></a>The outputs will be comprised of</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1154921"></a>The starting token positions (having the shape [&lt;dataset size&gt;])</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1154922"></a>The ending token positions (having the shape [&lt;dataset size&gt;]</p>
    </li>
  </ul>
  <pre class="programlisting">def data_gen(input_ids, attention_mask, start_positions, end_positions):
    for inps, attn, start_pos, end_pos in zip(
        input_ids, attention_mask, start_positions, end_positions
    ):
        
        yield (inps, attn), (start_pos, end_pos)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154929"></a>Our data generator returns data in a very specific format. It returns an input tuple and an output tuple. The input tuple has the token IDs (<span class="fm-code-in-text">input_ids</span>) returned by the tokenizer and the attention mask (<span class="fm-code-in-text">attention_mask</span>), in that order. The output tuple has the answer’s start positions (<span class="fm-code-in-text">start_positions</span>) and end positions (<span class="fm-code-in-text">end_positions</span>) of all the inputs.</p>

  <p class="body"><a class="calibre8" id="pgfId-1154930"></a>We must define our data generator as a callable (i.e., it returns a function, not the generator object). This is a requirement for the <span class="fm-code-in-text">tf.data.Dataset</span> we will be defining. To get a callable function from the generator we defined earlier, let’s use the <span class="fm-code-in-text">partial()</span> function<a class="calibre8" id="marker-1154931"></a>,. <span class="fm-code-in-text">partial()</span> function takes in a callable, optional keyword argument of the callable and returns a partially populated callable, to which you only need to provide the missing arguments (i.e., arguments not specified during the partial call):</p>
  <pre class="programlisting">from functools import partial
 
train_data_gen = partial(
    data_gen,
    input_ids=train_encodings['input_ids'],
    attention_mask=train_encodings['attention_mask'],
    start_positions=train_encodings['start_positions'],  
    end_positions=train_encodings['end_positions']
)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154942"></a><span class="fm-code-in-text">train_data_gen</span> can be treated as a function that has no arguments, as we have already provided all arguments in the partial call. As we have defined our data in the form of a generator, we can use the <span class="fm-code-in-text">tf.data.Dataset.from_generator()</span> function<a class="calibre8" id="marker-1154943"></a> to generate the data. Remember that we have to pass the <span class="fm-code-in-text">output_types</span> argument when defining the data through a generator. All our outputs are <span class="fm-code-in-text">int32</span> type<a class="calibre8" id="marker-1154945"></a>. But they come as a pair of tuples:</p>
  <pre class="programlisting">train_dataset = tf.data.Dataset.from_generator(
    train_data_gen, output_types=(('int32', 'int32'), ('int32', 'int32'))
)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154951"></a>Next, we shuffle the data to make sure there’s no order. Make sure you pass the <span class="fm-code-in-text">buffer_size</span> argument that specifies how many samples are brought into memory for the shuffle. We will set the <span class="fm-code-in-text">buffer_size</span> to 20,000 as we plan to use 10,000 samples from that as validation samples:</p>
  <pre class="programlisting">train_dataset = train_dataset.shuffle(20000)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154956"></a>It’s time to split the <span class="fm-code-in-text">train_dataset</span> into training and validation data, as we will be using the original validation data subset as the testing data. To split the <span class="fm-code-in-text">train_dataset</span> into training and validation subsets, we will take the following approach. After the shuffle, define the first 10,000 samples as the valid data set. We will be batching the data using the <span class="fm-code-in-text">tf.data.Dataset.batch()</span> function<a class="calibre8" id="marker-1154957"></a> with a batch size of 8:</p>
  <pre class="programlisting">valid_dataset = train_dataset.take(10000)
valid_dataset = valid_dataset.batch(8)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154962"></a>Skip the first 10,000 datapoints, as they belong to the validation set, and have the rest as the <span class="fm-code-in-text">train_dataset</span>. We will be batching the data using the <span class="fm-code-in-text">tf.data.Dataset.batch()</span> function with a batch size of 8:</p>
  <pre class="programlisting">train_dataset = train_dataset.skip(10000)
train_dataset = train_dataset.batch(8)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154967"></a>Finally, the test data is defined using the same data generator:</p>
  <pre class="programlisting">test_data_gen = partial(data_gen,
    input_ids=test_encodings['input_ids'], 
    attention_mask=test_encodings['attention_mask'],
    start_positions=test_encodings['start_positions'], 
    end_positions=test_encodings['end_positions']
)
test_dataset = tf.data.Dataset.from_generator(
    test_data_gen, output_types=(('int32', 'int32'), ('int32', 'int32'))
)
test_dataset = test_dataset.batch(8)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154985"></a>Next we will work through defining<a class="calibre8" id="marker-1154979"></a><a class="calibre8" id="marker-1154980"></a><a class="calibre8" id="marker-1154981"></a> the<a class="calibre8" id="marker-1154982"></a><a class="calibre8" id="marker-1154983"></a><a class="calibre8" id="marker-1154984"></a> model<a class="calibre8" id="marker-1174889"></a>.</p>

  <h3 class="fm-head1" id="sigil_toc_id_176"><a id="pgfId-1154986"></a>13.3.3 Defining the DistilBERT model</h3>

  <p class="body"><a class="calibre8" id="pgfId-1154992"></a>We<a class="calibre8" id="marker-1154987"></a><a class="calibre8" id="marker-1154988"></a><a class="calibre8" id="marker-1154989"></a><a class="calibre8" id="marker-1154990"></a><a class="calibre8" id="marker-1154991"></a> have looked closely at the data, cleaned and processed it using a tokenizer, and defined a <span class="fm-code-in-text">tf.data.Dataset</span> to quickly retrieve batches of examples in the format our model will accept. It’s time to define the model. To define the model, we will import the following module:</p>
  <pre class="programlisting">from transformers import TFDistilBertForQuestionAnswering</pre>

  <p class="body"><a class="calibre8" id="pgfId-1154996"></a>The <span class="fm-code-in-text">transformers</span> library provides you with a remarkable selection of ready-made model templates that you can download and train on the task. In other words, you don’t have to fiddle around with the library to find out how to plug downstream models on top of the pretrained transformers. For example, we are solving a question-answering problem and we want to use the DistilBERT model for that. The <span class="fm-code-in-text">transformers</span> library has the built-in question-answering adaptation of the DistilBERT model. All you need to do is import the module and call the <span class="fm-code-in-text">from_pretrained()</span> function<a class="calibre8" id="marker-1154997"></a> with the model tag to download it:</p>
  <pre class="programlisting">model = TFDistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")</pre>

  <p class="body"><a class="calibre8" id="pgfId-1155001"></a>This will download the model and save it on your local computer.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1155002"></a>What other ready-to-use models does the Transformers library provide?</p>

    <p class="fm-sidebar-text"><a id="pgfId-1155003"></a>You can a look at <span class="fm-hyperlink"><a class="url" href="http://mng.bz/8Mwz">http://mng.bz/8Mwz</a></span> to get an idea about what you can and cannot easily do with the DistilBERT model. <span class="fm-code-in-text1">transformers</span> is a fully fledged library that you can use to solve almost all the common NLP tasks with Transformer models. Here we will look at the options for the DistilBERT model.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1155004"></a><b class="fm-bold">TFDistilBertForMaskedLM</b></p>

    <p class="fm-sidebar-text"><a id="pgfId-1155005"></a>This allows you to pretrain the DistilBERT model using the masked language modeling task. In the masked language modeling task, in a given corpus of text, words are masked at random and the model is asked to predict the masked words.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1155006"></a><b class="fm-bold">TFDistilBertForSequenceClassification</b></p>

    <p class="fm-sidebar-text"><a id="pgfId-1155007"></a>If your problem has a single input sequence and you want to predict a label for the input (e.g., spam classification, sentiment analysis, etc.), you can use this model to train the model end to end.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1155008"></a><b class="fm-bold">TFDistilBertForMultipleChoice</b></p>

    <p class="fm-sidebar-text"><a id="pgfId-1155009"></a>DistilBERT can be used to solve multiple-choice problems with this variant. The input consists of a question and several answers. These are typically combined into a single sequence (i.e., [CLS] [Question] [SEP] [Answer 1] [SEP] [Answer 2] [SEP], etc.), and the model is asked to predict the best answer to the question, which is typically done by feeding the representation for the [CLS] token to a classification layer that will predict the index of the correct answer (e.g., first answer, second answer, etc.).</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1155010"></a><b class="fm-bold">TFDistilBertForTokenClassification</b></p>

    <p class="fm-sidebar-text"><a id="pgfId-1155011"></a>Tasks like named-entity recognition or part-of-speech tagging require the model to predict a label (e.g., person, organization, etc.) for every token in the input sequence. For such tasks, this type of model can be used simply.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1155012"></a><b class="fm-bold">TFDistilBertForQuestionAnswering</b></p>

    <p class="fm-sidebar-text"><a id="pgfId-1155013"></a>This is the model we will use in our scenario. We have a question and a context, and the model needs to predict the answer (or the starting/ending positions of the answer in the context). Such problems can be solved using this module.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1179445"></a>Table 13.1 is a summary of the models in this sidebar and their usage.</p>

    <p class="fm-table-caption">Table 13.1 Different models in Hugging Face’s <span class="fm-code-in-text">transformers</span> library and their usage</p>

    <p class="fm-figure"><img alt="13_table2" class="calibre10" src="../../OEBPS/Images/13_table2.png" width="751" height="638"/><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1155056"></a>The blog <span class="fm-hyperlink"><a class="url" href="http://jalammar.github.io/illustrated-bert/">http://jalammar.github.io/illustrated-bert/</a></span> provides a very informative diagram of how BERT-like models can be used for different tasks (as outlined under the “Task-specific models” section).</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1155057"></a>The training and evaluation of these models will be very similar to how you would use them if they were Keras models. You can call <span class="fm-code-in-text1">model.fit()</span>, <span class="fm-code-in-text1">model.predict()</span> or <span class="fm-code-in-text1">model.evaluate()</span> on these models given that the data is in the correct format the model expects.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1155058"></a>To train these models, you also have the ability to use an advanced Trainer (<span class="fm-hyperlink"><a class="url" href="http://mng.bz/EWZd">http://mng.bz/EWZd</a></span>). We will discuss the Trainer object in more detail later.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1155060"></a>The next part of this came from a wakeup call that I had while using the library. Typically, once the model is defined, you can use it just like a Keras model. This means you can call <span class="fm-code-in-text">model.fit()</span> with a <span class="fm-code-in-text">tf.data.Dataset</span> and train the model. TensorFlow and Keras, when training models, expect the model output to be a tensor or a tuple of tensors. However, Transformer models’ outputs are specific objects (a descendant of the <span class="fm-code-in-text">transformers.file_utils.ModelOutput</span> object), as outlined in <span class="fm-hyperlink"><a class="url" href="http://mng.bz/N6Rn">http://mng.bz/N6Rn</a></span>. This will throw an error like</p>
  <pre class="programlisting">TypeError: The two structures don't have the same sequence type. 
Input structure has type &lt;class 'tuple'&gt;, while shallow structure has type 
&lt;class 'transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput'&gt;.</pre>

  <p class="body"><a class="calibre8" id="pgfId-1155064"></a>To fix this, the <span class="fm-code-in-text">transformers</span> library allows you to set a certain configuration called <span class="fm-code-in-text">return_dict</span> and make sure the model returns a tuple, not an object. Then we define a <span class="fm-code-in-text">Config</span> object that has <span class="fm-code-in-text">return_dict=False</span> and override the default config of the model with the new config object. For example, for the DistilBERT model, this can be done with</p>
  <pre class="programlisting">from transformers import DistilBertConfig, TFDistilBertForQuestionAnswering
 
config = DistilBertConfig.from_pretrained(
    "distilbert-base-uncased", return_dict=False
)
model = TFDistilBertForQuestionAnswering.from_pretrained(
    "distilbert-base-uncased", config=config
)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1155075"></a>Unfortunately, I was not able to get the model to behave in the way I expected it to by using this configuration. This goes to show that when writing code, you need to anticipate things that can go wrong even when using top-notch libraries. The easiest fix would be to let the <span class="fm-code-in-text">transformers</span> library output an <span class="fm-code-in-text">ModelOutput</span> object<a class="calibre8" id="marker-1155076"></a> and write a wrapper function that will extract the required outputs of that object and create a <span class="fm-code-in-text">tf.keras.Model</span> from those outputs. The function in the following listing does that for us.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1155078"></a>Listing 13.8 Wrapping the model in a <span class="fm-code-in-listingcaption">tf.keras.models.Model</span> object to prevent errors</p>
  <pre class="programlisting">def tf_wrap_model(model):
    """ Wraps the huggingface's model with in the Keras Functional API """
    
    # Define inputs
    input_ids = tf.keras.layers.Input(
        [None,], dtype=tf.int32, name="input_ids"
    )                                                  <span class="fm-combinumeral">❶</span>
    attention_mask = tf.keras.layers.Input(
      [None,], dtype=tf.int32, name="attention_mask"   <span class="fm-combinumeral">❷</span>
    )
    
   out = model([input_ids, attention_mask])            <span class="fm-combinumeral">❸</span>
    
   wrap_model = tf.keras.models.Model(
     [input_ids, attention_mask], 
     outputs=(out.start_logits, out.end_logits)        <span class="fm-combinumeral">❹</span>
   )
    
   return wrap_model</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1176008"></a><span class="fm-combinumeral">❶</span> Define an input layer that will take a batch of a token sequence.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1176029"></a><span class="fm-combinumeral">❷</span> Define an input for the attention mask returned when encoding with the tokenizer.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1176053"></a><span class="fm-combinumeral">❸</span> Get the model output given the input_ids and the attention_mask.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1176070"></a><span class="fm-combinumeral">❹</span> Define a wrapper model that takes the defined inputs as the inputs and output logits of the start and end indices’ prediction layers.</p>

  <p class="body"><a class="calibre8" id="pgfId-1155103"></a>You can generate the model that generates the corrected outputs by calling the function in listing 13.8:</p>
  <pre class="programlisting">model_v2 = tf_wrap_model(model)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1155107"></a>Finally, compile the model with a loss function (sparse categorical cross-entropy, as we are using integer labels), metric(s) (sparse accuracy), and an optimizer (Adam optimizer):</p>
  <pre class="programlisting">loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
acc = tf.keras.metrics.SparseCategoricalAccuracy()
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)
 
model_v2.compile(optimizer=optimizer, loss=loss, metrics=[acc])</pre>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1155114"></a>Transformers in vision</p>

    <p class="fm-sidebar-text"><a id="pgfId-1155115"></a>Transformer models typically thrive in the NLP domain. Until recently, little effort was put into understanding their place in the computer vision domain. Then came several notable papers from Google and Facebook AI that investigated how Transformer models could be used in the computer vision domain.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1155116"></a><b class="fm-bold">Vision Transformer (ViT)</b></p>

    <p class="fm-sidebar-text"><a id="pgfId-1155117"></a>First, the paper “An Image Is Worth 16X16 Words: Transformers for Image Recognition at Scale” by Dosovitskiy et al. was published in October 2020 (<span class="fm-hyperlink"><a class="url" href="https://arxiv.org/pdf/2010.11929.pdf">https://arxiv.org/pdf/2010.11929.pdf</a></span>). This can be considered the first substantial step toward vision transformers. In this paper, the authors adapt the original Transformer model proposed for the NLP domain to a computer vision with minimal changes to the architecture. This model is called the Vision Transformer (ViT).</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1155118"></a>The idea is to decompose an image to small patches of 16 × 16 and consider each as a separate token. Each image path is flattened to a 1D vector, and its position is encoded by a positional-encoding mechanism similar to the original Transformer. It is important to note that the positional encoding in the original Transformer is 1D. However, images are 2D. The authors argue that a 1D positional encoding was adequate and there was no major performance difference between 1D and 2D positional encoding. Once the image is broken into patches of 16 × 16 and flattened, each image can be presented as a sequence of tokens, just like a textual input sequence.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1179560"></a>Then the model is pretrained in a self-supervised fashion, using a vision data set called JFT-300M (<span class="fm-hyperlink"><a class="url" href="https://paperswithcode.com/dataset/jft-300m">https://paperswithcode.com/dataset/jft-300m</a></span>). It is not trivial to formulate a self-supervised task in vision as it is in NLP. In NLP, you can form an objective simply to predict masked tokens in a text sequence. However, in the context of computer vision, a token is a sequence of continuous values (normalized pixel values). Therefore, ViT is pretrained to predict the mean three-bit RGB color of a given image patch. Each channel (i.e., red, green, and blue) is represented with three bits (i.e., each bit having the value of 0 or 1), which gives 512 possibilities or classes. In <a id="pgfId-1155119"></a>other words, for a given image, patches are masked randomly (using the same approach as BERT), and the model is asked to predict the mean three-bit RGB color of that image patch.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1155120"></a>After pretraining, the model can be fine-tuned for a task-specific problem by fitting a classification or a regression head on top of ViT, just like BERT. ViT also has the [CLS] token at the beginning of the sequence, which will be used as the input representation for downstream vision models that are plugged on top of ViT. The following figure illustrates the mechanics of ViT.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1155121"></a>The original code for ViT is written with a framework known as Jax (<span class="fm-hyperlink"><a class="url" href="https://github.com/google-research/vision_transformer">https://github .com/google-research/vision_transformer</a></span>). However, there are several third-party TensorFlow wrappers for the model (e.g., <span class="fm-hyperlink"><a class="url" href="https://github.com/emla2805/vision-transformer">https://github.com/emla2805/vision-transformer</a></span>). If using a third-party version, make sure you read the code and verify the correctness as the third party is not affiliated with the original research team.</p>

    <p class="fm-figure"><img alt="13-09-unnumb-1" class="calibre10" src="../../OEBPS/Images/13-09-unnumb-1.png" width="1069" height="1106"/><br class="calibre2"/></p>

    <p class="fm-figure-caption"><a id="pgfId-1180819"></a>The Vision Transformer (ViT) model architecture and how it is used for a downstream image classification task</p>

    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1155129"></a>Unified Transformer (UniT)</p>

    <p class="fm-sidebar-text"><a id="pgfId-1155130"></a>Then came an even more groundbreaking paper from Facebook AI called “Transformer Is All You Need: Multimodal Multitask Learning with a Unified Transformer” by Hu et al. (<span class="fm-hyperlink"><a class="url" href="https://arxiv.org/pdf/2102.10772.pdf">https://arxiv.org/pdf/2102.10772.pdf</a></span>). The model is called the Unified Transformer (UniT). UniT can perform a plethora of tasks in both computer vision and NLP domains, just by changing the classification head.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1155131"></a>The model, even though it is complex, is straightforward. There are three Transformer models. One Transformer encodes the image input (if present) to generate an encoded representation of an image. The next Transformer encodes the text input (if present). Finally, another Transformer takes a task index as the input, gets the embedding, and passes it to a cross self-attention layer that takes the concatenated image and text encoding as the query and key and the task embedding (after passing through a self-attention layer) as the value. This is similar to how the Transformer decoder, in its encoder-decoder attention layer, uses the last encoder output to generate the query and key and uses the decoder’s input as the value. This model is depicted in the figure on the next page.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1155138"></a>UniT is evaluated on seven tasks involving eight data sets. These tasks include object detection, visual question answering, object annotation, and four language-only tasks. The four language-only tasks are from the GLUE benchmarking data sets, which include the following:</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1155139"></a><b class="fm-bold">Tasks</b></p>

    <p class="fm-sidebar-text"><a id="pgfId-1155140"></a><i class="fm-italics">Object detection</i>—The model predicts the rectangular coordinates of objects present in images (data sets COCO and Visual Genome Detection [VGD]).</p>

    <p class="fm-sidebar-text"><a id="pgfId-1155141"></a><i class="fm-italics">Visual question answering</i> (VQAv2)—An image and a question is given, and then the model predicts the answer to the question, which can be found in the image (data set: VQAv2).</p>

    <p class="fm-sidebar-text"><a id="pgfId-1155142"></a><i class="fm-italics">Visual entailment task</i>—A visual entailment task where an image and a text sequence are given and the model predicts whether the sentence semantically entails the image (SNLI-VE).</p>

    <p class="fm-sidebar-text"><a id="pgfId-1155143"></a><i class="fm-italics">Question Natural Language Inference</i> (QNLI)—Question answering by extracting the answer from a given context.</p>

    <p class="fm-sidebar-text"><a id="pgfId-1155144"></a><i class="fm-italics">Quora Question Pairs</i> (QQP)—Identify duplicate questions from a given pair of questions.</p>

    <p class="fm-sidebar-text"><a id="pgfId-1155145"></a><i class="fm-italics">Textual entailment</i>—Textual entailment focuses on predicting if sentence A entails/ contradicts/is neutral to sentence B. (data set MNLI).</p>

    <p class="fm-sidebar-text"><a id="pgfId-1155146"></a><i class="fm-italics">Sentiment analysis</i>—Predicts a sentiment (positive/negative/is neutral) for a given review/text (data set Stanford Sentiment Treebank [SST-2]).</p>

    <p class="fm-figure"><img alt="13-09-unnumb-2" class="calibre10" src="../../OEBPS/Images/13-09-unnumb-2.png" width="1025" height="1056"/><br class="calibre2"/></p>

    <p class="fm-figure-caption"><a id="pgfId-1180851"></a>The overall architecture of UniT. The model consists of three components: an image encoder, a text encoder, and a task decoder. Finally, there are several classification heads fitted on top of the task decoder.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1155153"></a>Next we will train the model we just<a class="calibre8" id="marker-1170889"></a><a class="calibre8" id="marker-1170890"></a><a class="calibre8" id="marker-1170891"></a><a class="calibre8" id="marker-1170892"></a><a class="calibre8" id="marker-1170893"></a> defined.</p>

  <h3 class="fm-head1" id="sigil_toc_id_177"><a id="pgfId-1155154"></a>13.3.4 Training the model</h3>

  <p class="body"><a class="calibre8" id="pgfId-1155159"></a>We<a class="calibre8" id="marker-1155155"></a><a class="calibre8" id="marker-1155156"></a><a class="calibre8" id="marker-1155157"></a><a class="calibre8" id="marker-1155158"></a> have been patiently chipping away at this problem to arrive at this moment. Finally, we can train the model we just defined with the <span class="fm-code-in-text">train_dataset</span> data set we created earlier and use the <span class="fm-code-in-text">valid_dataset</span> to monitor the accuracy of the model:</p>
  <pre class="programlisting">model_v2.fit(
    train_dataset, 
    validation_data=valid_dataset,    
    epochs=3
)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1155166"></a>This will print the following output:</p>
  <pre class="programlisting">Epoch 1/3
<b class="fm-code-bold">WARNING:tensorflow:The parameters `output_attentions`, </b>
<span class="fm-code-continuation-arrow">➥</span> <b class="fm-code-bold">`output_hidden_states` and `use_cache` cannot be updated when calling a </b>
<span class="fm-code-continuation-arrow">➥</span> <b class="fm-code-bold">model.They have to be set to True/False in the config object (i.e.: </b>
<span class="fm-code-continuation-arrow">➥</span> <b class="fm-code-bold">`config=XConfig.from_pretrained('name', output_attentions=True)`).</b>
 
<b class="fm-code-bold">WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode </b>
<span class="fm-code-continuation-arrow">➥</span> <b class="fm-code-bold">and will always be set to `True`.</b>
 
9700/9700 [==============================] - 3308s 340ms/step - loss: 
<span class="fm-code-continuation-arrow">➥</span> 4.3138 - tf_distil_bert_for_question_answering_loss: 2.2146 - 
<span class="fm-code-continuation-arrow">➥</span> tf_distil_bert_for_question_answering_1_loss: 2.0992 - 
<span class="fm-code-continuation-arrow">➥</span> tf_distil_bert_for_question_answering_sparse_categorical_accuracy: 
<span class="fm-code-continuation-arrow">➥</span> 0.4180 - 
<span class="fm-code-continuation-arrow">➥</span> tf_distil_bert_for_question_answering_1_sparse_categorical_accuracy: 
<span class="fm-code-continuation-arrow">➥</span> 0.4487 - val_loss: 2.3849 - 
<span class="fm-code-continuation-arrow">➥</span> val_tf_distil_bert_for_question_answering_loss: 1.2053 - 
<span class="fm-code-continuation-arrow">➥</span> val_tf_distil_bert_for_question_answering_1_loss: 1.1796 - 
<span class="fm-code-continuation-arrow">➥</span> val_tf_distil_bert_for_question_answering_sparse_categorical_accuracy: 
<span class="fm-code-continuation-arrow">➥</span> 0.6681 - 
<span class="fm-code-continuation-arrow">➥</span> val_tf_distil_bert_for_question_answering_1_sparse_categorical_accuracy
<span class="fm-code-continuation-arrow">➥</span> : 0.6909
 
...
 
Epoch 3/3
9700/9700 [==============================] - 3293s 339ms/step - loss: 
<span class="fm-code-continuation-arrow">➥</span> 1.6349 - tf_distil_bert_for_question_answering_loss: 0.8647 - 
<span class="fm-code-continuation-arrow">➥</span> tf_distil_bert_for_question_answering_1_loss: 0.7703 - 
<span class="fm-code-continuation-arrow">➥</span> tf_distil_bert_for_question_answering_sparse_categorical_accuracy: 
<span class="fm-code-continuation-arrow">➥</span> 0.7294 - 
<span class="fm-code-continuation-arrow">➥</span> tf_distil_bert_for_question_answering_1_sparse_categorical_accuracy: 
<span class="fm-code-continuation-arrow">➥</span> 0.7672 - val_loss: 2.4049 - 
<span class="fm-code-continuation-arrow">➥</span> val_tf_distil_bert_for_question_answering_loss: 1.2048 - 
<span class="fm-code-continuation-arrow">➥</span> val_tf_distil_bert_for_question_answering_1_loss: 1.2001 - 
<span class="fm-code-continuation-arrow">➥</span> val_tf_distil_bert_for_question_answering_sparse_categorical_accuracy: 
<span class="fm-code-continuation-arrow">➥</span> 0.6975 - 
<span class="fm-code-continuation-arrow">➥</span> val_tf_distil_bert_for_question_answering_1_sparse_categorical_accuracy
<span class="fm-code-continuation-arrow">➥</span> : 0.7200</pre>

  <p class="body"><a class="calibre8" id="pgfId-1155179"></a>The training updates are quite long, so let’s break them down a bit. There are two losses:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1155180"></a><span class="fm-code-in-text">tf_distil_bert_for_question_answering_loss</span>—Measures the loss on the starting index prediction head</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1155181"></a><span class="fm-code-in-text">tf_distil_bert_for_question_answering_1_loss</span>—Measures the loss on the ending index prediction head</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1155182"></a>As mentioned earlier, for question answering, we have two classification heads: one to predict the starting index and one to predict the ending index. We have something similar for the accuracy. There are two accuracies to measure the performance of individual heads:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1155183"></a><span class="fm-code-in-text">tf_distil_bert_for_question_answering_sparse_categorical_accuracy</span>—Measures the accuracy of the classification head predicting the starting index</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1155184"></a><span class="fm-code-in-text">tf_distil_bert_for_question_answering_1_sparse_categorical_accuracy</span>—Measures the accuracy of the classification head predicting the ending index</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1155185"></a>We can see that the model has achieved a training accuracy of around 77% for both starting and ending index prediction, where the validation accuracy is around 70% and 72% for starting/ending index prediction, respectively. These are good accuracies given that we only trained the model for three epochs.</p>

  <p class="fm-callout"><a id="pgfId-1155186"></a><span class="fm-callout-head">NOTE</span> On an Intel Core i5 machine with an NVIDIA GeForce RTX 2070 8 GB, the training took approximately 2 hours and 45 minutes to run three epochs.</p>

  <p class="body"><a class="calibre8" id="pgfId-1155187"></a>You can see that there are several warnings produced during the model training. Since this is a new library, it is extremely important to pay attention to these warnings to make sure they make sense in the context we’re using these models. You don’t need to worry about these as you would if you saw an error, as warnings do not always indicate a problem. Depending on the problem we’re solving, some warnings are not applicable and can be safely ignored. The first warning says that the parameters <span class="fm-code-in-text">output_attentions</span>, <span class="fm-code-in-text">output_hidden_states</span> and <span class="fm-code-in-text">use_cache</span> cannot be updated when calling the model but need to be passed in as config objects. We are not worried about this as we are not interested in introducing any custom modifications to the model, and we are using one that is already designed for question answering.</p>

  <p class="body"><a class="calibre8" id="pgfId-1155188"></a>The second warning says that <span class="fm-code-in-text">return_dict</span> will always be set to <span class="fm-code-in-text">TRUE</span>. Setting <span class="fm-code-in-text">return_dict=True</span> means that the Transformer model will return a <span class="fm-code-in-text">ModelOutput</span> object that is not understood by TensorFlow or Keras. This creates problems down the road when we want to use the model with a Keras API. This is one of the reasons we create the <span class="fm-code-in-text">tf_wrap_model()</span> function<a class="calibre8" id="marker-1170262"></a>: to make sure we get a <span class="fm-code-in-text">tf.keras.Model</span> that always outputs a tuple and not a <span class="fm-code-in-text">ModelOutput</span> object.</p>

  <p class="body"><a class="calibre8" id="pgfId-1155190"></a>Finally, we will save the model:</p>
  <pre class="programlisting">import os
 
# Create folders
if not os.path.exists('models'):
    os.makedirs('models')
if not os.path.exists('tokenizers'):
    os.makedirs('tokenizers')
 
tokenizer.save_pretrained(os.path.join('tokenizers', 'distilbert_qa'))
 
model_v2.get_layer(
    "tf_distil_bert_for_question_answering").save_pretrained(
        os.path.join('models', 'distilbert_qa')
    )
)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1155207"></a>Make sure to save both the tokenizer and the model. To save the tokenizer, you can simply call the <span class="fm-code-in-text">save_pretrained()</span> function<a class="calibre8" id="marker-1155208"></a> and provide it a folder path. The tokenizer will be saved in that directory. Saving the model requires a bit more work. We will not be able to save the model (model_v2) as is, because when your model has a custom layer, to be saved correctly, the layer needs to implement the <span class="fm-code-in-text">get_config()</span> function and specify all the attributes of that layer. However, doing this for the Transformer model, which is there as a custom layer, would be extremely difficult. Therefore, we will only save the Transformer model by calling the <span class="fm-code-in-text">model_v2.get_layer()</span> function with the layer name (i.e., <span class="fm-code-in-text">tf_distil_bert_for_question_answering</span>) and then calling the <span class="fm-code-in-text">save_pretrained()</span> method with a folder path. Any time we have to build the full model, we can simply call the <span class="fm-code-in-text">tf_wrap_model()</span> function on that saved<a class="calibre8" id="marker-1155211"></a><a class="calibre8" id="marker-1155212"></a><a class="calibre8" id="marker-1155213"></a><a class="calibre8" id="marker-1155214"></a> model.</p>

  <h3 class="fm-head1" id="sigil_toc_id_178"><a id="pgfId-1155215"></a><a id="marker-1174760"></a><a id="marker-1174846"></a><a id="marker-1174931"></a>13.3.5 Ask BERT a question</h3>

  <p class="body"><a class="calibre8" id="pgfId-1155221"></a>Evaluating<a class="calibre8" id="marker-1155216"></a><a class="calibre8" id="marker-1155217"></a><a class="calibre8" id="marker-1155218"></a><a class="calibre8" id="marker-1155219"></a> the model is also a matter of calling the <span class="fm-code-in-text">model_v2.evaluate()</span> function with the <span class="fm-code-in-text">test_dataset</span> we created earlier:</p>
  <pre class="programlisting">model_v2.evaluate(test_dataset)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1155223"></a>This will print</p>
  <pre class="programlisting">1322/1322 [==============================] - 166s 126ms/step - loss: 2.4756 
<span class="fm-code-continuation-arrow">➥</span> - tf_distil_bert_for_question_answering_loss: 1.2702 - 
<span class="fm-code-continuation-arrow">➥</span> tf_distil_bert_for_question_answering_1_loss: 1.2054 - 
<span class="fm-code-continuation-arrow">➥</span> tf_distil_bert_for_question_answering_sparse_categorical_accuracy: 
<span class="fm-code-continuation-arrow">➥</span> 0.6577 - 
<span class="fm-code-continuation-arrow">➥</span> tf_distil_bert_for_question_answering_1_sparse_categorical_accuracy: 
<span class="fm-code-continuation-arrow">➥</span> 0.6942</pre>

  <p class="body"><a class="calibre8" id="pgfId-1155225"></a>This is wonderful news! We have achieved around 65.7% accuracy on predicting the start index of the answer, where the model can predict the end index with approximately 69.4% accuracy. Two things to observe are that both starting and ending accuracies are similar, meaning that the model gets the answer correct (start to end) around that accuracy. Finally, this accuracy falls close to the validation accuracy, meaning that we don’t have unusual overfitting occurring.</p>

  <p class="body"><a class="calibre8" id="pgfId-1155226"></a>As I have alluded to numerous times, it is usually not enough to look at a number alone to judge a model on its performance. Visual inspection has always been a natural tendency for humans when evaluating an object. Therefore, as a scrupulous data scientist or a machine learning engineer, it should be a part of the machine learning workflow whenever possible. In the next listing, we will provide our model a question from the test set and see what the model produces.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1155228"></a>Listing 13.9 Inferring the textual answer from the model for a given question</p>
  <pre class="programlisting">i = 5
 
sample_q = test_questions[i]                                          <span class="fm-combinumeral">❶</span>
sample_c = test_contexts[i]                                           <span class="fm-combinumeral">❶</span>
sample_a = test_answers[i]                                            <span class="fm-combinumeral">❶</span>
sample_input = (
    test_encodings["input_ids"][i:i+1], 
    test_encodings["attention_mask"][i:i+1]
)
 
def ask_bert(sample_input, tokenizer):
    
    out = model_v2.predict(sample_input)                              <span class="fm-combinumeral">❷</span>
    pred_ans_start = tf.argmax(out[0][0])                             <span class="fm-combinumeral">❸</span>
    pred_ans_end = tf.argmax(out[1][0])                               <span class="fm-combinumeral">❹</span>
    print(
        "{}-{} token ids contain the answer".format(
            pred_ans_start, pred_ans_end
        )
    )
    ans_tokens = sample_input[0][0][pred_ans_start:pred_ans_end+1]    <span class="fm-combinumeral">❺</span>
    
    return " ".join(tokenizer.convert_ids_to_tokens(ans_tokens))      <span class="fm-combinumeral">❻</span>
 
print("Question")                                                     <span class="fm-combinumeral">❼</span>
print("\t", sample_q, "\n")                                           <span class="fm-combinumeral">❼</span>
print("Context")                                                      <span class="fm-combinumeral">❼</span>
print("\t", sample_c, "\n")                                           <span class="fm-combinumeral">❼</span>
print("Answer (char indexed)")                                        <span class="fm-combinumeral">❼</span>
print("\t", sample_a, "\n")                                           <span class="fm-combinumeral">❼</span>
print('='*50,'\n')
 
sample_pred_ans = ask_bert(sample_input, tokenizer)                   <span class="fm-combinumeral">❽</span>
 
print("Answer (predicted)")                                           <span class="fm-combinumeral">❾</span>
print(sample_pred_ans)                                                <span class="fm-combinumeral">❾</span>
print('='*50,'\n')</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1174982"></a><span class="fm-combinumeral">❶</span> Define a sample question, context, and answer.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1175012"></a><span class="fm-combinumeral">❷</span> Predict with the model for the sample input. This returns the starting and ending index probability vectors.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1175029"></a><span class="fm-combinumeral">❸</span> Get the predicted starting index by getting the maximum index from the starting index probability vector.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1175046"></a><span class="fm-combinumeral">❹</span> Get the predicted ending index by getting the maximum index from the ending index probability vector.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1175063"></a><span class="fm-combinumeral">❺</span> Get the string answer tokens by getting the text between the starting/ending indices.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1175080"></a><span class="fm-combinumeral">❻</span> Return the list of tokens as a single string.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1175097"></a><span class="fm-combinumeral">❼</span> Print the inputs to the model.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1175114"></a><span class="fm-combinumeral">❽</span> Call the ask_bert function on the defined input.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1174983"></a><span class="fm-combinumeral">❾</span> Print the answer.</p>

  <p class="body"><a class="calibre8" id="pgfId-1155276"></a>Let’s flesh out the details of the code in listing 13.9. First, we define an index <span class="fm-code-in-text">i</span>. This index will be used to retrieve a sample from the test set. <span class="fm-code-in-text">sample_q</span>, <span class="fm-code-in-text">sample_c</span>, and <span class="fm-code-in-text">sample_a</span> represent the question, context, and answer of the sample we have selected. With these, we can define the <span class="fm-code-in-text">sample_input</span>, which will contain the encoded representation of the input understood by the model. The function <span class="fm-code-in-text">ask_bert()</span> takes in an input prepared for the model with the tokenizer (denoted by the <span class="fm-code-in-text">sample_input</span>) to convert token IDs of the answer back to readable tokens. The function first predicts the output for the input and gets the starting and ending token IDs of the answer. Finally, the function converts these IDs along with the words in between into a single comprehensible answer and returns the text. If you print the output of this process, you will get the following:</p>
  <pre class="programlisting">Question
     What was the theme of Super Bowl 50? 
 
Context
     Super Bowl 50 was an American football game to determine the 
<span class="fm-code-continuation-arrow">➥</span> champion of the National Football League (NFL) for the 2015 season. The 
<span class="fm-code-continuation-arrow">➥</span> American Football Conference (AFC) champion Denver Broncos defeated the 
<span class="fm-code-continuation-arrow">➥</span> National Football Conference (NFC) champion Carolina Panthers 24-10 to 
<span class="fm-code-continuation-arrow">➥</span> earn their third Super Bowl title. The game was played on February 7, 
<span class="fm-code-continuation-arrow">➥</span> 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, 
<span class="fm-code-continuation-arrow">➥</span> California. As this was the 50th Super Bowl, the league emphasized the 
<span class="fm-code-continuation-arrow">➥</span> "golden anniversary" with various gold-themed initiatives, as well as 
<span class="fm-code-continuation-arrow">➥</span> temporarily suspending the tradition of naming each Super Bowl game 
<span class="fm-code-continuation-arrow">➥</span> with Roman numerals (under which the game would have been known as 
<span class="fm-code-continuation-arrow">➥</span> "Super Bowl L"), so that the logo could prominently feature the Arabic 
<span class="fm-code-continuation-arrow">➥</span> numerals 50. 
 
Answer (char indexed)
     {'answer_start': 487, 'text': '"golden anniversary"', 
<span class="fm-code-continuation-arrow">➥</span> 'answer_end': 507} 
 
================================================== 
 
98-99 token ids contain the answer
Answer (predicted)
golden anniversary
================================================== </pre>

  <p class="body"><a class="calibre8" id="pgfId-1155293"></a>This ends our conversation about using Hugging Face’s Transformer library to implement Transformer models. We have gone through all the steps you are likely to come across while solving any NLP task with a Transformer model. And Hugging Face’s <span class="fm-code-in-text">transformers</span> library still enjoys its reputation as one of the best libraries for implementing Transformer models in TensorFlow or PyTorch.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1155294"></a>Visualizing attention heads</p>

    <p class="fm-sidebar-text"><a id="pgfId-1155295"></a>Whenever there’s a chance for us to interpret deep learning models and understand why a model made a certain decision, it is important to make the most of it. Having the ability to dissect and interpret models helps to build trust among users. Interpreting Transformer models is made very easy due to the presence of the self-attention layers. Using the self-attention layer, we can find out which words the model paid attention to while coming up with a hidden representation of a token.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1155297"></a>We can use the <span class="fm-code-in-text1">bert_viz</span> library<a id="marker-1179724"></a> (<span class="fm-hyperlink"><a class="url" href="https://github.com/jessevig/bertviz">https://github.com/jessevig/bertviz</a></span>) to visualize the attention patterns in any attention head found in any layer. It is important to note that <span class="fm-code-in-text1">bert_viz</span><a id="marker-1167804"></a> does not support TensorFlow but uses the PyTorch library. Despite this small technical difference, using <span class="fm-code-in-text1">bert_viz</span> is easy and straightforward.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1155298"></a>First, import the required libraries:</p>
    <pre class="programlisting">import torch
from bertviz import head_view</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1155301"></a>Next, define a BERT model. Make sure to pass the <span class="fm-code-in-text1">output_attentions=True</span> config to output attention outputs, as it is turned off by default:</p>
    <pre class="programlisting">config = BertConfig.from_pretrained(
    'bert-base-uncased', output_attentions=True
)
bert = TFBertModel.from_pretrained(
    "bert-base-uncased", config=config
)</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1155309"></a>Encode the input text and then get the output of the model:</p>
    <pre class="programlisting">encoded_input = tokenizer(text, return_tensors='tf')
output = model(encoded_input)</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1155313"></a>Finally call the <span class="fm-code-in-text1">head_view()</span> function. You can get the attention outputs by simply calling <span class="fm-code-in-text1">output.attentions</span>, which will return a tuple of 12 tensors. Each tensor corresponds to a separate layer in BERT. Also, make sure you convert them to <span class="fm-code-in-text1">torch</span> tensors<a id="marker-1179816"></a><a id="marker-1170952"></a>. Otherwise, this function errors out. The output is visualized in figure 13.10.</p>
    <pre class="programlisting">head_view(
    [torch.from_numpy(layer_attn.numpy()) for layer_attn in output.attentions],
    encoded_tokens
)</pre>
  </div>

  <p class="body">This ends our discussion about Transformer models. However, it’s important to keep in mind that Transformer models are evolving and becoming better even as we speak. In the next chapter, we will discuss an important visualization tool, called TensorBoard, that is shipped with TensorFlow.</p>

  <p class="fm-figure"><img alt="13-10" class="calibre10" src="../../OEBPS/Images/13-10.png" width="477" height="999"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1180965"></a>Figure 13.10 The attention output of the <span class="fm-code-in-figurecaption">bert_viz</span> library. You can select different layers from the dropdown menu. The different shades represent different attention heads in that layer, which can be switched on or off. The lines between the two columns represent which words the model paid attention to when generating the hidden representation of a given token.<a id="marker-1180966"></a><a id="marker-1180967"></a><a id="marker-1180968"></a><a id="marker-1180969"></a><a id="marker-1180970"></a><a id="marker-1180971"></a></p>

  <p class="fm-head2"><a id="pgfId-1155328"></a>Exercise 3</p>

  <p class="body"><a class="calibre8" id="pgfId-1155329"></a>You are asked to implement a named-entity recognition model. Named-entity recognition is a token classification task, where each token is assigned a label (e.g., person, organization, geographical, other, etc.). There are seven different labels. If you want to use the <span class="fm-code-in-text">distilbert-base-uncased</span> model for this, how would you define the model? Remember that in the <span class="fm-code-in-text">transformers</span> library, you can pass the <span class="fm-code-in-text">num_labels</span> as a keyword to define the number of output classes. For example, if you have a config <span class="fm-code-in-text">"a"</span> that you would like to set to <span class="fm-code-in-text">"abc"</span>, you<a class="calibre8" id="marker-1155330"></a> can do</p>
  <pre class="programlisting">&lt;model&gt;.from_pretrained(&lt;model_tag&gt;, <i class="fm-italics">"</i>a<i class="fm-italics">"</i>= <i class="fm-italics">"</i>abc<i class="fm-italics">"</i>)</pre>

  <h2 class="fm-head" id="sigil_toc_id_179"><a id="pgfId-1155339"></a>Summary</h2>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1155340"></a>The main subcomponents of a Transformer model are embeddings (token and positional), a self-attention sublayer, a fully connected sublayer, residual connections, and layer normalization sublayers.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1155341"></a>BERT is a Transformer encoder model that produces a hidden (attended) representation for each token passed in the input.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1155342"></a>BERT uses special tokens like [CLS] (denotes the beginning and is used to generate the output for classification heads), [SEP] (to mark the separation between two subsequences; e.g., question and the context in question answering), [PAD] (to denote padded tokens to bring all sequences to a fixed length), and [MASK] (to mask out tokens in the input sequence; e.g., padded tokens).</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1155343"></a>BERT can be used for downstream, task-specific classification tasks by fitting a classification head (e.g., a logistic regression layer) on top of the final output of BERT. Depending on the type of task, multiple classification heads might be required, and the utilization of the classification head might differ.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1155344"></a><a id="marker-1174775"></a>Hugging Face’s <span class="fm-code-in-text">transformers</span> library provides implementations of all the NLP-related Transformer models and can be easily downloaded and used in the workflow. The pretrained models available for download have two main components: the tokenizer that will tokenize a provided string to a sequence of tokens and the model that takes in the sequence of tokens to generate the final hidden output.</p>
    </li>
  </ul>

  <h2 class="fm-head" id="sigil_toc_id_180"><a id="pgfId-1155345"></a>Answers to exercises</h2>

  <p class="body"><a class="calibre8" id="pgfId-1155346"></a><b class="fm-bold">Exercise 1</b></p>

  <p class="fm-equation"><a id="pgfId-1159125"></a><i class="fm-italics">PE</i>(<i class="fm-italics">pos</i>,2<i class="fm-italics">i</i> ) = sin(<i class="fm-italics">pos</i>/10000<sup class="fm-superscript">21/d<sub class="fm-subscript2">model</sub></sup>)</p>
  <pre class="programlisting">import tensorflow as tf
 
# Defining some hyperparameters
n_steps = 25 # Sequence length
n_en_vocab = 300 # Encoder's vocabulary size
n_heads = 8 # Number of attention heads
d = 512 # The feature dimensionality of each layer
 
# Encoder input layer
en_inp = tf.keras.layers.Input(shape=(n_steps,))
# Encoder input embedddings
en_emb = tf.keras.layers.Embedding(
    n_en_vocab, 512, input_length=n_steps
)(en_inp)
 
pos_inp = tf.constant(
    [[p/(10000**(2*i/d)) for p in range(d)] for i in range(n_steps)]
)
pos_inp = tf.expand_dims(pos_inp, axis=0)
en_pos_emb = tf.math.sin(pos_inp)
 
en_final_emb = en_emb + en_pos_emb
 
# Two encoder layers
en_out1 = EncoderLayer(d, n_heads)(en_emb)
en_out2 = EncoderLayer(d, n_heads)(en_out1)
 
model = tf.keras.models.Model(inputs=en_inp, output=en_out2)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1155380"></a><b class="fm-bold">Exercise 2</b></p>
  <pre class="programlisting">hub_classifier, hub_encoder = bert.bert_models.classifier_model(
    bert_config=bert_config, hub_module_url=bert_url, num_labels=5
)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1155385"></a><b class="fm-bold">Exercise 3</b><a class="calibre8" id="marker-1155384"></a></p>
  <pre class="programlisting">from transformers import TFDistilBertForTokenClassification
 
model = TFDistilBertForTokenClassification.from_pretrained(
    "distilbert-base-uncased", num_labels=7
<i class="fm-italics">)</i></pre>
</div>
</div>
</body>
</html>