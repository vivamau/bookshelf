<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xml:lang="en"
      lang="en"
      xmlns="http://www.w3.org/1999/xhtml"
      xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<title>TensorFlow in Action</title>
<link rel="stylesheet" type="text/css" href="../../override_v1.css"/>
<link rel="stylesheet" type="text/css" href="../../stylesheet.css"/><link rel="stylesheet" type="text/css" href="../../page_styles.css"/>
</head>
<body>
<div id="book-content">
<div id="sbo-rt-content" class="calibre"><h1 class="tochead"><a id="pgfId-1145862"></a>12 Sequence-to-sequence learning: Part 2</h1>

  <p class="co-summary-head"><a id="pgfId-1145864"></a>This chapter<a id="marker-1147383"></a> covers</p>

  <ul class="calibre9">
    <li class="co-summary-bullet"><a class="calibre8" id="pgfId-1145865"></a>Implementing the attention mechanism for the seq2seq model</li>

    <li class="co-summary-bullet"><a class="calibre8" id="pgfId-1145866"></a>Generating visualizations from the attention layer to glean insights from the model</li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1145867"></a>In the previous chapter, we built an English-to-German machine translator. The machine learning model was a sequence-to-sequence model that could learn to map arbitrarily long sequences to other arbitrarily long sequences. It had two main components: an encoder and a decoder. To arrive at that, we first downloaded a machine translation data set, examined the structure of that data set, and applied some processing (e.g., adding SOS and EOS tokens) to prepare it for the model. Next, we defined the machine translation model using standard Keras layers. A special characteristic of this model is its ability to take in raw strings and convert them to numerical representations internally. To achieve this, we used the Keras’s <span class="fm-code-in-text">TextVectorization</span> layer. When the model was defined, we trained it using the data set we processed and evaluated it on two metrics: per-word accuracy of the sequences produced and BLEU. BLEU is a more advanced metric than accuracy that mimics how a human would evaluate the quality of a translation. To train the model, we used a technique known as teacher forcing. When teacher forcing is used, we feed the decoder, with the target translation offset by 1. This means the decoder predicts the next word in the target sequence given the previous word(s), instead of trying to predict the whole target sequence without any knowledge of the target sequence. This leads to better performance. Finally, we had to redefine our model to suit inference. This is because we had to modify the decoder such that it predicted one word at a time instead of a sequence. This way, we can create a recursive decoder at inference time, which predicts a word and feeds the predicted word as an input to predict the next word in the sequence.</p>

  <p class="body"><a class="calibre8" id="pgfId-1145868"></a>In this chapter, we will explore ways to increase the accuracy of our model. To do that, we will use the attention mechanism. Without attention, machine translation models rely on the last output produced after processing the input sequence. Through the attention mechanism, the model is able to obtain rich representations from all the time steps (while processing the input sequence) during the generation of the translation. Finally, we will conclude the chapter by visualizing the attention mechanisms that will give insights into how the model pays attention to words provided to it during the translation process.</p>

  <p class="body"><a class="calibre8" id="pgfId-1145869"></a>The data and the processing we do in this chapter are going to be identical to the last chapter. Therefore, we will not discuss data in detail. You have been provided all the code necessary to load and process data in the notebook. But let’s refresh the key steps we performed:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1145871"></a>Download the data set manually from <span class="fm-hyperlink"><a class="url" href="http://www.manythings.org/anki/deu-eng.zip">http://www.manythings.org/anki/deu-eng.zip</a></span>.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1145872"></a>The data is in tab-separated format and <span class="fm-code-in-text">&lt;German phrase&gt;&lt;tab&gt;&lt;English phrase&gt;&lt;tab&gt;&lt;Attribution&gt;</span> format. We really care about the first two tab-separated values in a record. We are going to predict the German phrase given the English phrase.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1145873"></a>We randomly sample 50,000 data points from the data set and use 5,000 (i.e., 10%) as validation data and another 5,000 (i.e., 10%) as test data.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1145874"></a>We add a start token (e.g., SOS) and an end token (e.g., EOS) to each German phrase. This is an important preprocessing step, as this helps us to recursively infer words from our recursive decoder at inference time (i.e., provide SOS as the initial seed and keep predicting until the model outputs EOS or reaches a maximum length).</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1145875"></a>We look at summary statistics of vocabulary size and sequence length, as these hyperparameters are very important for our <span class="fm-code-in-text">TextVectorization</span> layer (the layer can be found at tensorflow.keras.layers.experimental.preprocessing.TextVectorization<i class="fm-italics">).</i></p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1145876"></a>The vocabulary size is set as the number of unique words that appear more than 10 times in the corpus for both languages, and the sequence length is set as the 99% quantile (plus a buffer of 5) for both languages.</p>
    </li>
  </ul>

  <h2 class="fm-head" id="sigil_toc_id_159"><a id="pgfId-1145877"></a>12.1 Eyeballing the past: Improving our model with attention</h2>

  <p class="body"><a class="calibre8" id="pgfId-1145880"></a>You<a class="calibre8" id="marker-1145878"></a><a class="calibre8" id="marker-1145879"></a> have a working prototype of the translator but still think you can push the accuracy up by using attention. Attention provides a richer output from the encoder to the decoder by allowing the decoder to look at all the outputs produced by the encoder over the entire input sequence. You will modify the previously implemented model to incorporate an attention layer that takes all the encoder outputs (one for each time step) and produces a sequence of outputs for each decoder step that will be concatenated with the standard output produced by the decoder.</p>

  <p class="body"><a class="calibre8" id="pgfId-1145881"></a>We have a working machine translator model that can translate from English to German. Performance of this model can be pushed further using something known as <i class="fm-italics">Bahdanau attention</i>. Bahdanau attention was introduced in the paper “Neural Machine Translation by Jointly Learning to Align and Translate” by Bahdanau et al. (<span class="fm-hyperlink"><a class="url" href="https://arxiv.org/pdf/1409.0473.pdf">https://arxiv.org/pdf/1409.0473.pdf</a></span>). We already discussed self-attention in chapter 5. The underlying principle between the two attention mechanisms is the same. They both allow the model to get a rich representation of historical/future input in a sequence to facilitate the model in understanding the language better. Let’s see how the attention mechanism can be tied in with the encoder-decoder model we have.</p>

  <p class="body"><a class="calibre8" id="pgfId-1145883"></a>The attention mechanism produces an output for each decoder time step, similar to how the decoder’s GRU model produces an output at each time step. The attention output is combined with the decoder’s GRU output and fed to the subsequent hidden layer in the decoder. The attention output produced at each time step of the decoder combines the encoder’s outputs from all the time steps, which provides valuable information about the English input sequence to the decoder. The attention layer is allowed to mix the encoder outputs differently to produce the output for each decoder time step, depending on which part of the translation the decoder model is working on at a given moment. You should be able to see how powerful the attention mechanism is. Previously, the context vector was the only input from the encoder that was accessible to the decoder. This is a massive performance bottleneck, as it is impractical for the encoder to encode all the information present in a sentence using a small-sized vector.</p>

  <p class="body"><a class="calibre8" id="pgfId-1145884"></a>Let’s probe a bit more to understand the specific computations that transpire during the computation of the attention outputs. Let’s assume that the encoder output at position <i class="fm-timesitalic">j</i> (1 &lt; <i class="fm-timesitalic">j</i> &lt; <i class="fm-timesitalic">T</i><sub class="fm-subscript">e</sub>) is denoted by <i class="fm-timesitalic">h</i><sub class="fm-subscript">j</sub>, and the decoder RNN output state at time i (1 &lt; <i class="fm-timesitalic">i</i> &lt; <i class="fm-timesitalic">T</i><sub class="fm-subscript">d</sub>) is denoted by <i class="fm-timesitalic">s</i><sub class="fm-subscript">i</sub> ; then the attention output <i class="fm-timesitalic">c</i><sub class="fm-subscript">i</sub> for the <i class="fm-timesitalic">i</i><sup class="fm-superscript">th</sup> decoding step is computed by</p>

  <p class="fm-equation"><a id="pgfId-1147487"></a><i class="fm-italics">e</i><sub class="fm-subscript">ij</sub> = <i class="fm-italics">v</i><sup class="fm-superscript">T</sup> <i class="fm-italics">tanh</i>(<i class="fm-italics">s</i><sub class="fm-subscript">i -1</sub> <i class="fm-italics">W</i> + <i class="fm-italics">h</i><sub class="fm-subscript">j</sub><i class="fm-italics">U</i>)</p>

  <p class="fm-equation"><img alt="12_00a" class="calibre10" src="../../OEBPS/Images/12_00a.png" width="194" height="113"/><br class="calibre2"/>
  <a id="pgfId-1148764"></a></p>

  <p class="fm-equation"><a id="pgfId-1147571"></a> </p>

  <p class="fm-equation"><img alt="12_00b" class="calibre10" src="../../OEBPS/Images/12_00b.png" width="144" height="96"/><br class="calibre2"/>
  <a id="pgfId-1145888"></a></p>

  <p class="body"><a class="calibre8" id="pgfId-1145897"></a>Here, W, U, and v are weight matrices (initialized randomly just like neural network weights). Their shapes are defined in accordance with the dimensionality of hidden representations s and h, which will be discussed in detail soon. In summary, this set of equations, for a given decoder position</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1145898"></a>Computes energy values representing how important each encoder output is for that decoding step using a small fully connected network whose weights are W, U, and v</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1145899"></a>Normalizes energies to represent a probability distribution over the encoder steps</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1145900"></a>Computes a weighted sum of encoder outputs using the probability distribution</p>
    </li>
  </ul>

  <h3 class="fm-head1" id="sigil_toc_id_160"><a id="pgfId-1145901"></a>12.1.1 Implementing Bahdanau attention in TensorFlow</h3>

  <p class="body"><a class="calibre8" id="pgfId-1145906"></a>Unfortunately<a class="calibre8" id="marker-1145902"></a><a class="calibre8" id="marker-1145903"></a><a class="calibre8" id="marker-1145904"></a><a class="calibre8" id="marker-1145905"></a>, TensorFlow does not have a built-in layer to readily use in our models to enable the attention mechanism. Therefore, we will implement an <span class="fm-code-in-text">Attention</span> layer<a class="calibre8" id="marker-1145907"></a> using the Keras subclassing API. We will call this the <span class="fm-code-in-text">DecoderRNNAttentionWrapper</span> and will have to implement the following functions:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1145909"></a><span class="fm-code-in-text">__init__</span>—Defines various initializations that need to happen before the layer can operate correctly</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1145911"></a><span class="fm-code-in-text">build</span><a class="calibre8" id="marker-1145910"></a><span class="fm-code-in-text">()</span>—Defines the parameters (e.g., trainable weights) and their shapes associated with the computation</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1145913"></a><span class="fm-code-in-text">call</span><a class="calibre8" id="marker-1145912"></a><span class="fm-code-in-text">()</span>—Defines the computations and the final output that should be produced by the layer</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1145914"></a>The <span class="fm-code-in-text">__init__()</span> function initializes the layer with any attributes it requires to operate correctly. In this case, our <span class="fm-code-in-text">DecoderRNNAttentionWrapper</span> takes in a <span class="fm-code-in-text">cell_fn</span> as the argument. <span class="fm-code-in-text">cell_fn</span> needs to be a Keras layer object that implements the <span class="fm-code-in-text">tf.keras .layers.AbstractRNNCell</span> interface<a class="calibre8" id="marker-1145915"></a> (<span class="fm-hyperlink"><a class="url" href="http://mng.bz/pO18">http://mng.bz/pO18</a></span>). There are several options, such as <span class="fm-code-in-text">tf.keras.layers.GRUCell</span>, <span class="fm-code-in-text">tf.keras.layers.LSTMCell</span>, and <span class="fm-code-in-text">tf.keras.layers.RNNCell</span>. In this case, we will use the <span class="fm-code-in-text">tf.keras.layers.GRUCell</span>.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1145917"></a>Difference between tf.keras.layers.GRUCell and tf.keras.layers.GRU</p>

    <p class="fm-sidebar-text"><a id="pgfId-1145918"></a>The <span class="fm-code-in-text1">GRUCell</span> can be thought of as an abstraction of the GRU layer. The <span class="fm-code-in-text1">GRUCell</span> encompasses the most minimalist computation you can think of in an RNN layer. Given an input and a previous state, it computes the next output and the next state. This is the most primitive computation that governs an RNN layer:</p>
    <pre class="programlisting">output, next_state = tf.keras.layers.GRUCell(input, state)</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1145920"></a>In other words, a <span class="fm-code-in-text1">GRUCell</span> encapsulates the computations required to compute a single time step in an input sequence. The GRU layer is a fully fledged implementation of the <span class="fm-code-in-text1">GRUCell</span> that can process the whole sequence. Furthermore, the GRU layer gives options like <span class="fm-code-in-text1">return_state</span> and <span class="fm-code-in-text1">return_sequence</span> to control the output produced by the GRU layer.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1145921"></a>In short, the GRU layer provides the convenience for processing input sequences, and the <span class="fm-code-in-text1">GRUCell</span> exposes the more fine-grained implementation details that allow one to process a single time step in the sequence.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1145923"></a>Here, we have decided to go with GRU, as the GRU model is a lot simpler than an LSTM (meaning there is reduced training time) but achieves roughly similar results on NLP tasks:</p>
  <pre class="programlisting">def __init__(self, cell_fn, units, **kwargs):
    self._cell_fn = cell_fn
    self.units = units
    super(DecoderRNNAttentionWrapper, self).__init__(**kwargs)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1145929"></a>Next, the <span class="fm-code-in-text">build()</span> function is defined. The build function declares the three weight matrices used in the attention computation: W, U and v. The argument <span class="fm-code-in-text">input_shape</span> contains the shapes of the inputs. Our input will be a tuple containing encoder outputs and the decoder RNN inputs:</p>
  <pre class="programlisting">def build(self, input_shape):
 
 
    self.W_a = self.add_weight(
        name='W_a',
        shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),
        initializer='uniform',
        trainable=True
    )
    
    self.U_a = self.add_weight(
        name='U_a',
        shape=tf.TensorShape((self._cell_fn.units, self._cell_fn.units)),
        initializer='uniform',
        trainable=True
    )
 
    self.V_a = self.add_weight(
        name='V_a',
        shape=tf.TensorShape((input_shape[0][2], 1)),
        initializer='uniform',
        trainable=True
    )
 
    super(DecoderRNNAttentionWrapper, self).build(input_shape)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1145957"></a>The most important argument to note in the weight definitions is the shape argument. We are defining them so that</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1145958"></a><span class="fm-code-in-text">W_a</span> (representing W) has a shape of [ &lt;encoder hidden size&gt;, &lt;attention hidden size&gt;]</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1145959"></a><span class="fm-code-in-text">U_a</span> (representing U) has a shape of [&lt;decoder hidden size&gt;, &lt;attention hidden size&gt;]</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1145960"></a><span class="fm-code-in-text">V_a</span> (representing v) has a shape of [&lt;attention hidden size&gt;, 1]</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1145961"></a>Here, the &lt;encoder hidden size&gt; and &lt;decoder hidden size&gt; are the number of units in the final output of the RNN layer of the encoder or the decoder, respectively. We typically keep the encoder and decoder RNN sizes the same to simplify the computations. The &lt;attention hidden size&gt; is a hyperparameter of the layer that can be set to any value and represents the dimensionality of internal computations of the attention. Finally, we define the <span class="fm-code-in-text">call()</span> method (see listing 12.1). The <span class="fm-code-in-text">call()</span> method encapsulates the computations that take place when the layer is called with inputs. This is where the heavy lifting required to compute the attention outputs happens. At a high level, the attention layer needs to traverse all the encoder inputs (i.e., each time step) for each decoder input.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1145964"></a>Listing 12.1 Attention computation in the <span class="fm-code-in-listingcaption">DecoderRNNAttentionWrapper</span></p>
  <pre class="programlisting">def call(self, inputs, initial_state, training=False):
                
    def _step(inputs, states):
        """ Step function for computing energy for a single decoder state
        inputs: (batchsize * de_in_dim)
        states: [(batchsize * de_latent_dim)]
        """
        
        encoder_full_seq = states[-1]                                   <span class="fm-combinumeral">❶</span>
 
        W_a_dot_h = K.dot(encoder_outputs, self.W_a)                    <span class="fm-combinumeral">❷</span>
                               
        U_a_dot_s = K.expand_dims(K.dot(states[0], self.U_a), 1)        <span class="fm-combinumeral">❸</span>
                             
        Wh_plus_Us = K.tanh(W_a_dot_h + U_a_dot_s)                      <span class="fm-combinumeral">❹</span>
       
        e_i = K.squeeze(K.dot(Wh_plus_Us, self.V_a), axis=-1)           <span class="fm-combinumeral">❺</span>
        a_i = K.softmax(e_i)                                            <span class="fm-combinumeral">❺</span>
 
        c_i = K.sum(encoder_outputs * K.expand_dims(a_i, -1), axis=1)   <span class="fm-combinumeral">❻</span>
                   
        s, states = self._cell_fn(K.concatenate([inputs, c_i], axis=-1), 
<span class="fm-code-continuation-arrow">➥</span> states)                                                              <span class="fm-combinumeral">❼</span>
        
        return (s, a_i), states
 
   """ Computing outputs """
   
   encoder_outputs, decoder_inputs = inputs                             <span class="fm-combinumeral">❽</span>
     
   _, attn_outputs, _ = K.rnn(
        step_function=_step, inputs=decoder_inputs, 
<span class="fm-code-continuation-arrow">➥</span> initial_states=[initial_state], constants=[encoder_outputs]          <span class="fm-combinumeral">❾</span>
   )
        
   # attn_out =&gt; (batch_size, de_seq_len, de_hidden_size)
   # attn_energy =&gt; (batch_size, de_seq_len, en_seq_len)
   attn_out, attn_energy = attn_outputs                                 <span class="fm-combinumeral">❿</span>
 
   return attn_out, attn_energy</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1157143"></a><span class="fm-combinumeral">❶</span> When calling the _step function, we are passing encoder_outputs as a constant, as we need access to the full encoder sequence. Here we access it within the _step function.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1157167"></a><span class="fm-combinumeral">❷</span> Computes S.Wa where S represents all the encoder outputs and S=[s0, s1, ..., si]. This produces a [batch size, en_seq_len, hidden size]-sized output.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1157251"></a><span class="fm-combinumeral">❸</span> Computes hj.Ua, where hj represent the j^{th} decoding step. This produces a [ batch_size, 1, hidden size]-sized output</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1157268"></a><span class="fm-combinumeral">❹</span> Computes tanh(S.Wa + hj.Ua). This produces a [batch_size, en_seq_len, hidden size]-sized output</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1157285"></a><span class="fm-combinumeral">❺</span> Computes the energies and normalizes them. Produces a [batch_size, en_seq_len] sized output</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1157302"></a><span class="fm-combinumeral">❻</span> Computes the final attention output (c_i) as a weighted sum of h_j (for all j), where weights are denoted by a_i. Produces a [batch_size, hidden_size] output</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1157319"></a><span class="fm-combinumeral">❼</span> Concatenate sthe current input and c_i and feeds it to the decoder RNN to get the output</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1157336"></a><span class="fm-combinumeral">❽</span> The inputs to the attention layer are encoder outputs and decoder RNN inputs.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1157353"></a><span class="fm-combinumeral">❾</span> The K.rnn() function executes the _step() function for every input in the decoder inputs to produce attention outputs for all the decoding steps.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1157370"></a><span class="fm-combinumeral">❿</span> The final output is two-fold: attention outputs of a [batch size, de_seq_len, hidden size]-sized output and attention energies [batch dize, de_seq_len, en_seq_len]-sized outputs</p>

  <p class="body"><a class="calibre8" id="pgfId-1146010"></a>Let’s demystify what’s done in this function. The input to this layer is an iterable of two elements: encoder output sequence (<span class="fm-code-in-text">encoder_outputs</span>) and decoder RNN input sequence (<span class="fm-code-in-text">decoder_inputs</span>). Next, we use a special backend function of Keras called <span class="fm-code-in-text">K.rnn</span><a id="marker-1146011"></a><a class="calibre8" id="marker-1157240"></a><span class="fm-code-in-text">()</span> (<span class="fm-hyperlink"><a class="url" href="http://mng.bz/OoPR">http://mng.bz/OoPR</a></span>) to iterate through these inputs while computing the final output required. In our example, it is called as</p>
  <pre class="programlisting">_, attn_outputs, _ = K.rnn(
        step_function=_step, inputs=decoder_inputs, initial_states=[initial_state], constants=[encoder_outputs],
   )</pre>

  <p class="body"><a class="calibre8" id="pgfId-1146017"></a>Here, it applies the <span class="fm-code-in-text">step_function</span> to each time step slice of the <span class="fm-code-in-text">inputs</span> tensor<a class="calibre8" id="marker-1146016"></a>. For example, the <span class="fm-code-in-text">decoder_inputs</span> is a [&lt;batch size&gt;, &lt;decoder time steps&gt;, &lt;embedding size&gt;]-sized input. Then the <span class="fm-code-in-text">K.rnn()</span> function applies the <span class="fm-code-in-text">step_function</span> to every [&lt;batch size&gt;, &lt;embedding size&gt;] output for &lt;decoder time steps&gt; number of times. The update this function does is a recurrent update, meaning that it takes an initial state and produces a new state until it reaches the end of the input sequence. For that, <span class="fm-code-in-text">initial_states</span> provides the starting states. Finally, we are passing <span class="fm-code-in-text">encoder_outputs</span> as a <span class="fm-code-in-text">constant</span> to the <span class="fm-code-in-text">step_function</span>. This is quite important as we need the full sequence of the encoder’s hidden outputs to compute attention at each decoding step. Within the <span class="fm-code-in-text">step_function</span>, <span class="fm-code-in-text">constants</span> gets appended to the value of the <span class="fm-code-in-text">states</span> argument. So, you can access <span class="fm-code-in-text">encoder_outputs</span> as the last element of <span class="fm-code-in-text">states</span>.</p>

  <p class="body"><a class="calibre8" id="pgfId-1146019"></a>The <span class="fm-code-in-text">_step</span> function does the computations we outlined in listing 12.1 for a single decoder time step. It takes inputs (a slice of the time dimension of the original input) and states (initialized with the <span class="fm-code-in-text">initial_states</span> value in the <span class="fm-code-in-text">K.rnn()</span> function). Next, using these two entities, the normalized attention energies (i.e., <span class="fm-symbol1">α</span><sub class="fm-subscript">ij</sub>) for a single time step are computed (<span class="fm-code-in-text">a_i</span>). Following that, <span class="fm-code-in-text">c_i</span> is computed, which is a weighted sum of <span class="fm-code-in-text">encoder_outputs</span> weighted by <span class="fm-code-in-text">a_i</span>. Afterward, it updates the <span class="fm-code-in-text">cell_fn</span> (i.e., <span class="fm-code-in-text">GRUCell</span>) with the current input and the state. Note that the current input to the <span class="fm-code-in-text">cell_fn</span> is a concatenation of the decoder input and <span class="fm-code-in-text">c_i</span> (i.e., the weighted sum of encoder inputs). The cell function then outputs the output state along with the next state. We return this information out. In other words, the <span class="fm-code-in-text">_step()</span> function outputs the output for that time step (i.e., a tuple of decoder RNN output and normalized energies that computed the weighted sum of encoder inputs) and the next state of the decoder RNN.</p>

  <p class="body"><a class="calibre8" id="pgfId-1146026"></a>Finally, you can obtain the full output of the <span class="fm-code-in-text">_step</span> function for all the decoder time steps using the <span class="fm-code-in-text">K.rnn()</span> function as shown. We are only interested in the output itself (denoted by <span class="fm-code-in-text">attn_outputs</span>) and will ignore the other things output by the function.</p>

  <p class="body"><a class="calibre8" id="pgfId-1146027"></a>The <span class="fm-code-in-text">K.rnn()</span> function outputs the following outputs when called:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1146028"></a><span class="fm-code-in-text">last_output</span>—The last output produced by the _<span class="fm-code-in-text">step_function</span> after it reaches the end of the sequence</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1146029"></a><span class="fm-code-in-text">outputs</span>—All the outputs produced by the <span class="fm-code-in-text">step_function</span></p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1146030"></a><span class="fm-code-in-text">new_states</span>—The last states produced by the <span class="fm-code-in-text">step_function</span> after it reaches the end of the sequence</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1146031"></a>Finally, the <span class="fm-code-in-text">call()</span> function produces two outputs:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1146032"></a><span class="fm-code-in-text">attn_out</span>—Holds all the attention outputs for all the decoding steps</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1146033"></a><span class="fm-code-in-text">attn_energy</span>—Provides the normalized energy values for a batch of data, where the energy matrix for one example contains energy values for all the encoder time steps for every decoder time step</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1146034"></a>We have discussed the most important functions of the <span class="fm-code-in-text">DecoderRNNAttentionWrapper</span> layer<a class="calibre8" id="marker-1146035"></a>. If you want to see the full sub-classed implementation of the <span class="fm-code-in-text">DecoderRNNAttentionWrapper</span>, please refer to the code at<a class="calibre8" id="marker-1146036"></a><a class="calibre8" id="marker-1146037"></a><a class="calibre8" id="marker-1146038"></a><a class="calibre8" id="marker-1146039"></a> Ch11/11.1_seq2seq_machine_translation .ipynb.</p>

  <h3 class="fm-head1" id="sigil_toc_id_161"><a id="pgfId-1146040"></a>12.1.2 Defining the final model</h3>

  <p class="body"><a class="calibre8" id="pgfId-1146045"></a>When<a class="calibre8" id="marker-1146041"></a><a class="calibre8" id="marker-1146043"></a> defining the final model, the <span class="fm-code-in-text">get_vectorizer()</span> and <span class="fm-code-in-text">get_encoder()</span> functions remain identical to what was shown in the previous section. All the modifications required need to happen in the decoder. Therefore, let’s define a function, <span class="fm-code-in-text">get_ final_seq2seq_model_with_attention()</span>, that provides us the decoder with Bahdanau attention in place, as shown in the next listing.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1146047"></a>Listing 12.2 Defining the final sequence-to-sequence model with attention</p>
  <pre class="programlisting">def get_final_seq2seq_model_with_attention(n_vocab, encoder, vectorizer):
    """ Define the final encoder-decoder model """
    
    e_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input_final')          
    fwd_state, bwd_state, en_states = encoder(e_inp)                              <span class="fm-combinumeral">❶</span>
    
    d_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_input')           <span class="fm-combinumeral">❷</span>
    
    d_vectorized_out = vectorizer(d_inp)                                          <span class="fm-combinumeral">❸</span>
        
    d_emb_layer = tf.keras.layers.Embedding(
        n_vocab+2, 128, mask_zero=True, name='d_embedding'                        <span class="fm-combinumeral">❹</span>
    )    
    d_emb_out = d_emb_layer(d_vectorized_out)                                     <span class="fm-combinumeral">❹</span>
    
    d_init_state = tf.keras.layers.Concatenate(axis=-1)([fwd_state, bwd_state])   <span class="fm-combinumeral">❺</span>
    
    gru_cell = tf.keras.layers.GRUCell(256)                                       <span class="fm-combinumeral">❻</span>
    attn_out, _  = DecoderRNNAttentionWrapper(
        cell_fn=gru_cell, units=512, name="d_attention"
    )([en_states, d_emb_out], initial_state=d_init_state)                         <span class="fm-combinumeral">❼</span>
 
    d_dense_layer_1 = tf.keras.layers.Dense(512, activation='relu', name='d_dense_1')
    d_dense1_out = d_dense_layer_1(attn_out)                                      <span class="fm-combinumeral">❽</span>
    
    d_final_layer = tf.keras.layers.Dense(
        n_vocab+2, activation='softmax', name='d_dense_final'
    )
    d_final_out = d_final_layer(d_dense1_out)                                     <span class="fm-combinumeral">❽</span>
    
    seq2seq = tf.keras.models.Model(
        inputs=[e_inp, d_inp], outputs=d_final_out,                               <span class="fm-combinumeral">❾</span>
        name='final_seq2seq_with_attention'
    )
    
    return seq2seq</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1156450"></a><span class="fm-combinumeral">❶</span> Get the encoder outputs for all the timesteps.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1156474"></a><span class="fm-combinumeral">❷</span> The input is (None,1) shaped and accepts an array of strings.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1156491"></a><span class="fm-combinumeral">❸</span> Vectorize the data (assign token IDs).</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1156508"></a><span class="fm-combinumeral">❹</span> Define an embedding layer to convert IDs to word vectors.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1156525"></a><span class="fm-combinumeral">❺</span> Define the initial state to the decoder as the concatenation of the last forward and backward encoder states.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1156542"></a><span class="fm-combinumeral">❻</span> Define a GRUCell, which will then be used for the Attention layer.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1156559"></a><span class="fm-combinumeral">❼</span> Get the attention outputs. The GRUCell is passed as the cell_fn, where the inputs are en_states (i.e., all of the encoder states) and d_emb_out (input to the decoder RNN).</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1156576"></a><span class="fm-combinumeral">❽</span> Define the intermediate and final Dense layer outputs.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1156593"></a><span class="fm-combinumeral">❾</span> Define a model that takes encoder and decoder inputs as inputs and outputs the final predictions (d_final_out).</p>

  <p class="body"><a class="calibre8" id="pgfId-1146093"></a>We already have done all the hard work. Therefore, changes to the decoder can be summarized in two lines of code:</p>
  <pre class="programlisting">    gru_cell = tf.keras.layers.GRUCell(256)
    attn_out, _  = DecoderRNNAttentionWrapper(
        cell_fn=gru_cell, units=512, name="d_attention"
    )(
        [en_states, d_emb_out], initial_state=d_init_state
    )</pre>

  <p class="body"><a class="calibre8" id="pgfId-1146102"></a>We first define a <span class="fm-code-in-text">GRUCell</span> object<a class="calibre8" id="marker-1152005"></a> with 256 hidden units. Then we define the <span class="fm-code-in-text">DecoderRNNAttentionWrapper</span>, where the <span class="fm-code-in-text">cell_fn</span> is the <span class="fm-code-in-text">GRUCell</span> we defined and <span class="fm-code-in-text">units</span> is set to 512. <span class="fm-code-in-text">units</span> in the <span class="fm-code-in-text">DecoderRNNAttentionWrapper</span> defines the dimensionality of the weights and the intermediate attention outputs. We pass <span class="fm-code-in-text">en_states</span> (i.e., encoder output sequence) and <span class="fm-code-in-text">d_emb_out</span> (i.e., decoder input sequence to the RNN) and set the initial state as the final state of the encoder (i.e., <span class="fm-code-in-text">d_init_state</span>).</p>

  <p class="body"><a class="calibre8" id="pgfId-1146103"></a>Next, as before, we have to define a <span class="fm-code-in-text">get_vectorizer()</span> function (see the next listing) to get the English/German vectorizers.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1146105"></a>Listing 12.3 Defining the <span class="fm-code-in-listingcaption">TextVectorizers</span> for the encoder-decoder model</p>
  <pre class="programlisting">def get_vectorizer(
    corpus, n_vocab, max_length=None, return_vocabulary=True, name=None
):
    
    """ Return a text vectorization layer or a model """
    
    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='encoder_input')<span class="fm-combinumeral">❶</span>
    
    vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(
        max_tokens=n_vocab+2,                                              <span class="fm-combinumeral">❷</span>
        output_mode='int',
        output_sequence_length=max_length,        
        name=name
    )
    
    vectorize_layer.adapt(corpus)                                          <span class="fm-combinumeral">❸</span>
        
    vectorized_out = vectorize_layer(inp)                                  <span class="fm-combinumeral">❹</span>
        
    if not return_vocabulary: 
        return tf.keras.models.Model(inputs=inp, outputs=vectorized_out)   <span class="fm-combinumeral">❺</span>
    else:
        return tf.keras.models.Model(
            inputs=inp, outputs=vectorized_out                             <span class="fm-combinumeral">❻</span>
        ), vectorize_layer.get_vocabulary() </pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1156026"></a><span class="fm-combinumeral">❶</span> Define an input layer that takes a list of strings (or an array of strings).</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1156047"></a><span class="fm-combinumeral">❷</span> When defining the vocab size, there are two special tokens, (Padding) and '[UNK]' (OOV tokens), added automatically.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1156064"></a><span class="fm-combinumeral">❸</span> Fit the vectorizer layer on the data.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1156081"></a><span class="fm-combinumeral">❹</span> Get the token IDs for the data fed to the input.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1156098"></a><span class="fm-combinumeral">❺</span> Return only the model, which takes an array of a string and outputs a tensor of token IDs.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1156115"></a><span class="fm-combinumeral">❻</span> Return the vocabulary in addition to the model.</p>

  <p class="body"><a class="calibre8" id="pgfId-1146138"></a>The <span class="fm-code-in-text">get_encoder()</span> function shown in the following listing builds the encoder. As these have been discussed in detail, they will not be repeated here.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1146140"></a>Listing 12.4 The function that returns the encoder</p>
  <pre class="programlisting">def get_encoder(n_vocab, vectorizer):
    """ Define the encoder of the seq2seq model"""
    
    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input')  <span class="fm-combinumeral">❶</span>
 
    vectorized_out = vectorizer(inp)                                   <span class="fm-combinumeral">❷</span>
    
    
    emb_layer = tf.keras.layers.Embedding(
        n_vocab+2, 128, mask_zero=True, name='e_embedding'             <span class="fm-combinumeral">❸</span>
    )
    
    emb_out = emb_layer(vectorized_out)                                <span class="fm-combinumeral">❹</span>
 
    gru_layer = tf.keras.layers.Bidirectional(
        tf.keras.layers.GRU(128, name='e_gru'),                        <span class="fm-combinumeral">❺</span>
        name='e_bidirectional_gru'
    )
    
    gru_out = gru_layer(emb_out)                                       <span class="fm-combinumeral">❻</span>
    encoder = tf.keras.models.Model(
        inputs=inp, outputs=gru_out, name='encoder'
    )                                                                  <span class="fm-combinumeral">❼</span>
        
    return encoder</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1155485"></a><span class="fm-combinumeral">❶</span> The input is (None,1) shaped and accepts an array of strings.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1155513"></a><span class="fm-combinumeral">❷</span> Vectorize the data (assign token IDs).</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1155530"></a><span class="fm-combinumeral">❸</span> Define an embedding layer to convert IDs to word vectors.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1155547"></a><span class="fm-combinumeral">❹</span> Get the embeddings of the token IDs</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1155564"></a><span class="fm-combinumeral">❺</span> Define a bidirectional GRU layer. The encoder looks at the English text (i.e., the input) both backward and forward; this leads to better performance.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1155581"></a><span class="fm-combinumeral">❻</span> Get the output of the GRU layer (the last output state vector returned by the model).</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1155598"></a><span class="fm-combinumeral">❼</span> Define the encoder model; this takes in a list/array of strings as the input and returns the last output state of the GRU model as the output.</p>

  <p class="body"><a class="calibre8" id="pgfId-1146176"></a>As the very last step, we define the final model and compile it using the same specifications we used for the earlier<a class="calibre8" id="marker-1146177"></a><a class="calibre8" id="marker-1146179"></a> model:</p>
  <pre class="programlisting"># Get the English vectorizer/vocabulary
en_vectorizer, en_vocabulary = 
<span class="fm-code-continuation-arrow">➥</span> get_vectorizer(np.array(train_df["EN"].tolist()), en_vocab, max_length=en_seq_length, name='e_vectorizer')
# Get the German vectorizer/vocabulary
de_vectorizer, de_vocabulary = 
<span class="fm-code-continuation-arrow">➥</span> get_vectorizer(np.array(train_df["DE"].tolist()), de_vocab, 
<span class="fm-code-continuation-arrow">➥</span> max_length=de_seq_length-1, name='d_vectorizer')
 
# Define the final model with attention
encoder = get_encoder_with_attention(en_vocab, en_vectorizer)
final_model_with_attention = 
<span class="fm-code-continuation-arrow">➥</span> get_final_seq2seq_model_with_attention(de_vocab, encoder, de_vectorizer)
 
# Compile the model
final_model_with_attention.compile(
    loss='sparse_categorical_crossentropy', 
    optimizer='adam', 
    metrics=['accuracy']
)</pre>

  <h3 class="fm-head1" id="sigil_toc_id_162"><a id="pgfId-1146195"></a>12.1.3 Training the model</h3>

  <p class="body"><a class="calibre8" id="pgfId-1146199"></a>Training<a class="calibre8" id="marker-1146196"></a><a class="calibre8" id="marker-1146197"></a><a class="calibre8" id="marker-1146198"></a> the model is quite straightforward as it remains the same as before. All we need to do is call the <span class="fm-code-in-text">train_model()</span> function<a class="calibre8" id="marker-1146200"></a> with the arguments model (a Keras model to be trained/evaluated), vectorizer (a target language vectorizer to convert token IDs to text), <span class="fm-code-in-text">train_df</span> (training data), <span class="fm-code-in-text">valid_df</span> (validation data), <span class="fm-code-in-text">test_df</span> (testing data), epochs (an <span class="fm-code-in-text">int</span> to represent how many epochs the model needs to be trained) and <span class="fm-code-in-text">batch_size</span> (size of a training/evaluation batch):</p>
  <pre class="programlisting">epochs = 5
batch_size = 128
 
train_model(final_model_with_attention, de_vectorizer, train_df, valid_df, 
<span class="fm-code-continuation-arrow">➥</span> test_df, epochs, batch_size)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1146206"></a>This will output</p>
  <pre class="programlisting">Evaluating batch 39/39
Epoch 1/5
    (train) loss: 2.096887740951318 - accuracy: 0.6887444907274002 - 
<span class="fm-code-continuation-arrow">➥</span> bleu: 0.00020170408678925458
    (valid) loss: 1.5872839291890461 - accuracy: 0.7375801282051282 - 
<span class="fm-code-continuation-arrow">➥</span> bleu: 0.002304922518160425
...
 
Evaluating batch 39/39
Epoch 5/5
    (train) loss: 0.7739567615282841 - accuracy: 0.8378756006176655 - 
<span class="fm-code-continuation-arrow">➥</span> bleu: 0.20010080750506093
    (valid) loss: 0.8180131682982812 - accuracy: 0.837830534348121 - 
<span class="fm-code-continuation-arrow">➥</span> bleu: 0.20100039279462362
Evaluating batch 39/39
(test) loss: 0.8390972828253721 - accuracy: 0.8342147454237326 - bleu: 
<span class="fm-code-continuation-arrow">➥</span> 0.19782372616582572</pre>

  <p class="body"><a class="calibre8" id="pgfId-1146220"></a>Compared to the last model we had, this is quite an improvement. We have almost doubled the validation and testing BLEU scores. All this was possible because we introduced the attention mechanism to alleviate a huge performance bottleneck in the encoder-decoder model.</p>

  <p class="fm-callout"><a id="pgfId-1146221"></a><span class="fm-callout-head">NOTE</span> On an Intel Core i5 machine with an NVIDIA GeForce RTX 2070 8 GB, the training took approximately five minutes to run five epochs.</p>

  <p class="body"><a class="calibre8" id="pgfId-1146222"></a>Finally, for later use, we save the trained model, along with the vocabularies:</p>
  <pre class="programlisting">## Save the model
os.makedirs('models', exist_ok=True)
tf.keras.models.save_model(final_model_with_attention, 
<span class="fm-code-continuation-arrow">➥</span> os.path.join('models', 'seq2seq_attention'))
 
# Save the vocabulary
import json
os.makedirs(
    os.path.join('models', 'seq2seq_attention_vocab'), exist_ok=True
)
with open(os.path.join('models', 'seq2seq_attention_vocab', 
<span class="fm-code-continuation-arrow">➥</span> 'de_vocab.json'), 'w') as f:
    json.dump(de_vocabulary, f)
    
with open(os.path.join('models', 'seq2seq_attention_vocab', 
<span class="fm-code-continuation-arrow">➥</span> 'en_vocab.json'), 'w') as f:
    json.dump(en_vocabulary, f)</pre>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1146237"></a>State-of-the-art results for English-German translation</p>

    <p class="fm-sidebar-text"><a id="pgfId-1146238"></a>One way to know where our model stands is to compare it to the state-of-the-art result that has been achieved on English-German translation. In 2021, at the time of writing this book, a BLEU score of 0.3514 has been achieved by one of the models. The model is introduced in the paper “Lessons on Parameter Sharing across Layers in Transformers” by Takase et al. (<span class="fm-hyperlink"><a class="url" href="https://arxiv.org/pdf/2104.06022v1.pdf">https://arxiv.org/pdf/2104.06022v1.pdf</a></span>).</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1146239"></a>This should not be taken as an exact comparison to our model, as the benchmarked models are typically trained on the WMT English-German data set (<span class="fm-hyperlink"><a class="url" href="https://nlp.stanford.edu/projects/nmt/">https://nlp.stanford.edu/projects/nmt/</a></span>), which is a much larger and more complex data set. However, given that we have a relatively simple model with no special training time optimizations, 0.1978 is a decent score.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1146241"></a>With that, we will discuss how we can visualize the attention weights to see the attention patterns the model uses when decoding inputs.</p>

  <p class="fm-head2"><a id="pgfId-1146242"></a>Exercise 1</p>

  <p class="body"><a class="calibre8" id="pgfId-1146243"></a>You have invented a novel attention mechanism called AttentionX. Unlike Bahdanau attention, this attention mechanism takes encoder inputs and the decoder’s RNN outputs to produce the final output. The fully connected layers take this final output instead of the usual decoder’s RNN output. Given that, you’ve implemented the new attention mechanism in a layer called <span class="fm-code-in-text">AttentionX</span>. For encoder input <span class="fm-code-in-text">x</span> and decoder’s RNN output <span class="fm-code-in-text">y</span>, it can be called as</p>
  <pre class="programlisting">z = AttentionX()([x, y])</pre>

  <p class="body"><a class="calibre8" id="pgfId-1146245"></a>where the final output z is a [&lt;batch size&gt;, &lt;decoder time steps&gt;, &lt;hidden size&gt;]-sized output. How would you change the following decoder to use this new<a class="calibre8" id="marker-1146246"></a><a class="calibre8" id="marker-1146247"></a><a class="calibre8" id="marker-1146248"></a> attention<a class="calibre8" id="marker-1146249"></a><a class="calibre8" id="marker-1146250"></a> mechanism?</p>
  <pre class="programlisting">e_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input_final')          
fwd_state, bwd_state, en_states = encoder(e_inp)      
    
d_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_input')     
    
d_vectorized_out = vectorizer(d_inp)                                
        
d_emb_layer = tf.keras.layers.Embedding(
    n_vocab+2, 128, mask_zero=True, name='d_embedding'         
)    
d_emb_out = d_emb_layer(d_vectorized_out)     
    
d_init_state = tf.keras.layers.Concatenate(axis=-1)([fwd_state, bwd_state]) 
    
gru_out = tf.keras.layers.GRU(256, return_sequences=True)(
    d_emb_out, initial_state=d_init_state
)           
 
d_dense_layer_1 = tf.keras.layers.Dense(512, activation='relu', name='d_dense_1')
d_dense1_out = d_dense_layer_1(attn_out)         
    
d_final_layer = tf.keras.layers.Dense(
    n_vocab+2, activation='softmax', name='d_dense_final'
)
d_final_out = d_final_layer(d_dense1_out)   </pre>

  <h2 class="fm-head" id="sigil_toc_id_163"><a id="pgfId-1146276"></a>12.2 Visualizing the attention</h2>

  <p class="body"><a class="calibre8" id="pgfId-1146280"></a>You<a class="calibre8" id="marker-1146277"></a><a class="calibre8" id="marker-1146278"></a><a class="calibre8" id="marker-1146279"></a> have determined that the attention-based model works better than the one without attention. But you are skeptical and want to understand if the attention layer is producing meaningful outputs. For that, you are going to visualize attention patterns generated by the model for several input sequences.</p>

  <p class="body"><a class="calibre8" id="pgfId-1146281"></a>Apart from the performance, one of the lucrative advantages of the attention mechanism is the interpretability it brings along to the model. The normalized energy values, one of the interim outputs of the attention mechanism, can provide powerful insights into the model. Since the normalized energy values represent how much each encoder output contributed to decoding/translating at each decoding timestep, it can be used to generate a heatmap, highlighting the most important words in English that correspond to a particular German word.</p>

  <p class="body"><a class="calibre8" id="pgfId-1146282"></a>If we go back to the <span class="fm-code-in-text">DecoderRNNAttentionWrapper</span>, by calling it on a certain input, it produces two outputs:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1146283"></a>Decoder RNN output sequence</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1146284"></a>Alpha (i.e., normalized energy values) for all the encoder positions for every decoder position</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1146285"></a>The second output is what we are after. That tensor holds the key to unlocking the powerful interpretability brought by the attention mechanism.</p>

  <p class="body"><a class="calibre8" id="pgfId-1146287"></a>Let’s write a function called the <span class="fm-code-in-text">attention_visualizer</span><a class="calibre8" id="marker-1146286"></a><span class="fm-code-in-text">()</span> that will load the saved model and outputs not only the predictions of the model, but also the attention energies that will help us generate the final heatmap. In this function, we will load the model and create outputs by using the trained layers to retrace the interim and final outputs of the decoder, as shown in the next listing. This is similar to how we retraced the various steps in the model to create an inference model from the trained model.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1146289"></a>Listing 12.5 A model that visualizes attention patterns from input text</p>
  <pre class="programlisting">def attention_visualizer(save_path):
    """ Define the attention visualizer model """
    
    model = tf.keras.models.load_model(save_path)                  <span class="fm-combinumeral">❶</span>
        
    e_inp = tf.keras.Input(
        shape=(1,), dtype=tf.string, name='e_input_final'
    )                                                              <span class="fm-combinumeral">❷</span>
    en_model = model.get_layer("encoder")                          <span class="fm-combinumeral">❷</span>
    fwd_state, bwd_state, en_states = en_model(e_inp)              <span class="fm-combinumeral">❷</span>
    
    e_vec_out = en_model.get_layer("e_vectorizer")(e_inp)          <span class="fm-combinumeral">❸</span>
    
    d_inp = tf.keras.Input(
        shape=(1,), dtype=tf.string, name='d_infer_input'
    )                                                              <span class="fm-combinumeral">❹</span>
    
    d_vec_layer = model.get_layer('d_vectorizer')                  <span class="fm-combinumeral">❺</span>
    d_vec_out = d_vec_layer(d_inp)                                 <span class="fm-combinumeral">❺</span>
    
    d_emb_out = model.get_layer('d_embedding')(d_vec_out)          <span class="fm-combinumeral">❻</span>
        
    d_attn_layer = model.get_layer("d_attention")                  <span class="fm-combinumeral">❻</span>
            
    d_init_state = tf.keras.layers.Concatenate(axis=-1)(
        [fwd_state, bwd_state]
    )                                                              <span class="fm-combinumeral">❻</span>
 
    attn_out, attn_states = d_attn_layer(
        [en_states, d_emb_out], initial_state=d_init_state
    )                                                              <span class="fm-combinumeral">❻</span>
    
    d_dense1_out = model.get_layer("d_dense_1")(attn_out)          <span class="fm-combinumeral">❻</span>
    
    d_final_out = model.get_layer("d_dense_final")(d_dense1_out)   <span class="fm-combinumeral">❻</span>
    
    visualizer_model = tf.keras.models.Model(                      <span class="fm-combinumeral">❼</span>
        inputs=[e_inp, d_inp], 
        outputs=[d_final_out, attn_states, e_vec_out, d_vec_out]
    )
    
    return visualizer_model</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1154875"></a><span class="fm-combinumeral">❶</span> Load the model.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1154896"></a><span class="fm-combinumeral">❷</span> Define the encoder input for the model and get the final outputs of the encoder.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1154913"></a><span class="fm-combinumeral">❸</span> Get the encoder vectorizer (required to interpret the final output).</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1154933"></a><span class="fm-combinumeral">❹</span> Define the decoder input.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1154950"></a><span class="fm-combinumeral">❺</span> Get the decoder vectorizer and the output.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1154967"></a><span class="fm-combinumeral">❻</span> The next few steps just reiterate the steps in the trained model. We simply get the corresponding layers and pass the output of the previous step to the current step.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1154984"></a><span class="fm-combinumeral">❼</span> Here we define the final model to visualize attention patterns; we are interested in the attn_states output (i.e., normalized energy values). We will also need the vectorized token IDs to annotate the visualization.</p>

  <p class="body"><a class="calibre8" id="pgfId-1146341"></a>Note how the final model we defined returns four different outputs, as opposed to the trained model, which only returned the predictions. We will also need a <span class="fm-code-in-text">get_ vocabulary()</span> function that will load the saved vocabularies:</p>
  <pre class="programlisting">def get_vocabularies(save_dir):
    """ Load the vocabularies """
    
    with open(os.path.join(save_dir, 'en_vocab.json'), 'r') as f:
        en_vocabulary = json.load(f)
        
    with open(os.path.join(save_dir, 'de_vocab.json'), 'r') as f:
        de_vocabulary = json.load(f)
        
    return en_vocabulary, de_vocabulary</pre>

  <p class="body"><a class="calibre8" id="pgfId-1146353"></a>Finally, call these functions so that we have the vocabularies and the model ready:</p>
  <pre class="programlisting">print("Loading vocabularies")
en_vocabulary, de_vocabulary = get_vocabularies(
    os.path.join('models', 'seq2seq_attention_vocab')
)
 
print("Loading weights and generating the inference model")
visualizer_model = attention_visualizer(
    os.path.join('models', 'seq2seq_attention')
)
print("\tDone")</pre>

  <p class="body"><a class="calibre8" id="pgfId-1146364"></a>Next, we’ll move on to visualizing the outputs produced by the <span class="fm-code-in-text">visualizer_model</span>; we will be using the Python library <span class="fm-code-in-text">matplotlib</span><a class="calibre8" id="marker-1146365"></a> to visualize attention patterns for several examples. Let’s define a function called <span class="fm-code-in-text">visualize_attention</span><a class="calibre8" id="marker-1146366"></a><span class="fm-code-in-text">()</span> that takes in the <span class="fm-code-in-text">visualizer_model</span>, the two vocabularies, a sample English sentence, and the corresponding German translation (see the next listing). Then it will make a prediction on the inputs, retrieve the attention weights, generate a heatmap, and annotate the two axes with the English/German tokens.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1146368"></a>Listing 12.6 Visualizing attention patterns using input text</p>
  <pre class="programlisting">import matplotlib.pyplot as plt
%matplotlib inline
 
def visualize_attention(visualizer_model, en_vocabulary, de_vocabulary, 
<span class="fm-code-continuation-arrow">➥</span> sample_en_text, sample_de_text, fig_savepath):
    """ Visualize the attention patterns """
    
    print("Input: {}".format(sample_en_text))
    
    d_pred, attention_weights, e_out, d_out = visualizer_model.predict(
        [np.array([sample_en_text]), np.array([sample_de_text])]
    )                                                              <span class="fm-combinumeral">❶</span>
    
    d_pred_out = np.argmax(d_pred[0], axis=-1)                     <span class="fm-combinumeral">❷</span>
    
    y_ticklabels = []                                              <span class="fm-combinumeral">❸</span>
    for e_id in e_out[0]:                                          <span class="fm-combinumeral">❸</span>
        if en_vocabulary[e_id] == "":                              <span class="fm-combinumeral">❸</span>
            break                                                  <span class="fm-combinumeral">❸</span>
        y_ticklabels.append(en_vocabulary[e_id])                   <span class="fm-combinumeral">❸</span>
    
    x_ticklabels = []                                              <span class="fm-combinumeral">❹</span>
    for d_id in d_pred_out:                                        <span class="fm-combinumeral">❹</span>
        if de_vocabulary[d_id] == 'eos':                           <span class="fm-combinumeral">❹</span>
            break                                                  <span class="fm-combinumeral">❹</span>
        x_ticklabels.append(de_vocabulary[d_id])                   <span class="fm-combinumeral">❹</span>
            
    fig, ax = plt.subplots(figsize=(14, 14))
        
    attention_weights_filtered = attention_weights[
        0, :len(y_ticklabels), :len(x_ticklabels)
    ]                                                              <span class="fm-combinumeral">❺</span>
        
    im = ax.imshow(attention_weights_filtered)                     <span class="fm-combinumeral">❻</span>
    
    ax.set_xticks(np.arange(attention_weights_filtered.shape[1]))  <span class="fm-combinumeral">❼</span>
    ax.set_yticks(np.arange(attention_weights_filtered.shape[0]))  <span class="fm-combinumeral">❼</span>
    ax.set_xticklabels(x_ticklabels)                               <span class="fm-combinumeral">❼</span>
    ax.set_yticklabels(y_ticklabels)                               <span class="fm-combinumeral">❼</span>
 
    ax.tick_params(labelsize=20)                                   <span class="fm-combinumeral">❼</span>
    ax.tick_params(axis='x', labelrotation=90)                     <span class="fm-combinumeral">❼</span>
        
    plt.colorbar(im)                                               <span class="fm-combinumeral">❽</span>
    plt.subplots_adjust(left=0.2, bottom=0.2)
    
    save_dir, _ = os.path.split(fig_savepath)                      <span class="fm-combinumeral">❾</span>
    if not os.path.exists(save_dir):                               <span class="fm-combinumeral">❾</span>
        os.makedirs(save_dir, exist_ok=True)                       <span class="fm-combinumeral">❾</span>
    plt.savefig(fig_savepath)                                      <span class="fm-combinumeral">❾</span></pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1153863"></a><span class="fm-combinumeral">❶</span> Get the model predictions.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1153899"></a><span class="fm-combinumeral">❷</span> Get the token IDs of the predictions of the model.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1153916"></a><span class="fm-combinumeral">❸</span> Our y tick labels will be the input English words. We stop as soon as we see padding tokens.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1153933"></a><span class="fm-combinumeral">❹</span> Our x tick labels will be the predicted German words. We stop as soon as we see the EOS token.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1153950"></a><span class="fm-combinumeral">❺</span> We are going to visualize only the useful input and predicted words so that things like padded values and anything after the EOS token are discarded.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1153974"></a><span class="fm-combinumeral">❻</span> Generate the attention heatmap.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1153991"></a><span class="fm-combinumeral">❼</span> Set the x ticks, y ticks, and tick labels.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1154008"></a><span class="fm-combinumeral">❽</span> Generate the color bar to understand the value range found in the heat map.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1153864"></a><span class="fm-combinumeral">❾</span> Save the figure to the disk.</p>

  <p class="body"><a class="calibre8" id="pgfId-1146428"></a>First, we input the English and German input text to the model to generate a prediction. We need to input both the English and German inputs as we are still using the teacher-forced model. You might be wondering, “Does that mean I have to have the German translation ready and can only visualize attention patterns in the training mode?” Of course not! You can have an inference model defined, like we did in a previous section in this chapter, and still visualize the attention patterns. We are using the trained model itself to visualize patterns, as I want to focus on visualizing attention patterns rather than defining the inference model (which we already did for another model).</p>

  <p class="body"><a class="calibre8" id="pgfId-1146429"></a>Once the predictions and attention weights are obtained, we define two lists: <span class="fm-code-in-text">x_ticklabels</span> and <span class="fm-code-in-text">y_ticklabels</span>. They will be the labels (i.e., English/German words) you see on the two axes in the heatmap. We will have the English words on the row dimension and German words in the column dimension (figure 12.1). We will also do a simple filtering to get rid of paddings (i.e., <span class="fm-code-in-text">""</span>) and German text appearing after the EOS token and get the attention weights within the range that satisfy these two criteria. You can then simply call the <span class="fm-code-in-text">matplotlib</span>’s <span class="fm-code-in-text">imshow()</span> function<a class="calibre8" id="marker-1151341"></a> to generate the heatmap and set the axes’ ticks and the labels for those ticks. Finally, the figure is saved to the disk.</p>

  <p class="body"><a class="calibre8" id="pgfId-1146431"></a> Let’s give this a trial run! Let’s take a few examples from our test DataFrame and visualize attention patterns. We will create 10 visualizations and will also make sure that those 10 examples we choose have at least 10 English words to make sure we don’t visualize very short phrases:</p>
  <pre class="programlisting"># Generate attention patterns for a few inputs
i = 0
j = 0
while j&lt;9:
    sample_en_text = test_df["EN"].iloc[i]
    sample_de_text = test_df["DE"].iloc[i:i+1].str.rsplit(n=1, 
<span class="fm-code-continuation-arrow">➥</span> expand=True).iloc[:,0].tolist()
    i += 1
 
    if len(sample_en_text.split(" ")) &gt; 10:
        j += 1
    else:
        continue
    
    visualize_attention(
        visualizer_model, en_vocabulary, de_vocabulary, sample_en_text, 
        sample_de_text, os.path.join('plots','attention_{}.png'.format(i))
    )</pre>

  <p class="body"><a class="calibre8" id="pgfId-1146450"></a>If you run this code successfully, you should get 10 attention visualizations shown and stored on the disk. In figures 12.1 and 12.2, we show two such visualizations.</p>

  <p class="fm-figure"><img alt="12-01" class="calibre10" src="../../OEBPS/Images/12-01.png" width="825" height="800"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1158099"></a>Figure 12.1 Attention patterns visualized for an input English text</p>

  <p class="body"><a class="calibre8" id="pgfId-1146463"></a>In the figures, the lighter the color, the more the model has paid attention to that word. In figure 12.1, we can see that, when translating the words, “und” and “maria,” the model has mostly paid attention to “and” and “mary,” respectively. If you go to Google Translate and do the German translations for the word “and,” for example, you will see that this is, in fact, correct. In figure 12.2, we can see that when generating “hast keine nicht,” the model has paid attention to the phrase “have no idea.” The other observation we can make is that the attention pattern falls roughly diagonally. This makes sense as both these languages roughly follow the same writing style.</p>

  <p class="body"><a class="calibre8" id="pgfId-1158144"></a> </p>

  <p class="fm-figure"><img alt="12-02" class="calibre10" src="../../OEBPS/Images/12-02.png" width="825" height="772"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1158150"></a>Figure 12.2 Attention patterns visualized for an input English text</p>

  <p class="body"><a class="calibre8" id="pgfId-1146464"></a>This concludes our discussion about sequence-to-sequence models. In the next chapter, we will discuss a family of models that has been writing the state-of-the-art of machine learning for a few years: Transformers.</p>

  <p class="fm-head2"><a id="pgfId-1146465"></a>Exercise 2</p>

  <p class="body"><a class="calibre8" id="pgfId-1146466"></a>You have an attention matrix given by <span class="fm-code-in-text">attention_matrix</span>, with English words given by <span class="fm-code-in-text">english_text_labels</span> and German words given by <span class="fm-code-in-text">german_text_labels</span>. How would you create a visualization similar to figure 12.1? Here, you will need to use the <span class="fm-code-in-text">imshow()</span>, <span class="fm-code-in-text">set_xticks()</span>, <span class="fm-code-in-text">set_yticks()</span>, <span class="fm-code-in-text">set_xticklabels()</span>, and <span class="fm-code-in-text">set_yticklabels</span><a class="calibre8" id="marker-1146467"></a><a class="calibre8" id="marker-1146468"></a><a class="calibre8" id="marker-1146469"></a><span class="fm-code-in-text">()</span> functions.</p>

  <h2 class="fm-head" id="sigil_toc_id_164"><a id="pgfId-1146470"></a>Summary</h2>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1146471"></a>Using attention in sequence-to-sequence models can greatly help shoot their performance up.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1146472"></a>Using attention at each decoding time step, the decoder gets to see all the historical outputs of the encoder and select and mix these outputs to come up with an aggregated (e.g., summed) representation of that, which gives a holistic view of what was in the encoder input.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1146473"></a>One of the intermediate products in the attention computation is the normalized energy values, which give a probability distribution of how important each encoded position was for decoding a given time step for every decoding step. In other words, this is a matrix that has a value for every encoder time step and decoder time step combination. This can be visualized as a heatmap and can be used to interpret which words the decoder paid attention to when translating a certain token in the decoder.</p>
    </li>
  </ul>

  <h2 class="fm-head" id="sigil_toc_id_165"><a id="pgfId-1146474"></a>Answers to exercises</h2>

  <p class="body"><a class="calibre8" id="pgfId-1146475"></a><b class="fm-bold">Exercise 1</b></p>
  <pre class="programlisting">e_inp = tf.keras.Input(
    shape=(1,), dtype=tf.string, name='e_input_final'
)          
fwd_state, bwd_state, en_states = encoder(e_inp)      
    
d_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_input')     
    
d_vectorized_out = vectorizer(d_inp)                                
        
d_emb_layer = tf.keras.layers.Embedding(
    n_vocab+2, 128, mask_zero=True, name='d_embedding'         
)    
d_emb_out = d_emb_layer(d_vectorized_out)     
    
d_init_state = tf.keras.layers.Concatenate(axis=-1)([fwd_state, bwd_state]) 
    
gru_out = tf.keras.layers.GRU(256, return_sequences=True)(
    d_emb_out, initial_state=d_init_state
)           
 
<b class="fm-code-bold">attn_out = AttentionX()([en_states, gru_out])</b>
 
d_dense_layer_1 = tf.keras.layers.Dense(
    512, activation='relu', name='d_dense_1'
)
d_dense1_out = d_dense_layer_1(attn_out)         
    
d_final_layer = tf.keras.layers.Dense(
    n_vocab+2, activation='softmax', name='d_dense_final'
)
d_final_out = d_final_layer(d_dense1_out)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1146508"></a><b class="fm-bold">Exercise 2</b><a class="calibre8" id="marker-1146507"></a></p>
  <pre class="programlisting">im = ax.imshow(attention_matrix)  
    
ax.set_xticks(np.arange(attention_matrix.shape[1])) 
ax.set_yticks(np.arange(attention_matrix.shape[0])) 
 
ax.set_xticklabels(german_text_labels)  
ax.set_yticklabels(english_text_labels)</pre>
</div>
</div>
</body>
</html>