<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xml:lang="en"
      lang="en"
      xmlns="http://www.w3.org/1999/xhtml"
      xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<title>TensorFlow in Action</title>
<link rel="stylesheet" type="text/css" href="../../override_v1.css"/>
<link rel="stylesheet" type="text/css" href="../../stylesheet.css"/><link rel="stylesheet" type="text/css" href="../../page_styles.css"/>
</head>
<body>
<div id="book-content">
<div id="sbo-rt-content" class="calibre"><h1 class="tochead"><a id="pgfId-1083341"></a>4 Dipping toes in deep learning</h1>

  <p class="co-summary-head"><a id="pgfId-1083343"></a>This chapter<a id="marker-1085129"></a> covers</p>

  <ul class="calibre9">
    <li class="co-summary-bullet"><a class="calibre8" id="pgfId-1083344"></a>Implementing and training fully connected neural networks using Keras</li>

    <li class="co-summary-bullet"><a class="calibre8" id="pgfId-1083345"></a>Implementing and training convolutional neural networks to classify images</li>

    <li class="co-summary-bullet"><a class="calibre8" id="pgfId-1083346"></a>Implementing and training a recurrent neural network to solve a time-series problem</li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1083347"></a>In chapter 3, you learned about the different model-building APIs provided by TensorFlow and their advantages and disadvantages. You also learned about some of the options in TensorFlow to retrieve and manipulate data. In this chapter, you will learn how to leverage some of that to build deep neural networks and use them to solve problems.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083348"></a><i class="fm-italics">Deep learning</i> is a broad term that has many different algorithms under its wings. Deep learning algorithms come in many different flavors and colors and can be classified by many criteria: the type of data they consume (e.g., structured data, images, time-series data), depth (shallow, deep, and very deep), and so on. The main types of deep networks we are going to discuss and implement are as follows:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1083349"></a>Fully connected networks (FCNs)</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1083350"></a>Convolutional neural networks (CNNs)</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1083351"></a>Recurrent neural networks (RNNs)</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1083352"></a>Being able to comfortably implement these neural networks is a key skill to be successful in the field, whether you are a graduate student, a data scientist, or a research scientist. This knowledge directly extends to becoming skillful in implementing more complex deep neural networks that deliver state-of-the-art performance in various problem domains.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083353"></a>In chapter 2, we discussed FCN and various operations in CNNs, such as convolution and pooling operations. In this chapter, you will see the FCNs again, as well as a holistic implementation of CNNs showing how convolution and pooling operations coalesce to form a CNN. Finally, you will learn about a new type of model: RNNs. RNNs are typically used to solve time-series problems, where the task is to learn patterns in data over time so that, by looking at the past patterns, we can leverage them to forecast the future. We will also see how RNNs are used to solve an exciting real-world time-series problem.</p>

  <h2 class="fm-head" id="sigil_toc_id_47"><a id="pgfId-1083354"></a>4.1 Fully connected networks</h2>

  <p class="body"><a class="calibre8" id="pgfId-1083357"></a><i class="fm-italics">You</i><a class="calibre8" id="marker-1083355"></a><a class="calibre8" id="marker-1083356"></a> <i class="fm-italics">have found some precious photos of your grandmother while going through some storage boxes you found in the attic. Unfortunately, they have seen better days. Most of the photos are scratched, smudged, and even torn. You know that recently deep networks have been used to restore old photos and videos. In the hope of restoring these photos, you decide to implement an image restoration model</i> using TensorFlow. You will first develop a model that can restore corrupted images of handwritten digits, as this data set is readily available, in order to understand the model and the training process. You believe an autoencoder model (a type of FCN) would be a great starting point. This autoencoder will have the following specifications:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1083358"></a>Input layer with 784 nodes</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1083359"></a>A hidden layer with 64 nodes, having ReLU activation</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1083360"></a>A hidden layer with 32 nodes, having ReLU activation</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1083361"></a>A hidden layer with 64 nodes, having ReLU activation</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1083362"></a>An output layer with 784 nodes with tanh activation</p>
    </li>
  </ul>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1083363"></a>Hyperparameter optimization for deep learning</p>

    <p class="fm-sidebar-text"><a id="pgfId-1083364"></a>You might have noticed that when defining neural networks, we are choosing structural hyperparameters (e.g., number of units in hidden layers) somewhat arbitrarily. These values have, in fact, been chosen empirically through a few rounds of trial and error.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1087399"></a>Typically, in machine learning, these hyperparameters are chosen using a principled approach, such as hyperparameter optimization. But hyperparameter optimization is an expensive process that needs to evaluate hundreds of models with different hyperparameter choices to choose the best set of hyperparameters. This makes it very difficult to use for deep learning methods, as these methods usually deal with large, complex models and large amounts of data.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1083366"></a>Therefore, in deep learning, you will commonly see the following trends, in order to limit the time spent on hyperparameter optimization:</p>

    <ul class="calibre9">
      <li class="fm-sidebar-number">
        <p class="list1"><a id="pgfId-1083368"></a>Optimizing a subset of hyperparameters to limit the exploration space (e.g., type of activation instead of number of hidden units, regularization parameters, etc.)</p>
      </li>

      <li class="fm-sidebar-number">
        <p class="list1"><a id="pgfId-1083369"></a>Using robust optimizers, early stopping, learning rate decay, and so on, which are designed to reduce or prevent overfitting</p>
      </li>

      <li class="fm-sidebar-number">
        <p class="list1"><a id="pgfId-1083370"></a>Using model specifications from published models that have delivered state-of-the-art performance</p>
      </li>

      <li class="fm-sidebar-number">
        <p class="list1"><a id="pgfId-1083371"></a>Following rules of thumb such as reducing the output size as you go deeper into the network</p>
      </li>
    </ul>

    <p class="fm-sidebar-text"><a id="pgfId-1083373"></a>In this chapter, we will use model architectures chosen empirically. The focus of this chapter is to show how a given architecture can be implemented using TensorFlow 2 and not to find the architectures themselves.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1083375"></a>Let’s examine the data we’ll use to implement the FCN.</p>

  <h3 class="fm-head1" id="sigil_toc_id_48"><a id="pgfId-1083376"></a>4.1.1 Understanding the data</h3>

  <p class="body"><a class="calibre8" id="pgfId-1083380"></a>For this scenario, we will use the MNIST digit data set, a simple data set that contains the black-and-white images of hand-written digits and the corresponding labels representing the digits. Each image has a single digit and goes from 0-9. Therefore, the data set has 10 different classes. Figure 4.1 shows several samples from the data set along with the digit it represents.</p>

  <p class="fm-figure"><img alt="04-01" class="calibre10" src="../../OEBPS/Images/04-01.png" width="1033" height="382"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1096114"></a>Figure 4.1 Sample digit images. Each image contains a number from 0 to 9.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083387"></a>In TensorFlow, you can load the MNIST data set with a single line. Loading this data set has become an integral part of various machine learning libraries (including TensorFlow) due to its extremely common usage:</p>
  <pre class="programlisting">from tensorflow.keras.datasets.mnist import load_data
(x_train, y_train), (x_test, y_test) = load_data()</pre>

  <p class="body"><a class="calibre8" id="pgfId-1083391"></a>The <span class="fm-code-in-text">load_data()</span> method<a class="calibre8" id="marker-1083390"></a> returns two tuples: training data and testing data. Here, we will only use the training images (i.e., <span class="fm-code-in-text">x_train</span>) data set. As we covered earlier, this is an unsupervised task. Because of that, we will not need the labels (i.e., <span class="fm-code-in-text">y_train</span>) of the images to complete this task.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1083392"></a>Better than MNIST?</p>

    <p class="fm-sidebar-text"><a id="pgfId-1083393"></a>It should be noted that, due to advancements in the field of computer vision over the last decade, MNIST is considered too easy, where a test accuracy of more than 92% can be achieved with a simple logistic regression model (<span class="fm-hyperlink"><a class="url" href="http://mng.bz/j2l9">http://mng.bz/j2l9</a></span>) and a 99.84% accuracy with a state-of-the-art model (<span class="fm-hyperlink"><a class="url" href="http://mng.bz/d2Pv">http://mng.bz/d2Pv</a></span>). Furthermore, it’s being overused in the computer vision community. Because of this, a new data set known as Fashion-MNIST (<span class="fm-hyperlink"><a class="url" href="https://github.com/zalandoresearch/fashion-mnist">https://github.com/zalandoresearch/fashion-mnist</a></span>) has emerged. This is a black-and-white data set containing images belonging to 10 classes. Instead of digits, it contains images of various fashion categories (e.g., T-shirt, sandal, bag, etc.), which poses a much harder problem than recognizing digits.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1083398"></a>You can print <span class="fm-code-in-text">x_train</span> and <span class="fm-code-in-text">y_train</span> to understand those arrays a bit better using</p>
  <pre class="programlisting">print(x_train)
print('x_train has shape: {}'.format(x_train.shape))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1083401"></a>This will produce</p>
  <pre class="programlisting">[[[0 0 0 ... 0 0 0]
  [0 0 0 ... 0 0 0]
  [0 0 0 ... 0 0 0]
  ...
  [0 0 0 ... 0 0 0]
  [0 0 0 ... 0 0 0]
  [0 0 0 ... 0 0 0]]
 
 ...
 
 [[0 0 0 ... 0 0 0]
  [0 0 0 ... 0 0 0]
  [0 0 0 ... 0 0 0]
  ...
  [0 0 0 ... 0 0 0]
  [0 0 0 ... 0 0 0]
  [0 0 0 ... 0 0 0]]]
 
x_train has shape: (60000, 28, 28)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1083422"></a>And do the same for <span class="fm-code-in-text">y_train</span>:</p>
  <pre class="programlisting">print(y_train)
print('y_train has shape: {}'.format(y_train.shape))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1083425"></a>This will give</p>
  <pre class="programlisting">[5 0 4 ... 5 6 8]
 
y_train has shape: (60000,)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1083429"></a>Then we will do some basic data preprocessing. We will normalize all samples in the data set by bringing their pixel values from [0, 255] to [-1, 1]. This is done by subtracting 128 and dividing the result by 128 element-wise for all pixels. This is important because the final layer of the autoencoder has a tanh activation, which ranges between (-1, 1). Tanh is a nonlinear activation function like the sigmoid function, and for a given input, <i class="fm-italics">x</i> is computed as follows:</p>

  <p class="fm-equation"><img alt="04_01a" class="calibre10" src="../../OEBPS/Images/04_01a.png" width="200" height="89"/><br class="calibre2"/>
  <a id="pgfId-1085614"></a></p>

  <p class="body"><a class="calibre8" id="pgfId-1083434"></a>Therefore, we need to make sure we feed something to the model that is within the range of values that can be produced by the final layer. Also, if you look at the shape of <span class="fm-code-in-text">x_train</span>, you will see that it has a shape of (60000, 28, 28). The autoencoder takes a one-dimensional input, so we need to reshape the image to a one-dimensional vector of size 784. Both these transformations can be achieved by the following line:</p>
  <pre class="programlisting">norm_x_train = ((x_train - 128.0)/128.0).reshape([-1,784])</pre>

  <p class="body"><a class="calibre8" id="pgfId-1083436"></a>Here, <span class="fm-code-in-text">reshape([-1, 784])</span> will unwrap the two-dimensional images (size 28 × 28) in the data set to a single dimensional vector (size 784). When reshaping, you do not need to provide all the dimensions of the reshaped tensor. If you provide the sizes of all dimensions except one, NumPy can still infer the size of the missing dimension as it knows the dimensions of the original tensor. The dimension that you want NumPy to infer is denoted by -1.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083437"></a>You might be wondering, “These images look crisp and clean. How on earth can we train our model to restore corrupted images?” That’s a very easy fix. All we need to do is synthesize a corresponding corrupted set of images from the original set of images. For that, we will define the <span class="fm-code-in-text">generate_masked_inputs(...)</span> function:</p>
  <pre class="programlisting">import numpy as np
 
def generate_masked_inputs(x, p, seed=None):
    if seed:
        np.random.seed(seed)
    mask = np.random.binomial(n=1, p=p, size=x.shape).astype('float32')
    return x * mask
 
masked_x_train = generate_masked_inputs(norm_x_train, 0.5)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1083448"></a>This function will set pixels randomly (with 50% probability) to zero. But let’s inspect what we are doing in more detail. First, we will give the option to set a random seed so that we can deterministically change the generated random masks. We are creating a mask of 1s and 0s using the binomial distribution, which is the same size as <span class="fm-code-in-text">norm_x_train</span>. In simple words, the binomial distribution represents the probability of heads (1) or tails (0) if you flip a coin several times. The binomial distribution has several important parameters:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1083449"></a><span class="fm-code-in-text">N</span>—Number of trials</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1083450"></a><span class="fm-code-in-text">P</span>—Probability of a success (1)</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1083451"></a><span class="fm-code-in-text">Size</span>—The number of tests (i.e., trial sets)</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1083453"></a>Here, we have <span class="fm-code-in-text">x.shape</span> tests and one trial in each with a 50% success probability. Then this mask is multiplied element-wise with the original tensor. This will result in black pixels randomly distributed over the image (figure 4.2).</p>

  <p class="fm-figure"><img alt="04-02" class="calibre10" src="../../OEBPS/Images/04-02.png" width="1033" height="383"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1096148"></a>Figure 4.2 Some of the synthetically corrupted images</p>

  <p class="body"><a class="calibre8" id="pgfId-1083460"></a>Next, let’s discuss the fully connected network we’ll be implementing. It’s called an autoencoder model.</p>

  <h3 class="fm-head1" id="sigil_toc_id_49"><a id="pgfId-1083464"></a><a id="marker-1083465"></a><a id="marker-1083467"></a>4.1.2 Autoencoder model</h3>

  <p class="body"><a class="calibre8" id="pgfId-1083469"></a>Both<a class="calibre8" id="marker-1096208"></a><a class="calibre8" id="marker-1096209"></a> the autoencoder model and the multilayer perceptron (MLP<a class="calibre8" id="marker-1096210"></a><a id="marker-1083468"></a>) model (from chapter 1) are FCNs. These are called FCNs because all the input nodes are connected to all the output nodes, in every layer of the network. Autoencoders operate in a similar way to the multilayer perceptron. In other words, the computations (e.g., forward pass) you see in an autoencoder are exactly the same as in an MLP. However, the final objectives of the two are different. An MLP is trained to solve a supervised task (e.g., classifying flower species), whereas an autoencoder is trained to solve an unsupervised task (e.g., reconstructing the original image, given a corrupted/noisy image). Let’s now delve into what an autoencoder actually does.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1083470"></a>Supervised versus unsupervised learning</p>

    <p class="fm-sidebar-text"><a id="pgfId-1083471"></a>In supervised learning, a model is trained using a labeled data set. Each input (e.g., image/audio/movie review) has a corresponding label (e.g., object class for images, sentiment of the review) or continuous value(s) (e.g., bounding boxes of an object for images). Some examples of supervised tasks are image classification, object detection, speech recognition, and sentiment analysis.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1083472"></a>In unsupervised learning, the models are trained using unlabeled data (e.g., images/audio/text extracted from websites without any labeling). The training process varies significantly depending on the final expected outcome. For example, autoencoders are trained to reconstruct images as a pretraining step for an image-based supervised learning task. Some examples of unsupervised tasks are image reconstruction, image generation using generative adversarial networks, text clustering, and language modeling.</p>
  </div>

  <p class="fm-figure"><img alt="04-03" class="calibre10" src="../../OEBPS/Images/04-03.png" width="983" height="483"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1096182"></a>Figure 4.3 A simple autoencoder with one layer for compression and another layer for reconstruction. The black and white rectangles in the input image are the pixels present in the image.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083473"></a>Figure 4.3 depicts a simple autoencoder with two layers. An autoencoder has two phases in its functionality:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1083475"></a><i class="fm-italics">Compression phase</i>—Compresses a given image (i.e., the corrupted image) to a compressed hidden (i.e., latent) representation</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1083477"></a><i class="fm-italics">Reconstruction phase</i><a class="calibre8" id="marker-1083476"></a>—Reconstructs the original image from the hidden representation</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1083478"></a>In the compression phase, a compressed hidden representation is computed as follows</p>

  <p class="fm-equation"><a id="pgfId-1083479"></a><i class="fm-italics">h</i><sub class="fm-subscript">1</sub> = <i class="fm-italics">ReLU</i>(<i class="fm-italics">xW</i><sub class="fm-subscript">1</sub> + <i class="fm-italics">b</i><sub class="fm-subscript">1</sub>)</p>

  <p class="body"><a class="calibre8" id="pgfId-1083480"></a>where <i class="fm-timesitalic">W</i><sub class="fm-subscript">1</sub>,<i class="fm-timesitalic">b</i><sub class="fm-subscript">1</sub> are the weights and biases of the first compression layer and <i class="fm-timesitalic">h</i><sub class="fm-subscript">1</sub> is the final hidden representation of the layer.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083481"></a>Similarly, we compute the output of the reconstruction layers:</p>

  <p class="fm-equation"><a id="pgfId-1083482"></a><i class="fm-italics">ŷ</i> = <i class="fm-italics">ReLU</i>(<i class="fm-italics">h</i><sub class="fm-subscript">1</sub> <i class="fm-italics">W</i><sub class="fm-subscript">2</sub> + <i class="fm-italics">b</i><sub class="fm-subscript">2</sub>)</p>

  <p class="body"><a class="calibre8" id="pgfId-1083483"></a>This is known as the forward pass, as you are going from the input to the output. Then you compute a loss (e.g., mean squared error [MSE]) between the expected output (i.e., target) and the prediction. For example, mean squared error for a single image is computed as</p>

  <p class="fm-equation"><img alt="04_03a" class="calibre10" src="../../OEBPS/Images/04_03a.png" width="219" height="84"/><br class="calibre2"/>
  <a id="pgfId-1085664"></a></p>

  <p class="body"><a class="calibre8" id="pgfId-1083488"></a>where <i class="fm-timesitalic">D</i> is the dimensionality of the data (784 in our example), <i class="fm-timesitalic">y</i><sub class="fm-subscript">j</sub> is the <i class="fm-timesitalic">j</i><sup class="fm-superscript">th</sup> pixel in our image, and (<i class="fm-timesitalic">ŷ</i><sub class="fm-subscript">j</sub>) is the <i class="fm-timesitalic">j</i><sup class="fm-superscript">th</sup> pixel of the predicted image. We compute this loss for each batch of images and optimize the model parameters to minimize the computed loss. This is known as the backward pass.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083495"></a>You can have an arbitrary number of compression and reconstruction layers. In our assignment, we need to have two compression layers and two reconstruction layers (see the next listing).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1083497"></a>Listing 4.1 The denoising autoencoder model</p>
  <pre class="programlisting">from tensorflow.keras import layers, models
 
autoencoder = models.Sequential(
    [layers.Dense(64, activation='relu', input_shape=(784,)),  <span class="fm-combinumeral">❶</span>
    layers.Dense(32, activation='relu'),                       <span class="fm-combinumeral">❶</span>
    layers.Dense(64, activation='relu'),                       <span class="fm-combinumeral">❶</span>
    layers.Dense(784, activation='tanh')]                      <span class="fm-combinumeral">❶</span>
)
autoencoder.compile(loss='mse', optimizer='adam')              <span class="fm-combinumeral">❷</span>
autoencoder.summary()                                          <span class="fm-combinumeral">❸</span></pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1095392"></a><span class="fm-combinumeral">❶</span> Defining four Dense layers, three with ReLU activation and one with tanh activation</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1095420"></a><span class="fm-combinumeral">❷</span> Compiling the model with a loss function and an optimizer</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1095437"></a><span class="fm-combinumeral">❸</span> Printing the summary</p>

  <p class="body"><a class="calibre8" id="pgfId-1083512"></a>Let’s go over what we did in more detail. The first thing you should notice is that we used the Keras Sequential API for this task. This makes sense as this is a very simple deep learning model. Next, we added four <span class="fm-code-in-text">Dense</span> layers<a class="calibre8" id="marker-1083513"></a>. The first <span class="fm-code-in-text">Dense</span> layer<a class="calibre8" id="marker-1083514"></a> takes an input with 784 features and produces a 64-elements-long vector. Then the second layer takes the 64-elements-long vector and produces a 32-elements-long vector. The third dense layer takes the 32-elements-long vector and produces a 64-elements-long vector, passing it on to the final layer, which produces a 784-elements-long vector (i.e., size of the input). The first three layers have ReLU activation, and the last layer has a tanh activation, as the last layer needs to produce values between (-1, 1). Let’s remind ourselves how the ReLU and tanh activations are computed:</p>

  <p class="fm-equation"><i class="fm-italics">ReLU</i>(<i class="fm-italics">x</i>) = max (0, <i class="fm-italics">x</i>)</p>

  <p class="fm-equation"><img alt="04_03b" class="calibre10" src="../../OEBPS/Images/04_03b.png" width="199" height="77"/><br class="calibre2"/>
  <a id="pgfId-1085723"></a></p>

  <p class="body"><a class="calibre8" id="pgfId-1083519"></a>Finally, we compile the model using the mean squared error as the <span class="fm-code-in-text">loss</span> function and <span class="fm-code-in-text">adam</span><a class="calibre8" id="marker-1083520"></a> as the optimizer. The model we just described has the specifications we defined at the beginning of the section. With the model defined, you can now train the model. You will train the model for 10 epochs with batches of size 64:</p>
  <pre class="programlisting">history = autoencoder.fit(masked_x_train, norm_x_train, batch_size=64, epochs=10)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1083523"></a>The masked inputs we generated become the input, and the original images will be the ground truth. When you train the model, you will see a loss that goes down over time:</p>
  <pre class="programlisting">Train on 60000 samples
Epoch 1/10
60000/60000 [==============================] - 4s 72us/sample - loss: 0.1496
Epoch 2/10
60000/60000 [==============================] - 4s 67us/sample - loss: 0.0992
Epoch 3/10
...
60000/60000 [==============================] - 4s 66us/sample - loss: 0.0821
Epoch 8/10
60000/60000 [==============================] - 4s 66us/sample - loss: 0.0801
Epoch 9/10
60000/60000 [==============================] - 4s 67us/sample - loss: 0.0787
Epoch 10/10
60000/60000 [==============================] - 4s 67us/sample - loss: 0.0777</pre>

  <p class="body"><a class="calibre8" id="pgfId-1083538"></a>It seems the error (i.e., <span class="fm-code-in-text">loss</span> value) has gone down from approximately 0.15 to roughly 0.078. This is a strong indication that the model is learning to reconstruct images. You can get similar results by setting the seed using the <span class="fm-code-in-text">fix_random_seed(...)</span> function<a class="calibre8" id="marker-1087768"></a> we used in chapter 2 (provided in the notebook). Note that for this task we cannot define a metric like accuracy, as it is an unsupervised task.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1083540"></a>Denoising autoencoders</p>

    <p class="fm-sidebar-text"><a id="pgfId-1083541"></a>Normally an autoencoder maps a given input to a small latent space and then back to the original input space to reconstruct the original images. However, here we use the autoencoder for a special purpose: to restore original images or denoise original images. Such autoencoders are known as <i class="fm-italics">denoising</i><a id="marker-1087773"></a>. Read more about denoising autoencoders at <span class="fm-hyperlink"><a class="url" href="http://mng.bz/WxyX">http://mng.bz/WxyX</a></span>.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1083544"></a>Let’s now see what the trained model can do! It should now be able to decently restore an image of a corrupted digit. And to make things interesting, let’s make sure we generate a mask that the training data has not seen:</p>
  <pre class="programlisting">x_train_sample = x_train[:10]
y_train_sample = y_train[:10]
 
masked_x_train_sample = generate_masked_inputs(x_train_sample, 0.5, seed=2048)
norm_masked_x = ((x_train - 128.0)/128.0).reshape(-1, 784)
 
y_pred = autoencoder.predict(norm_masked_x)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1083552"></a>Here, we will be using the first 10 images in the data set to test out the model we just trained. However, we are making sure that the random mask is different by changing the seed. You can display some information about <span class="fm-code-in-text">y_pred</span> using</p>
  <pre class="programlisting">print(y_pred)
print('y_pred has shape: {}'.format(y_pred.shape))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1083555"></a>which will give</p>
  <pre class="programlisting">[[-0.99999976 -0.99999976 -0.99999976 ... -0.99999976 -0.99999976
  -0.99999976]
 [-0.99999976 -0.99999976 -0.99999976 ... -0.99999976 -0.99999976
  -0.99999976]
 [-0.99999976 -0.99999976 -0.99999976 ... -0.99999976 -0.99999976
  -0.99999976]
 ...
 [-0.99999976 -0.99999976 -0.9999996  ... -0.99999946 -0.99999976
  -0.99999976]
 [-0.99999976 -0.99999976 -0.99999976 ... -0.99999976 -0.99999976
  -0.99999976]
 [-0.99999976 -0.99999976 -0.99999976 ... -0.99999976 -0.99999976
  -0.99999976]]
 
y_pred has shape: (60000, 784)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1083571"></a>Finally, you can visualize what the model does by plotting the images (the code provided in the notebook). Figure 4.4 illustrates the corrupted images (top row) and the outputs of the model (bottom row). Though you are not yet restoring real-world photos of your grandmother, this a great start, as you now know the approach to follow.</p>

  <p class="fm-figure"><img alt="04-04" class="calibre10" src="../../OEBPS/Images/04-04.png" width="1133" height="233"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1096232"></a>Figure 4.4 Images restored by the model. It seems our model is doing a good job.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083578"></a>You might be wondering, “What do autoencoders help you to achieve in general?” Autoencoders are a great tool for learning unsupervised features from unlabeled data, which is handy when solving more interesting downstream tasks like image classification. When autoencoders are trained on an unsupervised task, they learn useful features for other tasks (e.g., image classification). Therefore, training an autoencoder model to classify images will get you to a well-performing model faster and with less labeled data than training a model from scratch. As you are probably aware, there’s much more unlabeled data in the world than labeled data, as labeling usually requires human intervention, which is time-consuming and expensive. Another use of autoencoders is that the hidden representation it produces can be used as a low-dimensional proxy to cluster the images.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083579"></a>In this section, you learned about the autoencoder model, which is a type of FCN and is used to reconstruct/restore damaged images in an unsupervised manner. This is a great way to leverage copious amounts of unlabeled data to pretrain models, which becomes useful in more downstream interesting tasks (e.g., image classification). You first learned the architecture and then how to implement an autoencoder model with the Keras Sequential API. Finally, you trained the model on a hand-written image data set (MNIST) to reconstruct the images in the data set. During the training process, to ensure the model was learning, you monitored the loss to make sure it decreased over time. Finally, you used the model to predict restorations of corrupted images and ensured the model was performing well.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083580"></a>In the next section, we will discuss a different type of deep learning network that has revolutionized the field of computer vision: CNNs.</p>

  <p class="fm-head2"><a id="pgfId-1083581"></a>Exercise 1</p>

  <p class="body"><a class="calibre8" id="pgfId-1083582"></a>Implement an autoencoder model that takes in a 512-elements-long vector. The network has a 32-node layer, a 16-node layer, and finally an output layer. In total, there are three layers. All these layers have the<a class="calibre8" id="marker-1083583"></a><a class="calibre8" id="marker-1083585"></a> sigmoid<a class="calibre8" id="marker-1083586"></a><a class="calibre8" id="marker-1083587"></a> activation.</p>

  <h2 class="fm-head" id="sigil_toc_id_50"><a id="pgfId-1083588"></a>4.2 Convolutional neural networks</h2>

  <p class="body"><a class="calibre8" id="pgfId-1083591"></a>You<a class="calibre8" id="marker-1083589"></a><a class="calibre8" id="marker-1083590"></a> have been working at a startup as a data scientist trying to model traffic congestion on the road. One important model in the company’s solution is building a model to predict whether a vehicle is present, given a patch or image, as a part of a larger plan. You plan to develop a model first on the cifar-10 data set and see how well it classifies vehicles. This is a great idea, as it will give a rough approximation of the feasibility of the idea while spending minimal time and money on labeling custom data. If we can achieve good accuracy on this data set, that is a very positive sign. You have learned that CNNs are great for computer vision tasks. So, you are planning to implement a CNN.</p>

  <h3 class="fm-head1" id="sigil_toc_id_51"><a id="pgfId-1083592"></a>4.2.1 Understanding the data</h3>

  <p class="body"><a class="calibre8" id="pgfId-1083596"></a>We<a class="calibre8" id="marker-1083593"></a> will use is the cifar-10 data set. We briefly looked at this data set in the previous chapter, and it is a great cornerstone for this task. It has various vehicles (e.g., automobile, truck) and other objects (e.g., dog, cat) as classes. Figure 4.5 illustrates some of the classes and corresponding samples for them.</p>

  <p class="fm-figure"><img alt="04-05" class="calibre10" src="../../OEBPS/Images/04-05.png" width="900" height="399"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1096266"></a>Figure 4.5 Sample images from cifar-10 data set along with their labels</p>

  <p class="body"><a class="calibre8" id="pgfId-1083597"></a>The data set consists of 50,000 training instances and 10,000 testing instances. Each instance is a 32 × 32 RGB image. There are 10 different classes of objects in this data set.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083604"></a>Let’s first load the data by executing the following line:</p>
  <pre class="programlisting">import tensorflow_datasets as tfds
data = tfds.load('cifar10')</pre>

  <p class="body"><a class="calibre8" id="pgfId-1083607"></a><span class="fm-code-in-text">print(data)</span> will yield</p>
  <pre class="programlisting">{'test': &lt;PrefetchDataset 
<span class="fm-code-continuation-arrow">➥</span> shapes: {id: (), image: (32, 32, 3), label: ()}, 
<span class="fm-code-continuation-arrow">➥</span> types: {id: tf.string, image: tf.uint8, label: tf.int64}&gt;, 'train': &lt;PrefetchDataset 
<span class="fm-code-continuation-arrow">➥</span> shapes: {id: (), image: (32, 32, 3), label: ()}, 
<span class="fm-code-continuation-arrow">➥</span> types: {id: tf.string, image: tf.uint8, label: tf.int64}&gt;}</pre>

  <p class="body"><a class="calibre8" id="pgfId-1083613"></a>If you explore the data a bit, you will realize that</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1083614"></a>Images are provided with the data type as unsigned eight-bit integers.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1083615"></a>Labels are provided as integer labels (i.e., not one-hot encoded).</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1083616"></a>Therefore, we will write a very simple function to convert the images to data type <span class="fm-code-in-text">float32</span> (to make the data type consistent with the model parameters) and labels to one-hot encoded vectors:</p>
  <pre class="programlisting">import tensorflow as tf
 
def format_data(x, depth):
    return (tf.cast(x["image"], 'float32'), tf.one_hot(x["label"], depth=depth))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1083621"></a>Finally, we will create a batched data set by applying this function to all the training data:</p>
  <pre class="programlisting">tr_data = data["train"].map(lambda x: format_data(x, depth=10)).batch(32)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1083623"></a>We can again look at the data with</p>
  <pre class="programlisting">for d in tr_data.take(1):
    print(d)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1083626"></a>which will produce</p>
  <pre class="programlisting">(
    &lt;tf.Tensor: shape=(32, 32, 32, 3), dtype=float32, numpy=
    array(
        [[[[143.,  96.,  70.],
           [141.,  96.,  72.],
           [135.,  93.,  72.],
           ...,     
           [ 52.,  34.,  31.],
           [ 91.,  74.,  59.],
           [126., 110.,  88.]]]], 
        dtype=float32)
    &gt;, 
    &lt;tf.Tensor: shape=(32, 10), dtype=float32, numpy=
    array(
        [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
         ... 
         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]], 
        dtype=float32)
    &gt;
)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1083655"></a>Now our data is ready to be fed to a<a class="calibre8" id="marker-1083652"></a> model.</p>

  <h3 class="fm-head1" id="sigil_toc_id_52"><a id="pgfId-1083656"></a>4.2.2 Implementing the network</h3>

  <p class="body"><a class="calibre8" id="pgfId-1083659"></a>To<a class="calibre8" id="marker-1083657"></a> classify these images, we will employ a CNN. CNNs have gained a stellar reputation for solving computer vision tasks and are a popular choice for image-related tasks for two main reasons:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1083660"></a>CNNs process the images while preserving their spatial information (i.e., while keeping the height and width dimensions as is), while a fully connected layer will need to unwrap the height and width dimensions to a single dimension, losing precious locality information.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1083661"></a>Unlike a fully connected layer where every input is connected to every output, the convolution operation shifts a smaller kernel over the entire image, demanding only a handful of parameters in a layer, making CNNs very parameter efficient.</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1083662"></a>A CNN consists of a set of interleaved convolution and pooling layers followed by several fully connected layers. This means there are three main types of layers in a CNN:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1083663"></a>Convolution layers</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1083664"></a>Pooling layers</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1083665"></a>Fully connected layers</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1083666"></a>A convolution layer consists of several filters (i.e., convolution kernels) that are convolved over the image to produce a <i class="fm-italics">feature map</i><a class="calibre8" id="marker-1083667"></a>. The feature map is a representation of how strongly a given filter is present in the image. For example, if the filter represents a vertical edge, the feature map represents where (and how strongly) in the image vertical edges are present. As another example, think of a neural network that is trained to identify faces. A filter might represent the shape of an eye and activate the corresponding area of the output highly when an eye is present in a given image (figure 4.6). We will discuss the convolution operation in more detail later in the chapter.</p>

  <p class="fm-figure"><img alt="04-06" class="calibre10" src="../../OEBPS/Images/04-06.png" width="872" height="456"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1096300"></a>Figure 4.6 The result of a convolution operation at a very abstract level. If we have an image of a human face and a convolution kernel that represents the shape/color of an eye, then the convolution result can be roughly thought of as a heatmap of where that feature (i.e., the eyes) are present in the image.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083674"></a>Another important characteristic of the convolution layers is that the deeper you go in the network (i.e., further away from the input), the more high-level features the layers learn. Going back to our face recognition example, the lower layers might learn various edges present; the next layer, the shape of an eye, ear, and a nose; the next layer, how two eyes are positioned, the alignment of the nose and mouth; and so on (figure 4.7).</p>

  <p class="fm-figure"><img alt="04-07" class="calibre10" src="../../OEBPS/Images/04-07.png" width="575" height="1181"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1096334"></a>Figure 4.7 Features learned by a convolutional neural network. The lower layers (closest to the input) are learning edges/lines, whereas the upper layers (furthest from input) are learning higher-level features. (Source: <span class="fm-hyperlink"><a class="url" href="http://mng.bz/8MPg">http://mng.bz/8MPg</a></span>)</p>

  <p class="body"><a class="calibre8" id="pgfId-1083682"></a>Next, the pooling layer takes in feature maps generated by a convolution layer and reduces their height and width dimensions. Why is it useful to reduce the height and width of the feature maps? It helps the model be translation invariant during the machine learning task. For instance, if the task is image classification, even if the objects appear several pixels offset from what was seen during training, the network is still able to identify the object.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083683"></a>Finally, to get the final probability distribution, you have several fully connected layers. But you might have suspected an issue we face here. A convolution/pooling layer produces a three-dimensional output (i.e., height, width, and channel dimensions). But a fully connected layer accepts a one-dimensional input. How do we connect the three-dimensional output of a convolution/pooling layer to a one-dimensional fully connected layer? There’s a simple answer to this problem. You squash all three dimensions into a single dimension. In other words, it is analogous to unwrapping a two-dimensional RGB image to a one-dimensional vector. This provides the fully connected layer with a one-dimensional input. Finally, a softmax activation is applied to the outputs of the final fully connected layer (i.e., scores of the network) to obtain a valid probability distribution. Figure 4.8 depicts a simple CNN.</p>

  <p class="fm-figure"><img alt="04-08" class="calibre10" src="../../OEBPS/Images/04-08.png" width="1072" height="767"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1096377"></a>Figure 4.8 A simple CNN. First, we have an image with height, width, and channel dimensions, followed by a convolution and pooling layer. Finally, the last convolution/pooling layer output is flattened and fed to a set of fully connected layers.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083690"></a>With a good understanding of what a CNN comprises, we will create the following CNN using the Keras Sequential API. However, if you run this code, you will get an error. We will investigate and fix this error in the coming sections (see the next listing).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1083692"></a>Listing 4.2 Defining a CNN with the Keras Sequential API</p>
  <pre class="programlisting">from tensorflow.keras import layers, models
import tensorflow.keras.backend as K
 
K.clear_session()                                                          <span class="fm-combinumeral">❶</span>
cnn = models.Sequential(
    [layers.Conv2D(
        filters=16, kernel_size= (9,9), strides=(2,2), activation='relu',  <span class="fm-combinumeral">❷</span>
           padding=’valid’, input_shape=(32,32,3)
        ),                                                                 <span class="fm-combinumeral">❷</span>
     layers.Conv2D(
         filters=32, kernel_size= (7,7), activation='relu', padding=’valid’
     ), 
     layers.Conv2D(
         filters=64, kernel_size= (7,7), activation='relu', padding=’valid’
     ), 
     layers.Flatten(),                                                     <span class="fm-combinumeral">❸</span>
     layers.Dense(64, activation='relu'),                                  <span class="fm-combinumeral">❹</span>
     layers.Dense(10, activation='softmax')]                               <span class="fm-combinumeral">❺</span>
)</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1094894"></a><span class="fm-combinumeral">❶</span> Clearing any existing Keras states (e.g., models) to start fresh</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1094915"></a><span class="fm-combinumeral">❷</span> Defining a convolution layer; it takes parameters like filters, kernel_size, strides, activation, and padding.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1094935"></a><span class="fm-combinumeral">❸</span> Before feeding the data to a fully connected layer, we need to flatten the output of the last convolution layer.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1094952"></a><span class="fm-combinumeral">❹</span> Creating an intermediate fully connected layer</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1094969"></a><span class="fm-combinumeral">❺</span> Final prediction layer</p>

  <p class="body"><a class="calibre8" id="pgfId-1083718"></a>You can see that the network consists of three convolution layers and two fully connected layers. Keras provides all the layers you need to implement a CNN. As you can see, it can be done in a single line of code for our image classification network. Let’s explore what is happening in this model in more detail. The first layer is specified as follows:</p>
  <pre class="programlisting">layers.Conv2D(filters=16,kernel_size=(9,9), strides=(2,2), activation='relu', input_shape=(32,32,3))</pre>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1083720"></a>Hyperparameters of convolutional neural networks</p>

    <p class="fm-sidebar-text"><a id="pgfId-1083721"></a>In the CNN network from listing 4.2, <span class="fm-code-in-text1">filters</span>, <span class="fm-code-in-text1">kernel_size</span>, and <span class="fm-code-in-text1">strides</span> of the <span class="fm-code-in-text1">Conv2D</span> layers<a id="marker-1085506"></a>, the number of hidden units in the <span class="fm-code-in-text1">Dense</span> layers<a id="marker-1085508"></a> (except the output layer) and the <span class="fm-code-in-text1">activation</span> function<a id="marker-1085509"></a> are known as the hyperparameters of the model. Ideally, these hyperparameters need to be selected using a hyperparameter optimization algorithm, which would run hundreds (if not thousands) of models with different hyperparameter values and choose the one that maximizes a predefined metric (e.g., model accuracy). However, here we have chosen the values for these hyperparameters empirically and will not be using hyperparameter optimization.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1083728"></a>First, the <span class="fm-code-in-text">Conv2D</span> layer<a class="calibre8" id="marker-1083727"></a> is the Keras implementation of the 2D convolution operation. As you’ll remember from chapter 1, we achieved this using the <span class="fm-code-in-text">tf.nn.convolution</span> operation<a class="calibre8" id="marker-1083729"></a>. The <span class="fm-code-in-text">Conv2D</span> layer<a class="calibre8" id="marker-1083730"></a> executes the same functionality under the hood. However, it hides some of the complexities met when using the <span class="fm-code-in-text">tf.nn.convolution</span> operation<a class="calibre8" id="marker-1083731"></a> directly (e.g., defining the layer parameters explicitly) There are several important arguments you need to provide to this layer:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1083732"></a><span class="fm-code-in-text">filters</span>—The number of output channels that will be present in the output.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1083733"></a><span class="fm-code-in-text">kernel_size</span>—The convolution window size on the height and width dimensions, in that order.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1083734"></a><span class="fm-code-in-text">strides</span>—Represents how many pixels are skipped on height and with dimensions (in that order) every time the convolution window shifts on the input. Having a higher value here helps to reduce the size of the convolution output quickly as you go deeper.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1083735"></a><span class="fm-code-in-text">activation</span>—The nonlinear activation of the convolution layer.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1083736"></a><span class="fm-code-in-text">padding</span>—Type of padding used for the border while performing the convolution operation. Padding borders gives more control over the size of the output.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1083737"></a><span class="fm-code-in-text">input_shape</span>—A three-dimensional tuple representing the input size on (height, width, channels) dimensions, in that order. Remember that Keras adds an unspecified batch dimension automatically when specifying the shape of the data using this argument.</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1083738"></a>Let’s now go over the convolution function and its parameters in more detail. We already know that the convolution operation shifts a convolution window (i.e., a kernel) over the image, while taking the sum of an element-wise product between the kernel and the portion of the image that overlaps the kernel at a given time (figure 4.9). Mathematically, the convolution operation can be stated as follows</p>

  <p class="fm-equation"><img alt="04_08a" class="calibre10" src="../../OEBPS/Images/04_08a.png" width="327" height="77"/><br class="calibre2"/>
  <a id="pgfId-1085779"></a></p>

  <p class="body"><a class="calibre8" id="pgfId-1083743"></a>where <i class="fm-timesitalic">x</i> is a <i class="fm-timesitalic">n</i> × <i class="fm-timesitalic">n</i> input matrix, <i class="fm-timesitalic">f</i> is a <i class="fm-timesitalic">m</i> × <i class="fm-timesitalic">m</i> filter, and <i class="fm-timesitalic">y</i> is the output.</p>

  <p class="fm-figure"><img alt="04-09" class="calibre10" src="../../OEBPS/Images/04-09.png" width="700" height="450"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1096411"></a>Figure 4.9 The computations that happen in the convolution operation while shifting the window</p>

  <p class="body"><a class="calibre8" id="pgfId-1083750"></a>Apart from the computations that take place during the convolution operation, there are four important hyperparameters that affect the size and values produced when using the <span class="fm-code-in-text">Conv2D</span> layer<a class="calibre8" id="marker-1083751"></a> in Keras:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1083752"></a>Number of filters</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1083753"></a>Kernel height and width</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1083754"></a>Kernel stride (height and width)</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1083755"></a>Type of padding</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1083756"></a>The first aspect we will discuss is the number of filters in the layer. Typically, a single convolution layer has multiple filters. For example, think of a neural network that is trained to identify faces. One of the layers in the network might learn to identify the shape of an eye, shape of a nose, and so on. Each of these features might be learned by a single filter in the layer.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083757"></a>The convolution layer takes an image, which is a three-dimensional tensor of some height, width, and channels. For example, if the image is an RGB image, there will be three channels. If the image is a grayscale image, the number of channels will be one. Then, convolving this tensor with <span class="fm-code-in-text">n</span> number of filters will result in a three-dimensional output of some height, width, and <span class="fm-code-in-text">n</span> channels. This is shown in figure 4.10. When used in a CNN, the filters are the parameters of a convolution layer. These filters are initialized randomly, and over time they evolve to become meaningful features that help solve the task at hand.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083758"></a>As we have said before, deep neural networks process data in batches. CNNs are no exception. You can see that we have set the <span class="fm-code-in-text">input_shape</span> parameter<a class="calibre8" id="marker-1088961"></a> to (32, 32, 3), where an unspecified batch dimension is automatically added, making it (<span class="fm-code-in-text">None</span>, 32, 32, 3). The unspecified dimension is denoted by <span class="fm-code-in-text">None</span>, and it means that the model can take any arbitrary number of items on that dimension. This means that a batch of data can have 3, 4, 100, or any number of images (as the computer memory permits) at run time while feeding data to the model. Therefore, the input/output of a <span class="fm-code-in-text">Conv2D</span> layer<a class="calibre8" id="marker-1088962"></a> is, in fact, a four-dimensional tensor with a batch, height, width, and channel dimension. Then the filters will be another four-dimensional tensor with a kernel height, width, incoming channel, and outgoing channel dimension. Table 4.1 summarizes this information.</p>

  <p class="fm-table-caption"><a id="pgfId-1088916"></a>Table 4.1 The dimensionality of the input, filters, and the output of a <span class="fm-code-in-figurecaption">Conv2D</span> layer<a id="marker-1089004"></a></p>

  <table border="1" class="contenttable" width="100%">
    <colgroup class="calibre12">
      <col class="calibre13" span="1" width="20%"/>
      <col class="calibre13" span="1" width="30%"/>
      <col class="calibre13" span="1" width="50%"/>
    </colgroup>

    <tr class="calibre14">
      <th class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1088922"></a> </p>
      </th>

      <th class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1088924"></a><b class="fm-bold">Dimensionality</b></p>
      </th>

      <th class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1088926"></a><b class="fm-bold">Example</b></p>
      </th>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1088928"></a>Input</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1088930"></a>[batch size, height, width, in channels]</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1088932"></a>[32, 64, 64, 3] (i.e., a batch of 32, 64 × 64 RGB images)</p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1088934"></a>Convolution filters</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1088936"></a>[height, width, in channels, out channels]</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1088938"></a>[5, 5, 3, 16] (i.e., 16 convolution filters of size 5 × 5 with 3 incoming channels)</p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1088940"></a>Output</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1088942"></a>[batch size, height, width, out channels]</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1088944"></a>[32, 64, 64, 16] (i.e., a batch of 32, 64 × 64 × 16 tensors)</p>
      </td>
    </tr>
  </table>

  <p class="body"><a class="calibre8" id="pgfId-1083790"></a>Figure 4.10 depicts how the inputs and outputs look in a convolution layer.</p>

  <p class="fm-figure"><img alt="04-10" class="calibre10" src="../../OEBPS/Images/04-10.png" width="1092" height="472"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1096445"></a>Figure 4.10 A computation of a convolution layer with multiple filters (randomly initialized). We left the batch dimension of the tensor representation to avoid clutter.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083797"></a>Next, kernel height and width are the size of the filter on height and width dimensions. Figure 4.11 depicts how different kernel sizes lead to different outputs. Typically, when implementing CNNs, we keep kernel height and width equal. With that, we will refer to both the height and width dimensions of the kernel generally as the <i class="fm-italics">kernel size</i><a class="calibre8" id="marker-1083798"></a>. We can compute the output size as a function of the kernel and input size as follows:</p>

  <p class="fm-equation"><a id="pgfId-1083799"></a><i class="fm-italics">size</i>(<i class="fm-italics">y</i>) = <i class="fm-italics">size</i>(<i class="fm-italics">x</i>) - <i class="fm-italics">size</i>(<i class="fm-italics">f</i>) + 1</p>

  <p class="body"><a class="calibre8" id="pgfId-1083800"></a>For example, if the image is a 7 × 7 matrix and the filter is a 3 × 3 matrix, then the output will be a (7 - 3 + 1, 7 - 3 + 1) = 5 × 5 matrix. Or, if the image is a 7 × 7 matrix and the filter is a 5 × 5 matrix, then the output will be a 3 × 3 matrix.</p>

  <p class="fm-figure"><img alt="04-11" class="calibre10" src="../../OEBPS/Images/04-11.png" width="581" height="436"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1096479"></a>Figure 4.11 Convolution operation with a kernel size of 2 and kernel size of 3. Increasing the kernel size leads to a reduced output size.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083801"></a>From a modeling perspective, increasing the kernel size (i.e., filter size) translates to an increased number of parameters. Typically, you should try to reduce the number of parameters in your network and target smaller-sized kernels. Having small kernel sizes encourages the model to learn more robust features with a small number of parameters, leading to better generalization of the model.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083808"></a>The next important parameter is the stride. Just like the kernel size, the stride has two components: height and width. Intuitively, the stride defines how many pixels/ values you skip while shifting the convolution operation. Figure 4.12 illustrates the difference between having stride = 1 (i.e., no stride versus stride = 2). As before, we can specify the output size as a function of the input size, kernel size, and stride:</p>

  <p class="fm-equation"><img alt="04_11a" class="calibre10" src="../../OEBPS/Images/04_11a.png" width="336" height="66"/><br class="calibre2"/>
  <a id="pgfId-1085838"></a></p>

  <p class="body"><a class="calibre8" id="pgfId-1096507"></a> </p>

  <p class="fm-figure"><img alt="04-12" class="calibre10" src="../../OEBPS/Images/04-12.png" width="1111" height="556"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1096513"></a>Figure 4.12 Convolution operation with stride = 1 (i.e., no stride) versus stride = 2. An increased stride leads to a smaller output.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083813"></a>From a modeling perspective, striding is beneficial, as it helps you to control the amount of reduction you need in the output. You might have noticed that, even without striding, you still get an automatic dimensionality reduction during convolution. However, when using striding, you can control the reduction you want to gain without affecting the kernel size.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083820"></a>Finally, padding decides what happens near the borders of the image. As you have already seen, when you convolve an image, you don’t get a same-sized output as the input. For example, if you have a 4 × 4 matrix and a 2 × 2 kernel, you get a 3 × 3 output (i.e., following the equation <i class="fm-italics">size</i>(<i class="fm-italics">y</i>) = <i class="fm-italics">size</i>(<i class="fm-italics">x</i>) - <i class="fm-italics">size</i>(<i class="fm-italics">f</i> ) + 1 we saw earlier, where <i class="fm-italics">x</i> is the input size and <i class="fm-italics">f</i> is the filter size). This automatic dimensionality reduction creates an issue when creating deep models. Specifically, it limits the number of layers you can have, as at some point the input will become a 1 × 1 pixel due to this automatic dimension reduction. Consequentially, this will create a very narrow bottleneck in passing information to the fully connected layers that follow, causing massive information loss.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083821"></a>You can use padding to alleviate this issue. With padding, you create an imaginary border of zeros around the image, such that you get the same-sized output as the input. More specifically, you append a border that is a <i class="fm-italics">size</i>(<i class="fm-italics">f</i> ) - 1-thick border of zeros in order to get an output of same size as the input. For example, if you have an input of size 4 × 4 and a kernel of size 2 × 2, then you would apply a border of size 2 - 1 = 1 vertically and horizontally. This means that the kernel is essentially processing a 5 × 5 input (i.e., (4 + 1) × (4 + 1)-sized input), resulting in a 4 × 4-sized output. This is called <i class="fm-italics">same padding</i><a class="calibre8" id="marker-1087303"></a>. Note that it does not always have to be zeros that you are padding. Though currently not supported in Keras, there are different padding strategies (some examples are available here: <span class="fm-hyperlink"><a class="url" href="https://www.tensorflow.org/api_docs/python/tf/pad">https://www.tensorflow.org/api_docs/python/tf/pad</a></span>), such as padding with</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1083823"></a>A constant value</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1083824"></a>A reflection of the input</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1083825"></a>The nearest value</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1083827"></a>If you don’t apply padding, that is called <i class="fm-italics">valid padding</i><a class="calibre8" id="marker-1083826"></a>. Not applying padding leads to the standard convolution operation we discussed earlier. The differences in padding are shown in figure 4.13.</p>

  <p class="fm-figure"><img alt="04-13" class="calibre10" src="../../OEBPS/Images/04-13.png" width="486" height="467"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1096650"></a>Figure 4.13 Valid versus same padding. Valid padding leads to a reduced output size, whereas same padding results in an output with equal dimensions to the input.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083834"></a>With that, we conclude our discussion about various hyperparameters of the <span class="fm-code-in-text">Conv2D</span> layer<a class="calibre8" id="marker-1083835"></a>. Now let’s circle back to the network we implemented. Unfortunately for you, if you try to run the code we discussed, you will be presented with a somewhat cryptic error like this:</p>
  <pre class="programlisting">---------------------------------------------------------------------------
...
 
InvalidArgumentError: Negative dimension size caused by subtracting 7 from 6 for 'conv2d_2/Conv2D' (op: 'Conv2D') with input shapes: [?,6,6,32], [7,7,32,64].</pre>

  <p class="body"><a class="calibre8" id="pgfId-1083840"></a>What have we done wrong here? It seems TensorFlow is complaining about a negative dimension size while trying to compute the output of a convolution layer. Since we have learned all about how to compute the size of the output under various circumstances (e.g., with stride, with padding, etc.), we will compute the final output of the convolution layers. We have the following layers:</p>
  <pre class="programlisting">layers.Conv2D(16,(9,9), strides=(2,2), activation='relu', padding=’valid’, input_shape=(32,32,3))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1083842"></a>We are starting with an input of size 32 × 32 × 3. Then, after the convolution operation, which has 16 filters, a kernel size of 9, and stride 2, we get an output of size (height and width)</p>
  <pre class="programlisting">(<span class="cambria">⌊</span>(32 - 9) / 2<span class="cambria">⌋ </span>+ 1 = 12</pre>

  <p class="body"><a class="calibre8" id="pgfId-1083844"></a>Here, we focus only on the height and width dimensions. The next layer has 32 filters, a kernel size of 7, and no stride:</p>
  <pre class="programlisting">     layers.Conv2D(32, (7,7), activation='relu', padding=’valid’)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1083846"></a>This layer produces an output of size</p>
  <pre class="programlisting">12 - 7 + 1 = 6</pre>

  <p class="body"><a class="calibre8" id="pgfId-1083848"></a>The final convolution layer has 64 filters, a kernel size of 7, and no stride</p>
  <pre class="programlisting">     layers.Conv2D(64, (7,7), activation='relu', padding=’valid’), </pre>

  <p class="body"><a class="calibre8" id="pgfId-1083850"></a>which will produce an output of size</p>

  <p class="fm-equation"><a id="pgfId-1083851"></a>6 - 7 + 1= 0</p>

  <p class="body"><a class="calibre8" id="pgfId-1083852"></a>We figured it out! With our chosen configuration, our CNN is producing an invalid zero-sized output. The term <i class="fm-italics">negative dimension</i><a class="calibre8" id="marker-1083853"></a> in the error refers to an output with invalid dimensions (i.e., less than one) being produced. The output always needs to be greater than or equal to 1.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083854"></a>Let’s correct this network by making sure the outputs will never have negative dimensions. Furthermore, we will introduce several interleaved max-pooling layers to the CNN, which helps the network to learn translation-invariant features (see the next listing).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1083856"></a>Listing 4.3 The corrected CNN model that has positive dimensions</p>
  <pre class="programlisting">from tensorflow.keras import layers, models
import tensorflow.keras.backend as K
 
K.clear_session()
 
cnn = models.Sequential([
     layers.Conv2D(                                                      <span class="fm-combinumeral">❶</span>
         filters=16,kernel_size=(3,3), strides=(2,2), activation='relu', <span class="fm-combinumeral">❶</span>
         padding='same', input_shape=(32,32,3)),                         <span class="fm-combinumeral">❶</span>
     layers.MaxPool2D(pool_size=(2,2), strides=(2,2), padding='same'),   <span class="fm-combinumeral">❷</span>
     layers.Conv2D(32, (3,3), activation='relu', padding='same'),        <span class="fm-combinumeral">❸</span>
     layers.MaxPool2D(pool_size=(2,2), strides=(2,2), padding='same'),   <span class="fm-combinumeral">❹</span>
     layers.Flatten(),                                                   <span class="fm-combinumeral">❺</span>
     layers.Dense(64, activation='relu'),                                <span class="fm-combinumeral">❻</span>
     layers.Dense(32, activation='relu'),                                <span class="fm-combinumeral">❻</span>
     layers.Dense(10, activation='softmax')]                             <span class="fm-combinumeral">❼</span>
)</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1094372"></a><span class="fm-combinumeral">❶</span> The first convolution layer. The output size reduces from 32 to 16.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1094396"></a><span class="fm-combinumeral">❷</span> The first max-pooling layer. The output size reduces from 16 to 8.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1094413"></a><span class="fm-combinumeral">❸</span> The second convolution layer. The output size stays the same as there is no stride.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1094430"></a><span class="fm-combinumeral">❹</span> The second max-pooling layer. The output size reduces from 8 to 4.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1094447"></a><span class="fm-combinumeral">❺</span> Squashing the height, width, and channel dimensions to a single dimension</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1094471"></a><span class="fm-combinumeral">❻</span> The two intermediate Dense layers with ReLU activation</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1094488"></a><span class="fm-combinumeral">❼</span> The final output layer with softmax activation</p>

  <p class="body"><a class="calibre8" id="pgfId-1083882"></a>Max-pooling is provided by the <span class="fm-code-in-text">tensorflow.keras.layers.MaxPool2D</span> layer<a class="calibre8" id="marker-1094463"></a><a id="marker-1083881"></a>. The hyperparameters of this layer are very similar to <span class="fm-code-in-text">tensorflow.keras.layers.Conv2D</span>:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1083885"></a><span class="fm-code-in-text">pool_size</span>—This is analogous to the kernel size parameter of the <span class="fm-code-in-text">Conv2D</span> layer<a class="calibre8" id="marker-1083884"></a>. It is a tuple representing (window height, window width), in that order.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1083887"></a><span class="fm-code-in-text">Strides</span>—This is analogous to the strides parameter of the <span class="fm-code-in-text">Conv2D</span> layer<a class="calibre8" id="marker-1083886"></a>. It is a tuple representing (height stride, width stride), in that order.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1083888"></a><span class="fm-code-in-text">Padding</span>—Padding can be same or valid and has the same effect as it has in the <span class="fm-code-in-text">Conv2D</span> layer<a class="calibre8" id="marker-1083889"></a>.</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1083890"></a>Let’s analyze the changes we made to our CNN:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1083893"></a>We used <span class="fm-code-in-text">padding='same'</span> for all the <span class="fm-code-in-text">Conv2D</span><a class="calibre8" id="marker-1083891"></a> and <span class="fm-code-in-text">MaxPool2D</span> layers<a class="calibre8" id="marker-1083892"></a>, meaning that there won’t be any automatic reduction of the output size. This eliminates the risk of mistakenly going into negative dimensions.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1083894"></a>We used stride parameters to control the reduction of the output size as we go deeper into the model.</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1083895"></a>You can follow the output sizes in listing 4.1 and make sure that the output will never be less than or equal to zero for the input images we have.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083899"></a>After the <span class="fm-code-in-text">Conv2D</span><a class="calibre8" id="marker-1083896"></a> and <span class="fm-code-in-text">MaxPool2D</span> layers<a class="calibre8" id="marker-1083897"></a>, we have to have at least one <span class="fm-code-in-text">Dense</span> layer<a class="calibre8" id="marker-1083898"></a>, as we are solving an image classification task. To get the final prediction probabilities (i.e., the probabilities of a given input belonging to the output classes), a <span class="fm-code-in-text">Dense</span> layer<a class="calibre8" id="marker-1083900"></a> is essential. But before having a <span class="fm-code-in-text">Dense</span> layer<a class="calibre8" id="marker-1083901"></a>, we need to flatten our four-dimensional output (i.e., [batch, height, width, channel] shaped) of the <span class="fm-code-in-text">Conv2D</span><a class="calibre8" id="marker-1083902"></a> or <span class="fm-code-in-text">MaxPool2D</span> layers<a class="calibre8" id="marker-1083903"></a> to a two-dimensional input (i.e., [batch, features] shaped) to the <span class="fm-code-in-text">Dense</span> layer<a class="calibre8" id="marker-1083904"></a>. That is, except for the batch dimension, everything else gets squashed to a single dimension. For this, we use the <span class="fm-code-in-text">tensorflow.keras.layers.Flatten</span> layer<a class="calibre8" id="marker-1083905"></a> provided by Keras. For example, if the output of our last <span class="fm-code-in-text">Conv2D</span> layer<a class="calibre8" id="marker-1083906"></a> was [<span class="fm-code-in-text">None</span>, 4, 4, 64], then the <span class="fm-code-in-text">Flatten</span> layer<a class="calibre8" id="marker-1083907"></a> will flatten this output to a [<span class="fm-code-in-text">None</span>, 1024]-sized tensor. Finally, we add three <span class="fm-code-in-text">Dense</span> layers<a class="calibre8" id="marker-1083908"></a>, where the first two dense layers have 64 and 32 output nodes and an activation of type ReLU. The final <span class="fm-code-in-text">Dense</span> layer<a class="calibre8" id="marker-1083909"></a> will have 10 nodes (1 for each class) and a softmax activation.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1083910"></a>Performance bottleneck of CNNs</p>

    <p class="fm-sidebar-text"><a id="pgfId-1083912"></a>Typically, in a CNN the very first <span class="fm-code-in-text1">Dense</span> layer<a id="marker-1085546"></a> after the convolution/pooling layers is considered a <i class="fm-italics">performance bottleneck</i><a id="marker-1085548"></a>. This is because this layer will usually contain a large proportion of the parameters of the network. Assume you have a CNN where the last pooling layer produces an 8 × 8 × 256 output followed by a <span class="fm-code-in-text1">Dense</span> layer<a id="marker-1085549"></a> with 1,024 nodes. This <span class="fm-code-in-text1">Dense</span> layer<a id="marker-1085550"></a> would contain 16,778,240 (more than 16 million) parameters. If you don’t pay attention to the first <span class="fm-code-in-text1">Dense</span> layer<a id="marker-1085551"></a> of the CNN, you can easily run into out-of-memory errors while running the model.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1083918"></a>It’s time to test our first CNN on the data. But before that we have to compile the model with appropriate parameters. Here, we will monitor the training accuracy of the mode:</p>
  <pre class="programlisting">cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])</pre>

  <p class="body"><a class="calibre8" id="pgfId-1083920"></a>Finally, you can use the training data we created earlier and train the model on the data by calling</p>
  <pre class="programlisting">history = cnn.fit(tr_data,epochs=25)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1083922"></a>You should get the following output:</p>
  <pre class="programlisting">Epoch 1/25
1563/1563 [==============================] - 23s 15ms/step - loss: 2.0566 - acc: 0.3195
Epoch 2/25
1563/1563 [==============================] - 13s 8ms/step - loss: 1.4664 - acc: 0.4699
...
Epoch 24/25
1563/1563 [==============================] - 13s 8ms/step - loss: 0.8070 - acc: 0.7174
Epoch 25/25
1563/1563 [==============================] - 13s 8ms/step - loss: 0.7874 - acc: 0.7227</pre>

  <p class="body"><a class="calibre8" id="pgfId-1083932"></a>It seems we are getting good at training accuracies (denoted by <span class="fm-code-in-text">acc</span>) and creating a steady reduction of the training loss (denoted by <span class="fm-code-in-text">loss</span>) for the task of identifying vehicles (around 72.2% accuracy). But we can go for far better accuracies by employing various techniques, as you will see in later chapters. This is very promising news for the team, as this means they can continue working on their full solution.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083933"></a>In this section, we looked at CNNs. CNNs work extremely well, especially in computer vision problems. In this instance, we looked at using a CNN to classify images to various classes (e.g., animals, vehicles, etc.) as a feasibility study for a model’s ability to detect vehicles. We looked at the technical aspects of the CNN in detail, while scrutinizing various operations like convolution and pooling, as well as the impact of the parameters associated with these operations (e.g., window size, stride, padding). We saw that if we do not pay attention to how the output changes while using these parameters, it can lead to errors in our code. Next, we went on to fix the error and train the model on the data set. Finally, we saw that the model showed promising results, quickly reaching for a training accuracy above 70%. Next, we will discuss RNNs, which are heavily invested in solving time-series problems.</p>

  <p class="fm-head2"><a id="pgfId-1083934"></a>Exercise 2</p>

  <p class="body"><a class="calibre8" id="pgfId-1083935"></a>Consider the following network:</p>
  <pre class="programlisting">from tensorflow.keras import layers, models
 
models.Sequential([
layers.Conv2D(                                                     
         filters=16, kernel_size=(5,5), padding='valid', input_shape=(64,64,3)
),                        
layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding='same'),  
layers.Conv2D(32, (3,3), activation='relu', padding='same'),       
layers.MaxPool2D(pool_size=(2,2), strides=(2,2), padding='same'),  
layers.Conv2D(32, (3,3), strides=(2,2), activation='relu', padding='same')
])       </pre>

  <p class="body"><a class="calibre8" id="pgfId-1083951"></a>What is the final output<a class="calibre8" id="marker-1083947"></a> size<a class="calibre8" id="marker-1083949"></a><a class="calibre8" id="marker-1083950"></a> (ignoring the batch dimension)?</p>

  <h2 class="fm-head" id="sigil_toc_id_53"><a id="pgfId-1083952"></a>4.3 One step at a time: Recurrent neural networks (RNNs)</h2>

  <p class="body"><a class="calibre8" id="pgfId-1083955"></a>You<a class="calibre8" id="marker-1083953"></a><a class="calibre8" id="marker-1083954"></a> are working for a machine learning consultant for the National Bureau of Meteorology. They have data for CO2 concentration over the last three decades. You have been tasked with developing a machine learning model that predicts CO2 concentration for the next five years. You are planning to implement a simple RNN that takes a sequence of CO2 concentrations (in this case, the values from the last 12 months) and predicts the next in the sequence.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083956"></a>It can be clearly seen that what we have in front of us is a time series problem. This is quite different from the tasks we have been solving so far. In previous tasks, one input did not depend on the previous inputs. In other words, you considered each input to be <i class="fm-italics">i.i.d</i> (independent and identically distributed<a class="calibre8" id="marker-1083957"></a>) inputs. However, in this problem, that is not the case. The CO2 concentration today will depend on what the CO2 concentration was over the last several months.</p>

  <p class="body"><a class="calibre8" id="pgfId-1083958"></a>Typical feed-forward networks (i.e., fully connected networks, CNNs) cannot learn from time series data without special adaptations. However, there is a special type of neural network that is designed to learn from time series data. These networks are generally known as RNNs. RNNs not only use the current input to make a prediction, but also use the <i class="fm-italics">memory</i> of the network from past time steps, at a given time step. Figure 4.14 depicts how a feed-forward network and an RNN differ in predicting CO2 concentration over the months. As you can see, if you use a feed-forward network, it has to predict the CO2 level for the next month based <i class="fm-italics">only</i> on the previous month, whereas an RNN looks at all the previous months.</p>

  <p class="fm-figure"><img alt="04-14" class="calibre10" src="../../OEBPS/Images/04-14.png" width="925" height="1367"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1096684"></a>Figure 4.14 The operational difference between a feed-forward network and an RNN in terms of a CO2 concentration-level prediction task</p>

  <h3 class="fm-head1" id="sigil_toc_id_54"><a id="pgfId-1083965"></a>4.3.1 Understanding the data</h3>

  <p class="body"><a class="calibre8" id="pgfId-1083969"></a>The<a class="calibre8" id="marker-1087325"></a> data set is very simple (downloaded from <span class="fm-hyperlink"><a class="url" href="https://datahub.io/core/co2-ppm/r/co2-mm-gl.csv">https://datahub.io/core/co2-ppm/r/co2-mm-gl.csv</a></span>). Each datapoint has a date (<span class="fm-code-in-text">YYYY-MM-DD</span> format) and a floating-point value representing the CO2 concentration in CSV format. The data is provided to us as a CSV file. Let’s download the file as follows:</p>
  <pre class="programlisting">import requests
import os
 
def download_data():
    """ This function downloads the CO2 data from 
    https:/ /datahub.io/core/co2-ppm/r/co2-mm-gl.csv
    if the file doesn't already exist
    """
    save_dir = "data"
    save_path = os.path.join(save_dir, 'co2-mm-gl.csv')
 
    # Create directories if they are not there
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
 
    # Download the data and save
    if not os.path.exists(save_path):
        url = "https:/ /datahub.io/core/co2-ppm/r/co2-mm-gl.csv"
        r = requests.get(url)
        with open(save_path, 'wb') as f:
            f.write(r.content)
    else:
        print("co2-mm-gl.csv already exists. Not downloading.")
    return save_path
 
# Downloading the data
save_path = download_data()</pre>

  <p class="body"><a class="calibre8" id="pgfId-1083997"></a>We can easily load this data set using pandas:</p>
  <pre class="programlisting">import pandas as pd
data = pd.read_csv(save_path)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1084001"></a>Now we can see what the data looks like, using the <span class="fm-code-in-text">head()</span> operation<a class="calibre8" id="marker-1084000"></a>, which will provide the first few entries in the data frame:</p>
  <pre class="programlisting">data.head()</pre>

  <p class="body"><a class="calibre8" id="pgfId-1084003"></a>This will give something like figure 4.15.</p>

  <p class="fm-figure"><img alt="04-15" class="calibre10" src="../../OEBPS/Images/04-15.png" width="476" height="255"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1096718"></a>Figure 4.15 Sample data in the data set</p>

  <p class="body"><a class="calibre8" id="pgfId-1084010"></a>In this data set, the only two columns we are interested in are the <span class="fm-code-in-text">Date</span> column and the <span class="fm-code-in-text">Average</span> column. Out of these, the <span class="fm-code-in-text">Date</span> column is important for visualization purposes only. Let’s set the <span class="fm-code-in-text">Date</span> column as the index of the data frame. This way, when we plot data, the <i class="fm-italics">x</i>-axis will be automatically annotated with the corresponding date:</p>
  <pre class="programlisting">data = data.set_index('Date')</pre>

  <p class="body"><a class="calibre8" id="pgfId-1084012"></a>We can now visualize the data (figure 4.16) by creating a line plot with</p>
  <pre class="programlisting">data[["Average"]].plot(figsize=(12,6))</pre>

  <p class="fm-figure"><img alt="04-16" class="calibre10" src="../../OEBPS/Images/04-16.png" width="1133" height="581"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1096752"></a>Figure 4.16 CO2 concentration plotted over time</p>

  <p class="body"><a class="calibre8" id="pgfId-1084020"></a>The obvious features of the data are that it has an upward trend and short repetitive cycles. Let’s see what sort of improvements we can do to this data. The clear upward trend the data is showing poses a problem. This means that the data is not distributed in a consistent range. The range increases as we go further and further down the timeline. If you feed data as it is to the model, usually the model will underperform, because any new data the model has to predict is out of the range of the data it saw during training. But if you forget the absolute values and think about this data relative to the previous value, you will see that it moves between a very small range of values (appx -2.0 to +1.5). In fact, we can test this idea easily. We will create a new column called <span class="fm-code-in-text">Average Diff</span>, which will contain the relative difference between two consecutive time steps:</p>
  <pre class="programlisting">data["Average Diff"]=data["Average"] - data["Average"].shift(1).fillna(method='bfill')</pre>

  <p class="body"><a class="calibre8" id="pgfId-1084022"></a>If you do a <span class="fm-code-in-text">data.head()</span> at this stage, you will see something similar to table 4.2.</p>

  <p class="fm-table-caption"><a id="pgfId-1090710"></a>Table 4.2 Sample data in the data set after introducing the <span class="fm-code-in-figurecaption">Average</span> <span class="fm-code-in-figurecaption">diff</span> column</p>

  <table border="1" class="contenttable" width="100%">
    <colgroup class="calibre12">
      <col class="calibre13" span="1" width="20%"/>
      <col class="calibre13" span="1" width="20%"/>
      <col class="calibre13" span="1" width="20%"/>
      <col class="calibre13" span="1" width="20%"/>
      <col class="calibre13" span="1" width="20%"/>
    </colgroup>

    <tr class="calibre14">
      <th class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1090720"></a><b class="fm-bold">Date</b></p>
      </th>

      <th class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1090722"></a><b class="fm-bold">Decimal data</b></p>
      </th>

      <th class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1090724"></a><b class="fm-bold">Average</b></p>
      </th>

      <th class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1090726"></a><b class="fm-bold">Trend</b></p>
      </th>

      <th class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1090728"></a><b class="fm-bold">Average diff</b></p>
      </th>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1090730"></a>1980-01-01</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1090732"></a>1980.042</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1090734"></a>338.45</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1090736"></a>337.83</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1090738"></a>0.00</p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1090740"></a>1980-02-01</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1090742"></a>1980.125</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1090744"></a>339.15</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1090746"></a>338.10</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1090748"></a>0.70</p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1090750"></a>1980-031-01</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1090752"></a>1980.208</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1090754"></a>339.48</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1090756"></a>338.13</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1090758"></a>0.33</p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1090760"></a>1980-04-01</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1090762"></a>1980.292</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1090764"></a>339.87</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1090766"></a>338.25</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1090768"></a>0.39</p>
      </td>
    </tr>

    <tr class="calibre14">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1090770"></a>1980-05-01</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1090772"></a>1980.375</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1090774"></a>340.30</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1090776"></a>338.78</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1090778"></a>0.43</p>
      </td>
    </tr>
  </table>

  <p class="body"><a class="calibre8" id="pgfId-1084029"></a>Here, we are subtracting a version of the <span class="fm-code-in-text">Average</span> column, where values are shifted forward by one time step, from the original average column. Figure 4.17 depicts this operation visually.</p>

  <p class="fm-figure"><img alt="04-17" class="calibre10" src="../../OEBPS/Images/04-17.png" width="878" height="811"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1096786"></a>Figure 4.17 Transformations taking place going from the original <span class="fm-code-in-figurecaption">Average</span> series to the <span class="fm-code-in-figurecaption">Average</span> <span class="fm-code-in-figurecaption">Diff</span> series</p>

  <p class="body"><a class="calibre8" id="pgfId-1084036"></a>Finally, we can visualize how the values behave (figure 4.18) using the <span class="fm-code-in-text">data["Average Diff"].plot(figsize=(12,6))</span> line.</p>

  <p class="fm-figure"><img alt="04-18" class="calibre10" src="../../OEBPS/Images/04-18.png" width="1100" height="571"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1096820"></a>Figure 4.18 Relative change of values (i.e., <span class="fm-code-in-figurecaption">Average[t]-Average[t-1]</span>) of CO2 concentration over time</p>

  <p class="body"><a class="calibre8" id="pgfId-1084043"></a>Can you see the difference? From an ever-increasing data stream, we have gone to a stream that changes within a short vertical span. The next step is creating batches of data for the model to learn. How do we create batches of data for a time series problem? Remember, we cannot just randomly sample data naively, as each input depends on its predecessors.</p>

  <p class="body"><a class="calibre8" id="pgfId-1084044"></a>Let’s assume we want to use 12 past CO2 concentration values (i.e., 12 time steps) to predict the current CO2 concentration value. The number of time steps is a hyperparameter you must choose carefully. In order to choose this hyperparameter confidently, you must have a solid understanding of the data and the memory limitations of the model you are using.</p>

  <p class="body"><a class="calibre8" id="pgfId-1084045"></a>We first randomly choose a position in the sequence and take the 12 values from that point on as the inputs and the 13<sup class="fm-superscript">th</sup> value as the output we’re interested in predicting so that the total sequence length (<span class="fm-code-in-text">n_seq</span>) you sample at a time is 13. If you do this process 10 times, you will have a batch of data with 10 elements. As you can see, this process exploits the randomness while preserving the temporal characteristics of the data, and while feeding data to the model. Figure 4.19 visually describes this process.</p>

  <p class="fm-figure"><img alt="04-19" class="calibre10" src="../../OEBPS/Images/04-19.png" width="931" height="483"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1096854"></a>Figure 4.19 Batching time series data. <span class="fm-code-in-figurecaption">n_seq</span> represents the number of time steps we see at a given time to create a single input and an output.</p>

  <p class="body"><a class="calibre8" id="pgfId-1084052"></a>To do this in Python, let’s write a function that gives the data at all positions as a single data set. In other words, this function returns all possible consecutive sequences with 12 elements as <span class="fm-code-in-text">x</span> and the corresponding next value for each sequence as <span class="fm-code-in-text">y</span>. It is possible to perform the shuffling while feeding this data to the model<a class="calibre8" id="marker-1084055"></a>, as the next listing shows.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1084057"></a>Listing 4.4 The code for generating time-series data sequences for the model</p>
  <pre class="programlisting">import numpy as np
 
def generate_data(co2_arr,n_seq):
    x, y = [],[]
    for i in range(co2_arr.shape[0]-n_seq):
        x.append(co2_arr[i:i+n_seq-1])         <span class="fm-combinumeral">❶</span>
        y.append(co2_arr[i+n_seq-1:i+n_seq])   <span class="fm-combinumeral">❷</span>
    x = np.array(x)                            <span class="fm-combinumeral">❸</span>
    y = np.array(y)                            <span class="fm-combinumeral">❸</span>
    return x,y</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1094174"></a><span class="fm-combinumeral">❶</span> Extracting a sequence of values n_seq long</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1094195"></a><span class="fm-combinumeral">❷</span> Extracting the next value in the sequence as the output</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1094212"></a><span class="fm-combinumeral">❸</span> Combining everything into an array</p>

  <h3 class="fm-head1" id="sigil_toc_id_55"><a id="pgfId-1084071"></a>4.3.2 Implementing the model</h3>

  <p class="body"><a class="calibre8" id="pgfId-1084074"></a>With<a class="calibre8" id="marker-1084073"></a> a good understanding of the data, we can start implementing the network. We will implement a network that has the following:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1084076"></a>A <span class="fm-code-in-text">rnn</span> layer<a class="calibre8" id="marker-1084075"></a> with 64 hidden units</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1084078"></a>A <span class="fm-code-in-text">Dense</span> layer<a class="calibre8" id="marker-1084077"></a> with 64 hidden units and a ReLU activation</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1084080"></a>A <span class="fm-code-in-text">Dense</span> layer<a class="calibre8" id="marker-1084079"></a> with a single output and a linear activation</p>
    </li>
  </ul>
  <pre class="programlisting">from tensorflow.keras import layers, models
 
rnn = models.Sequential([
    layers.SimpleRNN(64),
    layers.Dense(64, activation='relu'),
    layers.Dense(1)
])</pre>

  <p class="body"><a class="calibre8" id="pgfId-1084088"></a>Note that the hyperparameters of the network (e.g., number of hidden units) have been chosen empirically to work well for the given problem. The first layer is the most crucial component of the network, as it is the element that makes it possible to learn from time series data. The <span class="fm-code-in-text">SimpleRNN</span> layer<a class="calibre8" id="marker-1084089"></a> encapsulates the functionality shown in figure 4.20.</p>

  <p class="fm-figure"><img alt="04-20" class="calibre10" src="../../OEBPS/Images/04-20.png" width="1022" height="586"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1096888"></a>Figure 4.20 The functionality of a <span class="fm-code-in-figurecaption">SimpleRNN</span> cell. The cell goes from one input to another while producing a memory at every time step. The next step consumes the current input as well as the memory from the previous time step.</p>

  <p class="body"><a class="calibre8" id="pgfId-1084096"></a>The computations that happen in an RNN are more sophisticated than in an FCN. An RNN goes from one input to the other in the input sequence (i.e., x1, x2, x3) in the given order. At each step, the recurrent layer produces an output (i.e., o1, o2, o3) and passes the hidden computation (h0, h1, h2, h3) to the next time step. Here, the first hidden state (h0) is typically set to zero.</p>

  <p class="body"><a class="calibre8" id="pgfId-1084098"></a>At a given time step, the recurrent layer computes a hidden state, just like a <span class="fm-code-in-text">Dense</span> layer<a class="calibre8" id="marker-1084097"></a>. However, the specific computations involved are bit more complex and are out of the scope of this book. The hidden state size is another hyperparameter of the recurrent layer. The recurrent layer takes the current input as well as the previous hidden state computed by the cell. A larger-sized hidden state helps to maintain more memory but increases the memory requirement of the network. As the hidden state is dependent on itself from the previous time step, these networks are called RNNs.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1084099"></a>The algorithm used for SimpleRNN</p>

    <p class="fm-sidebar-text"><a id="pgfId-1084101"></a>The computations mimicked by the <span class="fm-code-in-text1">SimpleRNN</span> layer<a id="marker-1087192"></a> are also known as <i class="fm-italics">Elman networks</i><a id="marker-1087194"></a>. To learn more about specific computations taking place in recurrent layers, you can read the paper “Finding Structure in Time” by J.L. Elman (1990). For a more high-level overview of later variations of RNNs and their differences, see <span class="fm-hyperlink"><a class="url" href="http://mng.bz/xnJg">http://mng.bz/xnJg</a></span> and <span class="fm-hyperlink"><a class="url" href="http://mng.bz/Ay2g">http://mng.bz/Ay2g</a></span></p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1084104"></a>By default, the <span class="fm-code-in-text">SimpleRNN</span> does not expose the hidden state to the developer and will be propagated between time steps automatically. For this task, we only need the final output produced by each time step, which is the output of that layer by default. Therefore, you can simply connect the <span class="fm-code-in-text">SimpleRNN</span> in the Sequential API to a <span class="fm-code-in-text">Dense</span> layer<a class="calibre8" id="marker-1084105"></a> without any additional work.</p>

  <p class="body"><a class="calibre8" id="pgfId-1084106"></a>Did you notice that we haven’t provided an <span class="fm-code-in-text">input_shape</span> to the first layer? This is possible, as long as you provide the data in the correct shape during model fitting. Keras builds the layers lazily, so until you feed data to your model, the model doesn’t need to know the input sizes. But it is always safer to set the <span class="fm-code-in-text">input_shape</span> argument in the first layer of the model to avoid errors. For example, in the model we defined, the first layer (i.e., the <span class="fm-code-in-text">SimpleRNN</span> layer<a class="calibre8" id="marker-1084107"></a>) can be changed to <span class="fm-code-in-text">layers.SimpleRNN(64, input_shape=x)</span>, where <span class="fm-code-in-text">x</span> is a tuple containing the shape of the data accepted by the model.</p>

  <p class="body"><a class="calibre8" id="pgfId-1084108"></a>Another important difference in this model is that it is a regression model, not a classification model. In a classification model, there are distinct classes (represented by output nodes), and we try to associate a given input with a distinct class (or a node). A regression model predicts a continuous value(s) as the output. Here, in our regression model, there is no notion of classes in the outputs, but a real continuous value representing CO2 concentration. Therefore, we have to choose the loss function appropriately. In this case, we will use mean squared error (MSE<a class="calibre8" id="marker-1084109"></a>) as the loss. MSE is a very common loss function for regression problems. We will compile the <span class="fm-code-in-text">rnn</span> with the MSE loss and the <span class="fm-code-in-text">adam</span> optimizer<a class="calibre8" id="marker-1084110"></a>:</p>
  <pre class="programlisting">rnn.compile(loss='mse', optimizer='adam')</pre>

  <p class="body"><a class="calibre8" id="pgfId-1084112"></a>Let’s cross our fingers and train our model:</p>
  <pre class="programlisting">x, y = generate_data(data[“Average Diff”], n_seq=13)
rnn.fit(x, y, shuffle=True, batch_size=64, epochs=25)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1084115"></a>You’ll get the following exception:</p>
  <pre class="programlisting">ValueError: 
<span class="fm-code-continuation-arrow">➥</span> Input 0 of layer sequential_1 is incompatible with the layer: 
<span class="fm-code-continuation-arrow">➥</span> expected ndim=3, found ndim=2. Full shape received: [None, 12]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1084119"></a>It seems we have done something wrong. The line we just ran resulted in an exception, which says something is wrong with the dimensionality of the data given to the layer <span class="fm-code-in-text">sequential_1</span> (i.e., the <span class="fm-code-in-text">SimpleRNN</span> layer<a class="calibre8" id="marker-1084121"></a>). Specifically, the <span class="fm-code-in-text">sequential_1</span> layer expects a three-dimensional input but has a two-dimensional input. We need to investigate what’s happening here and solve this.</p>

  <p class="body"><a class="calibre8" id="pgfId-1084124"></a>The problem is that the <span class="fm-code-in-text">SimpleRNN</span><a class="calibre8" id="marker-1084123"></a> (or any other sequential layer in <span class="fm-code-in-text">tf.keras</span>) only accepts data in a very specific format. The data needs to be three-dimensional, with the following dimensions, in this order:</p>

  <ol class="calibre11">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1084125"></a>Batch dimension</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1084126"></a>Time dimension</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1084127"></a>Feature dimension</p>
    </li>
  </ol>

  <p class="body"><a class="calibre8" id="pgfId-1084128"></a>Even when you have a single element for any of these dimensions, they need to be present as a dimension of size 1 in the data. Let’s look at what the dimensionality of <span class="fm-code-in-text">x</span> is by printing <span class="fm-code-in-text">x.shape</span>. You will get <span class="fm-code-in-text">x.shape</span> = (429, 12). Now we know what went wrong. We tried to pass a two-dimensional data set when we should have passed a three-dimensional one. In this case, we need to reshape <span class="fm-code-in-text">x</span> into a tensor of shape (492, 12, 1). Let’s change our <span class="fm-code-in-text">generate_data(...)</span> function to reflect this change in the following listing.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1084131"></a>Listing 4.5 The previous <span class="fm-code-in-listingcaption">generate_data()</span> function with data in the correct shape</p>
  <pre class="programlisting">import numpy as np
 
def generate_data(co2_arr,n_seq):
    x, y = [],[]                              <span class="fm-combinumeral">❶</span>
    for i in range(co2_arr.shape[0]-n_seq):   <span class="fm-combinumeral">❷</span>
        x.append(co2_arr[i:i+n_seq-1])        <span class="fm-combinumeral">❸</span>
        y.append(co2_arr[i+n_seq-1:i+n_seq])  <span class="fm-combinumeral">❸</span>
    x = np.array(x).reshape(-1,n_seq-1,1)     <span class="fm-combinumeral">❹</span>
    y = np.array(y) 
    return x,y</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1093951"></a><span class="fm-combinumeral">❶</span> Create two lists to hold input sequences and scalar output targets.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1093972"></a><span class="fm-combinumeral">❷</span> Iterate through all the possible starting points in the data for input sequences.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1093989"></a><span class="fm-combinumeral">❸</span> Create the input sequence and the output target at the ith position.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1094006"></a><span class="fm-combinumeral">❹</span> Convert x from a list to an array and make x a 3D tensor to be accepted by the RNN.</p>

  <p class="body"><a class="calibre8" id="pgfId-1084146"></a>Let’s try training our model now:</p>
  <pre class="programlisting">x, y = generate_data(data[“Average Diff”], n_seq=13)
rnn.fit(x, y, shuffle=True, batch_size=64, epochs=25)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1084149"></a>You should see the MSE of the model going down:</p>
  <pre class="programlisting">Train on 429 samples
Epoch 1/25
429/429 [==============================] - 1s 2ms/sample - loss: 0.4951
Epoch 2/25
429/429 [==============================] - 0s 234us/sample - loss: 0.0776
...
Epoch 24/25
429/429 [==============================] - 0s 234us/sample - loss: 0.0153
Epoch 25/25
429/429 [==============================] - 0s 234us/sample - loss: 0.0152</pre>

  <p class="body"><a class="calibre8" id="pgfId-1084160"></a>We start with a loss of approximately 0.5 and end up with a loss of roughly 0.015. This is a very positive sign, as it indicates the model is learning the trends present in the<a class="calibre8" id="marker-1084162"></a> data.</p>

  <h3 class="fm-head1" id="sigil_toc_id_56"><a id="pgfId-1084163"></a>4.3.3 Predicting future CO2 values with the trained model</h3>

  <p class="body"><a class="calibre8" id="pgfId-1084166"></a>Thus<a class="calibre8" id="marker-1084165"></a> far, we have focused on classification tasks. It is much easier to evaluate models on classification tasks than regression tasks. In classification tasks (assuming a balanced data set), by computing the overall accuracy on the data, we can get a decent representative number on how well our model is doing. In regression tasks it’s not so simple. We cannot measure an accuracy on regressed values, as the predictions are real values, not classes. For example, the magnitude of the mean squared loss depends on values we are regressing, which makes them difficult to objectively interpret. To address this, we predict the values for the next five years and visually inspect what the model is predicting (see the next listing).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1084168"></a>Listing 4.6 The future CO2 level prediction logic using the trained model</p>
  <pre class="programlisting">history = data["Average Diff"].values[-12:].reshape(1,-1,1)     <span class="fm-combinumeral">❶</span>
true_vals = []
prev_true = data["Average"].values[-1]                          <span class="fm-combinumeral">❷</span>
for i in range(60):                                             <span class="fm-combinumeral">❸</span>
    p_diff = rnn.predict(history).reshape(1,-1,1)               <span class="fm-combinumeral">❹</span>
    history = np.concatenate((history[:,1:,:],p_diff),axis=1)   <span class="fm-combinumeral">❺</span>
    true_vals.append(prev_true+p_diff[0,0,0])                   <span class="fm-combinumeral">❻</span>
    prev_true = true_vals[-1]                                   <span class="fm-combinumeral">❼</span></pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1093433"></a><span class="fm-combinumeral">❶</span> The first data sequence to start predictions from, which is reshaped to the correct shape the SimpleRNN accepts</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1093566"></a><span class="fm-combinumeral">❷</span> Save the very last absolute CO2 concentration value to compute the actual values from the relative predictions.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1093583"></a><span class="fm-combinumeral">❸</span> Predict for the next 60 months.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1093600"></a><span class="fm-combinumeral">❹</span> Make a prediction using the data sequence.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1093617"></a><span class="fm-combinumeral">❺</span> Modify the history so that the latest prediction is included.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1093634"></a><span class="fm-combinumeral">❻</span> Compute the absolute CO2 concentration.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1093434"></a><span class="fm-combinumeral">❼</span> Update prev_true so that the absolute CO2 concentration can be computed in the next time step.</p>

  <p class="body"><a class="calibre8" id="pgfId-1084184"></a>Let’s review what we have done. First, we extracted the last 12 CO2 values (from the <span class="fm-code-in-text">Average Diff</span> column) from our training data to predict the first future CO2 value and reshaped it to the correct shape the model expects the data to be in:</p>
  <pre class="programlisting">history = data["Average Diff"].values[-12:].reshape(1,-1,1)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1084186"></a>Then, we captured the predicted CO2 values in <span class="fm-code-in-text">true_vals</span> list. Remember that our model only predicts the relative movement of CO2 values with respect to the previous CO2 values. Therefore, after the model predicts, to get the absolute CO2 value, we need the last CO2 value. <span class="fm-code-in-text">prev_true</span> captures this information, which initially has the very last value in the <span class="fm-code-in-text">Average</span> column of the data:</p>
  <pre class="programlisting">prev_true = data["Average"].values[-1]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1084188"></a>Now, for the next 60 months (or 5 years), we can recursively predict CO2 values, while making the last predicted the next input to the network. To do this we first predict a value using the <span class="fm-code-in-text">predict(...)</span> method<a class="calibre8" id="marker-1084189"></a> provided in Keras. Then, we need to make sure the prediction is also a three-dimensional tensor (though it’s a single value). Then we modify the history variable:</p>
  <pre class="programlisting">history = np.concatenate((history[:,1:,:],p_diff),axis=1)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1084191"></a>We are taking all but the first value from the history and appending the last predicted value to the end. Then we append the absolute predicted CO2 value by adding the <span class="fm-code-in-text">prev_true</span> value to <span class="fm-code-in-text">p_diff</span>:</p>
  <pre class="programlisting">true_vals.append(prev_true+p_diff[0,0,0])</pre>

  <p class="body"><a class="calibre8" id="pgfId-1084193"></a>Finally, we update <span class="fm-code-in-text">prev_true</span> to the last absolute CO2 value we predicted:</p>
  <pre class="programlisting">prev_true = true_vals[-1]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1084195"></a>By doing this set of operations recursively, we can get the predictions for the next 60 months (captured in <span class="fm-code-in-text">true_vals</span> variable). If we visualize the predicted values, they should look like figure 4.21.</p>

  <p class="fm-figure"><img alt="04-21" class="calibre10" src="../../OEBPS/Images/04-21.png" width="1097" height="661"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1096922"></a>Figure 4.21 The CO2 concentration predicted over the next five years. Dashed line represents the trend from the current data, and the solid line represents the predicted trend.</p>

  <p class="body"><a class="calibre8" id="pgfId-1084203"></a>Great work! Given the simplicity of the model, predictions look very promising. The model has definitely captured the annual trend of the CO2 concentration and has learned that the CO2 level is going to keep going up. You can now go to your boss and explain factually why we should be worried about climate change and dangerous levels of CO2 in the future. We end our discussion about different neural networks here.</p>

  <p class="fm-head2"><a id="pgfId-1084204"></a>Exercise 3</p>

  <p class="body"><a class="calibre8" id="pgfId-1084205"></a>Impressed by your work on predicting the CO2 concentration, your boss has provided you the data and asked you to enhance the model to predict both CO2 and temperature values. Keeping the other hyperparameters the same, how would you change the model for this task? Make sure you specify the <span class="fm-code-in-text">input_shape</span> parameter<a class="calibre8" id="marker-1084206"></a> for the<a class="calibre8" id="marker-1084208"></a> first<a class="calibre8" id="marker-1084209"></a><a class="calibre8" id="marker-1084210"></a> layer.</p>

  <h2 class="fm-head" id="sigil_toc_id_57"><a id="pgfId-1084211"></a>Summary</h2>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1084212"></a>Fully connected networks (FCNs) are one of the most straightforward neural networks.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1084213"></a>FCNs can be implemented using the Keras <span class="fm-code-in-text">Dense</span> layer.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1084214"></a>Convolutional neural networks (CNNs) are a popular choice for computer vision tasks.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1084215"></a>TensorFlow offers various layers, such as <span class="fm-code-in-text">Conv2D</span>, <span class="fm-code-in-text">MaxPool2D</span>, and <span class="fm-code-in-text">Flatten</span>, that help us implement CNNs quickly.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1084216"></a>CNNs have parameters such as kernel size, stride, and padding that must be set carefully. If not, this can lead to incorrectly shaped tensors and runtime errors.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1084217"></a>Recurrent neural networks (RNNs) are predominantly used to learn from time-series data.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1084218"></a>The typical RNN expects the data to be organized into a three-dimensional tensor with a batch, time, and feature dimension.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1084219"></a>The number of time steps the RNN looks at is an important hyperparameter that should be chosen based on the data.</p>
    </li>
  </ul>

  <h2 class="fm-head" id="sigil_toc_id_58"><a id="pgfId-1084220"></a>Answers to exercises</h2>

  <p class="body"><a class="calibre8" id="pgfId-1084221"></a><b class="fm-bold">Exercise 1:</b> You can do this using the Sequential API, and you will be using only the <span class="fm-code-in-text">Dense</span> layer.</p>

  <p class="body"><a class="calibre8" id="pgfId-1084222"></a><b class="fm-bold">Exercise 2</b></p>
  <pre class="programlisting">autoencoder = models.Sequential(
    [layers.Dense(32, activation='sigmoid', input_shape=(512,)),
    layers.Dense(16, activation='sigmoid'),
    layers.Dense(512, activation='sigmoid')]
)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1084229"></a><b class="fm-bold">Exercise 3</b></p>
  <pre class="programlisting">rnn = models.Sequential([
    layers.SimpleRNN(64, input_shape=(12, 2)),
    layers.Dense(64, activation='relu'),
    layers.Dense(2)
])</pre>
</div>
</div>
</body>
</html>