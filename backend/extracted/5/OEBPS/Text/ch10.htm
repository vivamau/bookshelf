<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xml:lang="en"
      lang="en"
      xmlns="http://www.w3.org/1999/xhtml"
      xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<title>TensorFlow in Action</title>
<link rel="stylesheet" type="text/css" href="../../override_v1.css"/>
<link rel="stylesheet" type="text/css" href="../../stylesheet.css"/><link rel="stylesheet" type="text/css" href="../../page_styles.css"/>
</head>
<body>
<div id="book-content">
<div id="sbo-rt-content" class="calibre"><h1 class="tochead"><a id="pgfId-1134487"></a>10 Natural language processing with TensorFlow: Language modeling</h1>

  <p class="co-summary-head"><a id="pgfId-1134491"></a>This chapter<a id="marker-1136526"></a><a id="marker-1136527"></a><a id="marker-1136528"></a> covers</p>

  <ul class="calibre9">
    <li class="co-summary-bullet"><a class="calibre8" id="pgfId-1134492"></a>Implementing an NLP data pipeline with TensorFlow</li>

    <li class="co-summary-bullet"><a class="calibre8" id="pgfId-1134494"></a><a class="calibre8" id="aHlk60153502"></a>Implementing a GRU-based language model</li>

    <li class="co-summary-bullet"><a class="calibre8" id="pgfId-1134495"></a>Using a perplexity metric for evaluating language models</li>

    <li class="co-summary-bullet"><a class="calibre8" id="pgfId-1134496"></a>Defining an inference model to generate new text from the trained model</li>

    <li class="co-summary-bullet"><a class="calibre8" id="pgfId-1134497"></a>Implementing beam search to uplift the quality of generated text</li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1134498"></a>In the last chapter, we discussed an important NLP task called sentiment analysis. In that chapter, you used a data set of video game reviews and trained a model to predict whether a review carried a negative or positive sentiment by analyzing the text. You learned about various preprocessing steps that you can perform to improve the quality of the text, such as removing stop words and lemmatizing (i.e., converting words to a base form; e.g., plural to singular). You used a special type of model known as long short-term memory (LSTM<a class="calibre8" id="marker-1134499"></a>). LSTM models can process sequences such as sentences and learn the relationships and dependencies in them to produce an outcome. LSTM models do this by maintaining a state (or memory) containing information about the past, as it processes a sequence one element at a time. The LSTM model can use the memory of past inputs it has seen along with the current input to produce an output at any given time.</p>

  <p class="body"><a class="calibre8" id="pgfId-1134500"></a>In this chapter, we will discuss a new task known as <i class="fm-italics">language modeling</i>. Language modeling has been at the heart of NLP. Language modeling refers to the task of predicting the next word given a sequence of previous words. For example, given the sentence, “I went swimming in the ____,” the model would predict the word “pool.” Ground-shattering models like BERT (bidirectional encoder representation from Transformers, which is a type of Transformer-based model<a class="calibre8" id="marker-1134501"></a>) are trained using language modeling tasks. This is a prime example of how language modelling can help to actualize innovative models that go on to be used in a plethora of areas and use cases.</p>

  <p class="body"><a class="calibre8" id="pgfId-1134502"></a>In my opinion, language modeling is an underdog in the world of NLP. It is not appreciated enough, mostly due to the limited use cases the task itself helps to realize. However, language modeling can provision the much-needed linguistic knowledge (e.g., semantics, grammar, dependency parsing, etc.) to solve other downstream use cases (e.g., information retrieval, question answering, machine translation, etc.). Therefore, as an NLP practitioner, you must understand the language modeling task.</p>

  <p class="body"><a class="calibre8" id="pgfId-1134503"></a>In this chapter, you will build a language model. You will learn about the various preprocessing steps involved, such as using n-grams instead of full words as features for the model to reduce the size of the vocabulary. You can convert any text to n-grams by splitting it every n characters (e.g., if you use bi-grams, <span class="fm-code-in-text">aabbbccd</span> becomes <span class="fm-code-in-text">aa</span>, <span class="fm-code-in-text">bb</span>, <span class="fm-code-in-text">bc</span>, and <span class="fm-code-in-text">cd</span>). You will define a <span class="fm-code-in-text">tf.data</span> pipeline<a class="calibre8" id="marker-1134504"></a> that will do most of this preprocessing for us. Next, you will use a close relative of the LSTM model known as <i class="fm-italics">gated recurrent unit</i> (GRU<a class="calibre8" id="marker-1134505"></a><a class="calibre8" id="marker-1134506"></a>) to do the language modeling task. GRU is much simpler than the LSTM model, making it faster to train while maintaining similar performance to the LSTM model. We will use a special metric called perplexity to measure how good our model is. Perplexity measures how surprised the model was to see the next word in the corpus given the previous words. You will learn more about this metric later in the chapter. Finally, you will learn about a technique known as <i class="fm-italics">beam search</i>, which can uplift the quality of the text generated by the model by a significant margin.</p>

  <h2 class="fm-head" id="sigil_toc_id_135"><a id="pgfId-1134507"></a>10.1 Processing the data</h2>

  <p class="body"><a class="calibre8" id="pgfId-1134512"></a>You’ve<a class="calibre8" id="marker-1134509"></a><a class="calibre8" id="marker-1134510"></a><a class="calibre8" id="marker-1134511"></a> been closely following a new generation of deep learning models that have emerged, known as Transformers. These models have been trained using language modeling. It is a technique that can be used to train NLP models to generate stories/ Python code/movie scripts, depending on the training data used. The idea is that when a sequence of n words, predict the n + 1 word. The training data can easily be generated from a corpus of text by taking a sequence of text as the input and shifting it right by 1 to generate the targets. This can be done at a character level, word level, or n-gram level. We will use two-grams for the language modeling task. We will use a children’s story data set from Facebook known as bAbI (<span class="fm-hyperlink"><a class="url" href="https://research.fb.com/downloads/babi/">https://research.fb.com/downloads/babi/</a></span>). You will create a TensorFlow data pipeline that performs these transformations to generate inputs and targets from text.</p>

  <h3 class="fm-head1" id="sigil_toc_id_136"><a id="pgfId-1134514"></a>10.1.1 What is language modeling?</h3>

  <p class="body"><a class="calibre8" id="pgfId-1134516"></a>We<a class="calibre8" id="marker-1136739"></a> have briefly discussed the task of language modeling. In a nutshell, language modeling, for the text <i class="fm-timesitalic">w</i><sub class="fm-subscript">1</sub>, <i class="fm-timesitalic">w</i><sub class="fm-subscript">2</sub>, ..., <i class="fm-timesitalic">w</i><sub class="fm-subscript">n</sub><sub class="fm-subscript">-1</sub>, <i class="fm-timesitalic">w</i><sub class="fm-subscript">n</sub>, where <i class="fm-timesitalic">w</i><sub class="fm-subscript">i</sub> is the <i class="fm-timesitalic">i</i><sup class="fm-superscript">th</sup> word in the text, computes the probability of <i class="fm-timesitalic">w</i><sub class="fm-subscript">n</sub> given <i class="fm-timesitalic">w</i><sub class="fm-subscript">1</sub>, <i class="fm-timesitalic">w</i><sub class="fm-subscript">2</sub>, ..., <i class="fm-timesitalic">w</i><sub class="fm-subscript">n</sub><sub class="fm-subscript">-1</sub>. In mathematical notation</p>

  <p class="fm-equation"><a id="pgfId-1136562"></a><i class="fm-italics">P</i>(<i class="fm-italics">w</i> <sub class="fm-subscript">n</sub>|<i class="fm-italics">w</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">w</i><sub class="fm-subscript">2</sub>, ..., <i class="fm-italics">w</i><sub class="fm-subscript">n</sub><sub class="fm-subscript">-1</sub>)</p>

  <p class="body"><a class="calibre8" id="pgfId-1134521"></a>In other words, it predicts <i class="fm-timesitalic">w</i><sub class="fm-subscript">n</sub> given <i class="fm-timesitalic">w</i><sub class="fm-subscript">1</sub>, <i class="fm-timesitalic">w</i><sub class="fm-subscript">2</sub>, ..., <i class="fm-timesitalic">w</i><sub class="fm-subscript">n</sub><sub class="fm-subscript">-1</sub>. When training the model, we train it to maximize this probability; in other words</p>

  <p class="fm-equation"><a id="pgfId-1136753"></a>argmax<sub class="fm-subscript">W</sub> <i class="fm-italics">P</i>(<i class="fm-italics">w</i> <sub class="fm-subscript">n</sub>|<i class="fm-italics">w</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">w</i><sub class="fm-subscript">2</sub>, ..., <i class="fm-italics">w</i><sub class="fm-subscript">n</sub><sub class="fm-subscript">-1</sub>)</p>

  <p class="body"><a class="calibre8" id="pgfId-1134526"></a>where the probability is computed using a model that has the trainable weights/ parameters W. This computation becomes computationally infeasible for large texts, as we need to look at it from the current word all the way to the very first word. To make this computationally realizable, let’s use the <i class="fm-italics">Markov property</i><a class="calibre8" id="marker-1134527"></a>, which states that you can approximate this sequence with limited history; in other words</p>

  <p class="fm-equation"><a id="pgfId-1136819"></a><i class="fm-italics">P</i>(<i class="fm-italics">w</i> <sub class="fm-subscript">n</sub>|<i class="fm-italics">w</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">w</i><sub class="fm-subscript">2</sub>, ..., <i class="fm-italics">w</i><sub class="fm-subscript">n</sub><sub class="fm-subscript">-1</sub>) <span class="fm-symbol1">≈</span> <i class="fm-italics">P</i>(<i class="fm-italics">w</i> <sub class="fm-subscript">n</sub>|<i class="fm-italics">w</i><sub class="fm-subscript">k</sub>, <i class="fm-italics">w</i><sub class="fm-subscript">k+1</sub>, ..., <i class="fm-italics">w</i><sub class="fm-subscript">n</sub><sub class="fm-subscript">-1</sub>) for some <i class="fm-italics">k</i></p>

  <p class="body"><a class="calibre8" id="pgfId-1134533"></a>As you can imagine, the smaller the <i class="fm-italics">k</i>, the better the approximation<a class="calibre8" id="marker-1134532"></a> is.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1134534"></a>Cloze task</p>

    <p class="fm-sidebar-text"><a id="pgfId-1134535"></a>Transformer models like BERT use a variant of language modeling called m<i class="fm-italics">asked language modeling</i><a id="marker-1146522"></a><a id="marker-1136912"></a>. Masked language modeling is inspired by the <i class="fm-italics">Cloze</i> task<a id="marker-1136913"></a> or the Cloze test. The idea is to ask the student, when given a sentence with one or more blanks, to fill in the blanks. This has been used in language assessment tests to measure the linguistic competency of students. In masked language modeling, the model becomes the student. Words are removed from inputs at random, and the model is asked to predict the missing word. This forms the foundation of the training process used in models like BERT.</p>
  </div>

  <h3 class="fm-head1" id="sigil_toc_id_137"><a id="pgfId-1134539"></a>10.1.2 Downloading and playing with data</h3>

  <p class="body"><a class="calibre8" id="pgfId-1134542"></a>As<a class="calibre8" id="marker-1134541"></a> the very first step, let’s download the data set using the code in the following listing.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1134544"></a>Listing 10.1 Downloading the Amazon review data set</p>
  <pre class="programlisting">import os
import requests
import tarfile
 
import shutil
# Retrieve the data
if not os.path.exists(os.path.join('data', 'lm','CBTest.tgz')):      <span class="fm-combinumeral">❶</span>
    url = "http:/ /www.thespermwhale.com/jaseweston/babi/CBTest.tgz"
    # Get the file from web
    r = requests.get(url)
 
    if not os.path.exists(os.path.join('data','lm')):
        os.mkdir(os.path.join('data','lm'))
    
    # Write to a file
    with open(os.path.join('data', 'lm', 'CBTest.tgz'), 'wb') as f:  <span class="fm-combinumeral">❷</span>
        f.write(r.content)
          
else:
    print("The tar file already exists.")
    
if not os.path.exists(os.path.join('data', 'lm', 'CBTest')):         <span class="fm-combinumeral">❸</span>
    # Write to a file
    tarf = tarfile.open(os.path.join("data","lm","CBTest.tgz"))
    tarf.extractall(os.path.join("data","lm"))  
else:
    print("The extracted data already exists")</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1151013"></a><span class="fm-combinumeral">❶</span> If the tgz file containing data has not been downloaded, download the data.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1151034"></a><span class="fm-combinumeral">❷</span> Write the downloaded data to the disk.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1151051"></a><span class="fm-combinumeral">❸</span> If the tgz file is available but has not been extracted, extract it to the given directory.</p>

  <p class="body"><a class="calibre8" id="pgfId-1134577"></a>Listing 10.1 will download the data to a local folder if it doesn’t already exist and extract the content. If you look in the data folder (specifically, data/lm/CBTest/data), you will see that it has three text files: cbt_train.txt, cbt_valid.txt, and cbt_test.txt. Each file contains a set of stories. We are going to read these files into memory. We will define a simple function to read these files into memory in the next listing.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1134579"></a>Listing 10.2 Reading the stories in Python</p>
  <pre class="programlisting">def read_data(path):
    stories = []                                           <span class="fm-combinumeral">❶</span>
 
    with open(path, 'r') as f:    
        s = []                                             <span class="fm-combinumeral">❷</span>
        for row in f:
 
            if row.startswith("_BOOK_TITLE_"):             <span class="fm-combinumeral">❸</span>
                if len(s)&gt;0:
                    stories.append(' '.join(s).lower())    <span class="fm-combinumeral">❹</span>
                s = []                                     <span class="fm-combinumeral">❺</span>
 
            s.append(row)                                  <span class="fm-combinumeral">❻</span>
 
 
    if len(s)&gt;0:
        stories.append(' '.join(s).lower())                <span class="fm-combinumeral">❼</span>
    
    return stories</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1150511"></a><span class="fm-combinumeral">❶</span> Define a list to hold all the stories.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1150539"></a><span class="fm-combinumeral">❷</span> Define a list to hold a story.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1150559"></a><span class="fm-combinumeral">❸</span> Whenever, we encounter a line that starts with _BOOK_TITLE, it’s a new story.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1150576"></a><span class="fm-combinumeral">❹</span> If we saw the beginning of a new story, add the already existing story to the list stories.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1150593"></a><span class="fm-combinumeral">❺</span> Reset the list containing the current story.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1150610"></a><span class="fm-combinumeral">❻</span> Append the current row of text to the list s.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1150627"></a><span class="fm-combinumeral">❼</span> Handle the edge case of the last story remaining in s once the loop is over.</p>

  <p class="body"><a class="calibre8" id="pgfId-1134606"></a>This code opens a given file and reads it row by row. We do have some additional logic to break the text into individual stories. As said earlier, each file contains multiple stories. And we want to create a list of strings at the end, where each string is a single story. The previous function does that. Next, we can read the text files and store them in variables like this:</p>
  <pre class="programlisting">stories = read_data(os.path.join('data','lm','CBTest','data','cbt_train.txt'))
val_stories = read_data(os.path.join('data','lm','CBTest','data','cbt_valid.txt'))
test_stories = read_data(os.path.join('data','lm','CBTest','data','cbt_test.txt'))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1134612"></a>Here, stories will contain the training data, <span class="fm-code-in-text">val_stories</span> will contain the validation data, and finally, <span class="fm-code-in-text">test_stories</span> will contain test data. Let’s quickly look at some high-level information about the data set:</p>
  <pre class="programlisting">print("Collected {} stories (train)".format(len(stories)))
print("Collected {} stories (valid)".format(len(val_stories)))
print("Collected {} stories (test)".format(len(test_stories)))
print(stories[0][:100])
print('\n', stories[10][:100])</pre>

  <p class="body"><a class="calibre8" id="pgfId-1134618"></a>This code checks how many stories are in each data set and prints the first 100 characters in the 11<sup class="fm-superscript">th</sup> story in the training set:</p>
  <pre class="programlisting">Collected 98 stories (train)
Collected 5 stories (valid)
Collected 5 stories (test)
 
chapter i. -lcb- chapter heading picture : p1.jpg -rcb- how the fairies 
<span class="fm-code-continuation-arrow">➥</span> were not invited to court .
 
 
 a tale of the tontlawald long , long ago there stood in the midst of a 
<span class="fm-code-continuation-arrow">➥</span> country covered with lakes a </pre>

  <p class="body"><a class="calibre8" id="pgfId-1134627"></a>Out of curiosity, let’s also analyze the vocabulary size we have to work with. To analyze the vocabulary size, we will first convert our list of strings to a list of lists of strings, where each string is a single word. Then we can leverage the built-in <span class="fm-code-in-text">Counter</span> object to get the word frequency of the text corpus. After that, we will create a pandas Series object with the frequencies as the values and words as indices and see how many words occur more than 10 times:</p>
  <pre class="programlisting">from collections import Counter
# Create a large list which contains all the words in all the reviews
data_list = [w for doc in stories for w in doc.split(' ')]
 
# Create a Counter object from that list
# Counter returns a dictionary, where key is a word and the value is the frequency
cnt = Counter(data_list)
 
# Convert the result to a pd.Series 
freq_df = pd.Series(
    list(cnt.values()), index=list(cnt.keys())
).sort_values(ascending=False)
 
# Count of words &gt;= n frequent
n=10
print("Vocabulary size (&gt;={} frequent): {}".format(n, (freq_df&gt;=n).sum()))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1134646"></a>This will return</p>
  <pre class="programlisting">,      348650
the    242890
.\n    192549
and    179205
to     120821
a      101990
of      96748
i       79780
he      78129
was     66593
dtype: int64
 
Vocabulary size (&gt;=10 frequent): 14473</pre>

  <p class="body"><a class="calibre8" id="pgfId-1134662"></a>Nearly 15,000 words; that’s quite a vocabulary—and that’s just the words that appear more than 10 times. In the previous chapter, we dealt with a vocabulary of approximately 11,000 words. So why should we be worried about the extra 4,000? Because more words mean more features for the model, and that means a larger number of parameters and more chances of overfitting. The short answer is it really depends on your use case.</p>

  <p class="body"><a class="calibre8" id="pgfId-1134663"></a>For example, in the sentiment analysis model we had in the last chapter, the final prediction layer was a single-node fully connected layer, regardless of the vocabulary size. However, in language modeling, the final prediction layer’s dimensionality depends on the vocabulary size, as the final goal is to predict the next word. This is done through a softmax layer that represents the probabilistic likelihood of the next word over the whole vocabulary, given a sequence of words. Not only the memory requirement, but also the computational time, increase as the softmax layer grows. Therefore, it is worthwhile investigating other techniques to reduce the vocabulary size.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1134664"></a>Is large vocabulary size the ultimate weakness of the softmax layer?</p>

    <p class="fm-sidebar-text"><a id="pgfId-1134665"></a>A major weakness of the softmax layer is its computational complexity. The softmax layer needs to first perform a matrix multiplication to get the logits (i.e., unnormalized scores output by the final layer of the network). Then it needs to sum over the last axis to compute softmax probabilities of the output. Specifically, for the input <span class="fm-code-in-text1">h</span>, the logits of the softmax layer are computed as</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1136958"></a><i class="fm-timesitalic">s</i> = <i class="fm-timesitalic">h</i>. <i class="fm-timesitalic">W</i> + <i class="fm-timesitalic">b</i> where <i class="fm-timesitalic">W</i> ∈ <i class="fm-timesitalic">R</i><sup class="fm-superscript1">|h|×|V|</sup> <sub class="fm-subscript1">∧</sub> <i class="fm-timesitalic">b</i> <sub class="fm-subscript1">∈</sub> <i class="fm-timesitalic">R</i><sup class="fm-superscript1">|V|</sup></p>

    <p class="fm-sidebar-text"><sup class="fm-superscript1"><br class="calibre3"/></sup></p>

    <p class="fm-sidebar-text"><a id="pgfId-1134670"></a>where <span class="fm-code-in-text1">W</span> is the weight matrix, <span class="fm-code-in-text1">b</span> is the bias of that final layer, <span class="fm-code-in-text1">|h|</span> is the size of the input, and <span class="fm-code-in-text1">|V|</span> is the size of the vocabulary. Then softmax normalization is applied as</p>

    <p class="fm-sidebar-text"><img alt="10_00a" class="calibre10" src="../../OEBPS/Images/10_00a.png" width="134" height="67"/><br class="calibre2"/>
    <a id="pgfId-1137090"></a></p>

    <p class="fm-sidebar-text"><a id="pgfId-1134675"></a>These computations should make it evident to you that a large vocabulary (whose size can easily reach hundreds of thousands for a real-world application) will create problems in executing this computation in a limited time during model training. Having to do this for thousands of batches of data makes the problem even worse. Therefore, better techniques to compute the loss without using all the logits have emerged. Two popular choices are</p>

    <ul class="calibre9">
      <li class="fm-sidebar-number">
        <p class="list1"><a id="pgfId-1134678"></a>Noise contrastive estimation (NCE<a id="marker-1151193"></a><a id="marker-1151194"></a>) loss</p>
      </li>

      <li class="fm-sidebar-number">
        <p class="list1"><a id="pgfId-1134679"></a>Hierarchical softmax</p>
      </li>
    </ul>

    <p class="fm-sidebar-text"><a id="pgfId-1134680"></a><b class="fm-bold">Noise contrastive estimation (NCE)</b><a id="marker-1140657"></a><a id="marker-1140658"></a></p>

    <p class="fm-sidebar-text"><a id="pgfId-1134681"></a>We will look at the prime motivation that drives these methods but not delve into the specifics, as this is considered out of scope for this book. For the details on these topics, refer to <span class="fm-hyperlink"><a class="url" href="https://ruder.io/word-embeddings-softmax">https://ruder.io/word-embeddings-softmax</a></span>. NCE only uses the logit indexed by the true target and a small sample of k random set of logits (termed <i class="fm-italics">noise</i>) to compute the loss. The more you match the true data distribution in the noise sample, the better the results will be. Specifically, if the true target is <i class="fm-italics">s</i> and the logit at the index <i class="fm-italics">s</i> is termed <i class="fm-italics">s</i><sub class="fm-subscript1">i</sub>, the following loss is used:</p>

    <p class="fm-sidebar-text"><img alt="10_00b" class="calibre10" src="../../OEBPS/Images/10_00b.png" width="421" height="79"/><br class="calibre2"/>
    <a id="pgfId-1137131"></a></p>

    <p class="fm-sidebar-text"><a id="pgfId-1134688"></a>Here, <i class="fm-timesitalic">σ</i> denotes that the sigmoidal activation is a common activation function used in neural networks and is computed as <i class="fm-timesitalic">σ</i> (<i class="fm-timesitalic">x</i>) = 1/(1 + <i class="fm-timesitalic">e</i><sup class="fm-superscript1">-x</sup>), where <i class="fm-timesitalic">s</i><sub class="fm-subscript1">i</sub> represents the logit value of the true target i, and j represents an index sampled from a predefined distribution over the vocabulary <i class="fm-timesitalic">P</i><sub class="fm-subscript1">n</sub>.</p>

    <p class="fm-sidebar-text"><br class="calibre2"/></p>

    <p class="fm-sidebar-text"><a id="pgfId-1134689"></a><b class="fm-bold">Hierarchical softmax</b></p>

    <p class="fm-sidebar-text"><a id="pgfId-1151223"></a>Unlike standard softmax, which has a flat structure where each node represents an element in the vocabulary, hierarchical softmax represents the words in the vocabulary as the leaf nodes in a binary tree, and the task becomes choosing whether to go left or right in order to reach the right node. The following figure depicts the formation</p>

    <p class="fm-sidebar-text"><a id="pgfId-1134690"></a>of the layer when hierarchical softmax is used. As is evident, to infer the probability of a word given the previous sequence of words, the layer only has to go through three steps of computation at max (shown by the dark path), as opposed to evaluating across all seven possible words.</p>

    <p class="fm-figure"><img alt="10-00unnumb" class="calibre10" src="../../OEBPS/Images/10-00unnumb.png" width="692" height="928"/><br class="calibre2"/></p>

    <p class="fm-figure-caption"><a id="pgfId-1151411"></a>Hierarchical softmax representation of the final layer. The dark path represents the path the model must follow to compute the probability of P(home| I went).</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1134698"></a>Next, we will see how we can deal with language in the case of a large vocabulary.</p>

  <h3 class="fm-head1" id="sigil_toc_id_138"><a id="pgfId-1134699"></a>10.1.3 Too large vocabulary? N-grams to the rescue</h3>

  <p class="body"><a class="calibre8" id="pgfId-1134704"></a>Here<a class="calibre8" id="marker-1134701"></a><a class="calibre8" id="marker-1134702"></a><a class="calibre8" id="marker-1134703"></a>, we start the first step of defining various text preprocessors and the data pipeline. We suspect that going forward with a large<a class="calibre8" id="marker-1134706"></a> vocabulary size can have adverse repercussions on our modeling journey. Let’s find some ways to reduce the vocabulary size. Given that children’s stories use a relatively simple language style, we can represent text as n-grams (at the cost of the expressivity of our model). N-grams are an approach where a word is decomposed to finer sub-words of fixed length. For example, the bigrams (or two-grams) of the sentence</p>
  <pre class="programlisting">I went to the bookshop</pre>

  <p class="body"><a class="calibre8" id="pgfId-1134708"></a>are</p>
  <pre class="programlisting">"I ", " w", "we", "en", "nt", "t ", " t", "to", "o ", "th", "he", "e ", " b", "bo", "oo", "ok", "ks", "sh", "ho", "op"</pre>

  <p class="body"><a class="calibre8" id="pgfId-1134710"></a>where three grams would be</p>
  <pre class="programlisting">"I w", " we", "wen", "ent", "nt ", "t t", " to", "to ", "o t", " th", "the", "he ", "e b", " bo", "boo", "ook", "oks", "ksh", "sho", "hop"</pre>

  <p class="body"><a class="calibre8" id="pgfId-1134712"></a>The unigrams (or one-grams) would simply be the individual characters. In other words, we are moving a window of fixed length (with a stride of 1), while reading the characters within that window at a time. You could also generate n-grams without overlaps by moving the window at a stride equal to the length of the window. For example, bigrams without overlapping would be</p>
  <pre class="programlisting">"I ", "we", "nt", " t", "o ", "th", "e ", "bo", "ok", "sh", "op"</pre>

  <p class="body"><a class="calibre8" id="pgfId-1134714"></a>Which one to use depends on your use case. For the language modeling task, it makes sense to use the non-overlapping approach. This is because by joining the n-grams that we generated, we can easily generate readable text. For certain use cases, the non-overlapping approach can be disadvantageous as it leads to a coarser representation of text because it doesn’t capture all the different n-grams that appear in text.</p>

  <p class="body"><a class="calibre8" id="pgfId-1134715"></a>By using bigrams instead of words to develop your vocabulary, you can cut down the size of the vocabulary by a significant factor. There are many other advantages that come with the n-gram approach, as we will see soon. We will write a function to generate n-grams given a text string:</p>
  <pre class="programlisting">def get_ngrams(text, n):
    return [text[i:i+n] for i in range(0,len(text),n)]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1134718"></a>All we do here is go from the beginning to the end of the text with a stride equal to n and read the sequence of characters from position <span class="fm-code-in-text">i</span> to <span class="fm-code-in-text">i+n</span>. We can test how this performs on sample text:</p>
  <pre class="programlisting">test_string = "I like chocolates"
print("Original: {}".format(test_string))
for i in list(range(3)):
    print("\t{}-grams: {}".format(i+1, get_ngrams(test_string, i+1)))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1134723"></a>This will print the following output:</p>
  <pre class="programlisting">Original: I like chocolates
    1-grams: ['I', ' ', 'l', 'i', 'k', 'e', ' ', 'c', 'h', 'o', 'c', 
<span class="fm-code-continuation-arrow">➥</span> 'o', 'l', 'a', 't', 'e', 's']
    2-grams: ['I ', 'li', 'ke', ' c', 'ho', 'co', 'la', 'te', 's']
    3-grams: ['I l', 'ike', ' ch', 'oco', 'lat', 'es']</pre>

  <p class="body"><a class="calibre8" id="pgfId-1134730"></a>Let’s now repeat the process for analyzing the vocabulary size, but with n-grams instead of words:</p>
  <pre class="programlisting">from itertools import chain
from collections import Counter
 
# Create a counter with the bi-grams
ngrams = 2
 
text = chain(*[get_ngrams(s, ngrams) for s in stories])
cnt = Counter(text)
 
# Create a pandas series with the counter results
freq_df = pd.Series(list(cnt.values()), index=list(cnt.keys())).sort_values(ascending=False)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1134743"></a>Now, if we check the number of words that appear at least 10 times in the text</p>
  <pre class="programlisting">n_vocab = (freq_df&gt;=10).sum()
print("Size of vocabulary: {}".format(n_vocab))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1134748"></a>we will see</p>
  <pre class="programlisting">Size of vocabulary: 735</pre>

  <p class="body"><a class="calibre8" id="pgfId-1134756"></a>Wow! Compared to the 15,000 words we had, 735 is much smaller and more<a class="calibre8" id="marker-1134753"></a><a class="calibre8" id="marker-1134754"></a><a class="calibre8" id="marker-1134755"></a> manageable.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1134757"></a>Advantages of n-grams</p>

    <p class="fm-sidebar-text"><a id="pgfId-1134758"></a>Here are some of the main advantages of using n-grams over words:</p>

    <ul class="calibre9">
      <li class="fm-sidebar-number">
        <p class="list1"><a id="pgfId-1134759"></a>The limited number of n-grams for small n limits the vocabulary size, leading to both memory and computational advantages.</p>
      </li>

      <li class="fm-sidebar-number">
        <p class="list1"><a id="pgfId-1134760"></a>N-grams lead to fewer chances of out-of-vocabulary words, as an unseen word can usually be constructed using n-grams seen in the past.</p>
      </li>
    </ul>
  </div>

  <h3 class="fm-head1" id="sigil_toc_id_139"><a id="pgfId-1134762"></a>10.1.4 Tokenizing text</h3>

  <p class="body"><a class="calibre8" id="pgfId-1134763"></a>We will <i class="fm-italics">tokenize</i> the text now (i.e., split a string into a list of smaller tokens—words). By the end of this section, you will have defined and fitted a tokenizer on the bigrams generated for your text. First, let’s import the <span class="fm-code-in-text">Tokenizer</span> from TensorFlow:</p>
  <pre class="programlisting">from tensorflow.keras.preprocessing.text import Tokenizer</pre>

  <p class="body"><a class="calibre8" id="pgfId-1134771"></a>We don’t have to do any preprocessing and want to convert text to word IDs as it is. We<a class="calibre8" id="marker-1134768"></a><a class="calibre8" id="marker-1134769"></a><a class="calibre8" id="marker-1134770"></a> will define the <span class="fm-code-in-text">num_words</span> argument to limit the vocabulary size as well as an <span class="fm-code-in-text">oov_token</span> that will be used to replace all the n-grams that appear less than 10 times in the training corpus:</p>
  <pre class="programlisting">tokenizer = Tokenizer(num_words=n_vocab, oov_token='unk', lower=False)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1134775"></a>Let’s generate n-grams from the stories in training data. <span class="fm-code-in-text">train_ngram_stories</span> will be a list of lists of strings, where the inner list represents a list of bigrams for a single story and the outer list represents all the stories in the training data set:</p>
  <pre class="programlisting">train_ngram_stories = [get_ngrams(s,ngrams) for s in stories]</pre>

  <p class="body"><a class="calibre8" id="pgfId-1134779"></a>We will fit the <span class="fm-code-in-text">Tokenizer</span> on the two-grams of the training stories:</p>
  <pre class="programlisting">tokenizer.fit_on_texts(train_ngram_stories)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1134783"></a>Now convert all training, validation, and testing stories to sequences of IDs, using the already fitted <span class="fm-code-in-text">Tokenizer</span> trained using two-grams from the training data:</p>
  <pre class="programlisting">train_data_seq = tokenizer.texts_to_sequences(train_ngram_stories)
 
val_ngram_stories = [get_ngrams(s,ngrams) for s in val_stories]
val_data_seq = tokenizer.texts_to_sequences(val_ngram_stories)
 
test_ngram_stories = [get_ngrams(s,ngrams) for s in test_stories]
test_data_seq = tokenizer.texts_to_sequences(test_ngram_stories)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1134792"></a>Let’s analyze how the data looks after converting to word IDs by printing some test data. Specifically, we’ll print the first three story strings (<span class="fm-code-in-text">test_stories</span>), n-gram strings (<span class="fm-code-in-text">test_ngram_stories</span>), and word ID sequences<a class="calibre8" id="marker-1134794"></a><a class="calibre8" id="marker-1134795"></a><a class="calibre8" id="marker-1134796"></a> (<span class="fm-code-in-text">test_data_seq</span>):</p>
  <pre class="programlisting">Original: the yellow fairy book the cat and the mouse in par
n-grams: ['th', 'e ', 'ye', 'll', 'ow', ' f', 'ai', 'ry', ' b', 'oo', 'k ', 
<span class="fm-code-continuation-arrow">➥</span> 'th', 'e ', 'ca', 't ', 'an', 'd ', 'th', 'e ', 'mo', 'us', 'e ', 'in', 
<span class="fm-code-continuation-arrow">➥</span> ' p', 'ar']
Word ID sequence: [6, 2, 215, 54, 84, 35, 95, 146, 26, 97, 123, 6, 2, 128, 
<span class="fm-code-continuation-arrow">➥</span> 8, 15, 5, 6, 2, 147, 114, 2, 17, 65, 52]
 
 
Original: chapter i. down the rabbit-hole alice was beginnin
n-grams: ['ch', 'ap', 'te', 'r ', 'i.', ' d', 'ow', 'n ', 'th', 'e ', 'ra', 
<span class="fm-code-continuation-arrow">➥</span> 'bb', 'it', '-h', 'ol', 'e ', 'al', 'ic', 'e ', 'wa', 's ', 'be', 'gi', 
<span class="fm-code-continuation-arrow">➥</span> 'nn', 'in']
Word ID sequence: [93, 207, 57, 19, 545, 47, 84, 18, 6, 2, 126, 344, 
<span class="fm-code-continuation-arrow">➥</span> 38, 400, 136, 2, 70, 142, 2, 66, 9, 71, 218, 251, 17]
 
 
Original: a patent medicine testimonial `` you might as well
n-grams: ['a ', 'pa', 'te', 'nt', ' m', 'ed', 'ic', 'in', 'e ', 'te', 'st', 
<span class="fm-code-continuation-arrow">➥</span> 'im', 'on', 'ia', 'l ', '``', ' y', 'ou', ' m', 'ig', 'ht', ' a', 's ', 
<span class="fm-code-continuation-arrow">➥</span> 'we', 'll']
Word ID sequence: [60, 179, 57, 78, 33, 31, 142, 17, 2, 57, 50, 125, 43, 
<span class="fm-code-continuation-arrow">➥</span> 266, 56, 122, 92, 29, 33, 152, 149, 7, 9, 103, 54]</pre>

  <h3 class="fm-head1" id="sigil_toc_id_140"><a id="pgfId-1134811"></a>10.1.5 Defining a tf.data pipeline</h3>

  <p class="body"><a class="calibre8" id="pgfId-1134815"></a>Now<a class="calibre8" id="marker-1134813"></a><a class="calibre8" id="marker-1134814"></a> the preprocessing has happened, and we have text converted to word ID sequences. We can define the <span class="fm-code-in-text">tf.data</span> pipeline<a class="calibre8" id="marker-1134816"></a> that will deliver the final processed data ready to be consumed by the model. The main steps involved in the process are illustrated in figure 10.1.</p>

  <p class="fm-figure"><img alt="10-01" class="calibre10" src="../../OEBPS/Images/10-01.png" width="969" height="1406"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1151459"></a>Figure 10.1 The high-level steps of the data pipeline we will be developing. First the individual stories are broken down to fixed-length sequences (or windows). Then, from the windowed sequences, inputs and targets are generated.</p>

  <p class="body"><a class="calibre8" id="pgfId-1134824"></a>As we did before, let’s define the word ID corpus as a <span class="fm-code-in-text">tf.RaggedTensor</span> object<a class="calibre8" id="marker-1134823"></a>, as the sentences in the corpus have variable sequence lengths:</p>
  <pre class="programlisting">text_ds = tf.data.Dataset.from_tensor_slices(tf.ragged.constant(data_seq))     </pre>

  <p class="body"><a class="calibre8" id="pgfId-1134828"></a>Remember that a ragged tensor is a tensor that has variable-sized dimensions. Then we will shuffle the data so that stories come at a random order if <span class="fm-code-in-text">shuffle</span> is set to <span class="fm-code-in-text">True</span> (e.g., training time):</p>
  <pre class="programlisting">if shuffle:
    text_ds = text_ds.shuffle(buffer_size=len(data_seq)//2)    </pre>

  <p class="body"><a class="calibre8" id="pgfId-1134833"></a>Now comes the tricky part. In this section, we will see how to generate fixed-sized windowed sequences from an arbitrarily long text. We will do that through a series of steps. This section can be slightly complex compared to the rest of the pipeline. This is because there will be interim steps that result in data sets nested up to three levels. Let’s try to go through this in as much detail as possible.</p>

  <p class="body"><a class="calibre8" id="pgfId-1134834"></a>First, let’s make it clear what we need to achieve. The first steps we need to perform are to do the following for each individual story S:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1134836"></a>Create a <span class="fm-code-in-text">tf.data.Dataset()</span> object<a class="calibre8" id="marker-1134835"></a> containing the word IDs of the story S as its items.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1134837"></a>Call the <span class="fm-code-in-text">tf.data.Dataset.window()</span> function to window word IDs with a <span class="fm-code-in-text">n_seq+1</span>-sized window and a predefined shift. The <span class="fm-code-in-text">window()</span> function<a class="calibre8" id="marker-1134838"></a> returns a <span class="fm-code-in-text">WindowDataset</span> object<a class="calibre8" id="marker-1134839"></a> for each story S.</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1134840"></a>After this, you will have a three-level nested data structure having the specification</p>
  <pre class="programlisting">tf.data.Dataset(              # &lt;- From the original dataset
  tf.data.Dataset(    # &lt;- From inner dataset containing word IDs of story S only
    tf.data.WindowDataset(...)  # &lt;- Dataset returned by the window() function
  )
)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1134847"></a>We will need to flatten this data set and untangle the nesting in our data set to end up with a flat <span class="fm-code-in-text">tf.data.Dataset</span>. You can remove these inner nestings using the <span class="fm-code-in-text">tf.data.Dataset.flat_map()</span> function<a class="calibre8" id="marker-1134848"></a>. We will soon see how to use the <span class="fm-code-in-text">flat_ map()</span> function<a class="calibre8" id="marker-1134849"></a><a class="calibre8" id="marker-1134850"></a>. To be specific, we have to use two <span class="fm-code-in-text">flat_map</span> calls to remove two levels of nesting so that we end up with only the flat original data set containing simple tensors as elements. In TensorFlow, this process can be achieved using the following line of code:</p>
  <pre class="programlisting">    text_ds = text_ds.flat_map(
        lambda x: tf.data.Dataset.from_tensor_slices(
            x
        ).window(
            n_seq+1,shift=shift
        ).flat_map(
            lambda window: window.batch(n_seq+1, drop_remainder=True)
        )
    )
    )</pre>

  <p class="body"><a class="calibre8" id="pgfId-1134865"></a>This is what we are doing here: first, we create a <span class="fm-code-in-text">tf.data.Dataset</span> object<a class="calibre8" id="marker-1134864"></a> from a single story (<span class="fm-code-in-text">x</span>) and then call the <span class="fm-code-in-text">tf.data.Dataset.window()</span> function<a class="calibre8" id="marker-1134866"></a> on that to create the windowed sequences. This windowed sequence contains windows, where each window is a sequence with <span class="fm-code-in-text">n_seq+1</span> consecutive elements in the story <span class="fm-code-in-text">x</span>.</p>

  <p class="body"><a class="calibre8" id="pgfId-1134868"></a>Then we call the <span class="fm-code-in-text">tf.data.Dataset.flat_map()</span> function<a class="calibre8" id="marker-1134867"></a>, where, for each <span class="fm-code-in-text">window</span> element<a class="calibre8" id="marker-1134869"></a>, we get all the individual IDs as a single batch. In other words, a single window element produces a single batch with all the elements in that window. Make sure you use <span class="fm-code-in-text">drop_remainder=True</span>; otherwise, the data set will return smaller subwindows within that window that contain fewer elements. Using <span class="fm-code-in-text">tf.data.Dataset.flat_map()</span> instead of map makes sure that the inner-most nesting will be removed. This whole thing is called with a <span class="fm-code-in-text">tf.data.Dataset.flat_map()</span> call, which gets rid of the next level of nesting immediately following the innermost nesting we removed. It is quite a complex process for a single liner. I suggest you go through it again if you have not fully understood the process.</p>

  <p class="body"><a class="calibre8" id="pgfId-1134870"></a>You might notice that we are defining the window size as <span class="fm-code-in-text">n_seq+1</span> and not <span class="fm-code-in-text">n_seq</span>. The reason for this will become evident later, but using <span class="fm-code-in-text">n_seq+1</span> makes our life so much easier when we have to generate inputs and targets from the windowed sequences.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1134871"></a>Difference between map and flat_map in tf.data.Dataset</p>

    <p class="fm-sidebar-text"><a id="pgfId-1134873"></a>Both functions <span class="fm-code-in-text1">tf.data.Dataset.map</span><a id="marker-1137340"></a><span class="fm-code-in-text1">()</span> and <span class="fm-code-in-text1">tf.data.Dataset.flat_map</span><a id="marker-1137341"></a><span class="fm-code-in-text1">()</span> achieve the same result but with different data set specifications. For example, assume the data set</p>
    <pre class="programlisting">ds = tf.data.Dataset.from_tensor_slices([[1,2,3], [5,6,7]])</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1134876"></a>Using the <span class="fm-code-in-text1">tf.data.Dataset.map()</span> function to square the elements as</p>
    <pre class="programlisting">ds = ds.map(lambda x: x**2) </pre>

    <p class="fm-sidebar-text"><a id="pgfId-1134878"></a>will result in a data set that has the elements</p>
    <pre class="programlisting">[[1, 4, 9], [25, 36, 49]]</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1134880"></a>As you can see, the result has the same structure as the original tensor. Using the <span class="fm-code-in-text1">tf.data.Dataset.flat_map()</span> function<a id="marker-1140908"></a><a id="marker-1140909"></a> to square the elements as</p>
    <pre class="programlisting">ds = ds.flat_map(lambda x: x**2)</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1134884"></a>will result in a data set that has</p>
    <pre class="programlisting">[1,4,9,25,36,49]</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1134886"></a>As you can see, that inner-most nesting has been flattened to produce a flat sequence of elements.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1134888"></a>The hardest part of our data pipeline is done. By now, you have a flat data set, and each item is <span class="fm-code-in-text">n_seq+1</span> consecutive word IDs belonging to a single story. Next, we will perform a window-level shuffle on the data. This is different from the first shuffle we did as that was on the story level (i.e., not the window level):</p>
  <pre class="programlisting">    # Shuffle the data (shuffle the order of n_seq+1 long sequences)
    if shuffle:
        text_ds = text_ds.shuffle(buffer_size=10*batch_size)    </pre>

  <p class="body"><a class="calibre8" id="pgfId-1134894"></a>We’re then going to batch the data so that we will get a batch of windows every time we iterate the data set:</p>
  <pre class="programlisting">    # Batch the data
    text_ds = text_ds.batch(batch_size)    </pre>

  <p class="body"><a class="calibre8" id="pgfId-1134898"></a>Finally, the reason we chose the sequence length as <span class="fm-code-in-text">n_seq+1</span> will become clearer. Now we will split the sequences into two versions, where one sequence will be the other shifted to the right by 1. In other words, the targets to this model will be inputs shifted to the right by 1. For example, if the sequence is <span class="fm-code-in-text">[0,1,2,3,4]</span>, then the two resulting sequences will be <span class="fm-code-in-text">[0,1,2,3]</span> and <span class="fm-code-in-text">[1,2,3,4]</span>. Furthermore, we will use prefetching to speed up the data ingestion:</p>
  <pre class="programlisting">    # Split each sequence to an input and a target
    text_ds = tf.data.Dataset.zip(
        text_ds.map(lambda x: (x[:,:-1], x[:, 1:]))
    ).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)    </pre>

  <p class="body"><a class="calibre8" id="pgfId-1134904"></a>Finally, the full code can be encapsulated in a function as in the next listing.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1134906"></a>Listing 10.3 The <span class="fm-code-in-listingcaption">tf.data</span> pipeline from free text sequences</p>
  <pre class="programlisting">def get_tf_pipeline(data_seq, n_seq, batch_size=64, shift=1, shuffle=True):
    """ Define a tf.data pipeline that takes a set of sequences of text and 
    convert them to fixed length sequences for the model """
    
    text_ds = tf.data.Dataset.from_tensor_slices(tf.ragged.constant(data_seq))<span class="fm-combinumeral">❶</span>
    
    if shuffle:
        text_ds = text_ds.shuffle(buffer_size=len(data_seq)//2)               <span class="fm-combinumeral">❷</span>
    
    text_ds = text_ds.flat_map(                                               <span class="fm-combinumeral">❸</span>
        lambda x: tf.data.Dataset.from_tensor_slices(
            x
        ).window(
            n_seq+1, shift=shift
        ).flat_map(
            lambda window: window.batch(n_seq+1, drop_remainder=True)
        )
    ) 
    
    if shuffle:
        text_ds = text_ds.shuffle(buffer_size=10*batch_size)                  <span class="fm-combinumeral">❹</span>
    
    text_ds = text_ds.batch(batch_size)                                       <span class="fm-combinumeral">❺</span>
    
    text_ds = tf.data.Dataset.zip(
        text_ds.map(lambda x: (x[:,:-1], x[:, 1:]))
    ).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)                     <span class="fm-combinumeral">❻</span>
    
    return text_ds   </pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1150076"></a><span class="fm-combinumeral">❶</span> Define a tf.dataset from a ragged tensor created from data_seq.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1150104"></a><span class="fm-combinumeral">❷</span> If shuffle is set, shuffle the data (shuffle story order).</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1150124"></a><span class="fm-combinumeral">❸</span> Here we create windows from longer sequences, given a window size and a shift, and then use a series of flat_map operations to remove the nesting that’s created in the process.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1150141"></a><span class="fm-combinumeral">❹</span> Shuffle the data (shuffle the order of the windows generated).</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1150158"></a><span class="fm-combinumeral">❺</span> Batch the data.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1150175"></a><span class="fm-combinumeral">❻</span> Split each sequence into an input and a target and enable pre-fetching.</p>

  <p class="body"><a class="calibre8" id="pgfId-1134942"></a>All this hard work wouldn’t mean as much as it should unless we looked at the generated data</p>
  <pre class="programlisting">ds = get_tf_pipeline(train_data_seq, 5, batch_size=6)
 
for a in ds.take(1):
    print(a)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1134948"></a>which will show you</p>
  <pre class="programlisting">(
&lt;tf.Tensor: shape=(6, 5), dtype=int32, numpy=
array([[161,  12,  69, 396,  17],
       [  2,  72,  77,  84,  24],
       [ 87,   6,   2,  72,  77],
       [276, 484,  57,   5,  15],
       [ 75, 150,   3,   4,  11],
       [ 11,  73, 211,  35, 141]])&gt;, 
&lt;tf.Tensor: shape=(6, 5), dtype=int32, numpy=
array([[ 12,  69, 396,  17,  44],
       [ 72,  77,  84,  24,  51],
       [  6,   2,  72,  77,  84],
       [484,  57,   5,  15,  67],
       [150,   3,   4,  11,  73],
       [ 73, 211,  35, 141,  98]])&gt;
)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1134965"></a>Great, you can see that we get a tuple of tensors as a single batch: inputs and targets. Moreover, you can validate the correctness of the results, as we can clearly see that the targets are the input shifted to the right by 1. One last thing: we will save the same hyperparameters to the disk. Particularly</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1134966"></a>n in n-grams</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1134967"></a>Vocabulary size</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1134968"></a>Sequence length</p>
    </li>
  </ul>
  <pre class="programlisting">print("n_grams uses n={}".format(ngrams))
print("Vocabulary size: {}".format(n_vocab))
 
n_seq=100
print("Sequence length for model: {}".format(n_seq))
 
with open(os.path.join('models', 'text_hyperparams.pkl'), 'wb') as f:
    pickle.dump({'n_vocab': n_vocab, 'ngrams':ngrams, 'n_seq': n_seq}, f)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1134977"></a>Here, we are defining the sequence length <span class="fm-code-in-text">n_seq=100</span>; this is the number of bigrams we will have in a single input/label sequence.</p>

  <p class="body"><a class="calibre8" id="pgfId-1134978"></a>In this section, we learned about the data used for language modeling and defined a capable <span class="fm-code-in-text">tf.data</span> pipeline that can convert sequences of text into input label sequences that can be used to train the model directly. Next, we will define a machine learning model to generate text with.</p>

  <p class="fm-head2"><a id="pgfId-1134979"></a>Exercise 1</p>

  <p class="body"><a class="calibre8" id="pgfId-1134980"></a>You are given a sequence x that has values <span class="fm-code-in-text">[1,2,3,4,5,6,7,8,9,0]</span>. You have been asked to write a <span class="fm-code-in-text">tf.data</span> pipeline<a class="calibre8" id="marker-1134981"></a><a class="calibre8" id="marker-1134982"></a> that generates an input and target tuple, and the target is the input shifted two elements to the right (i.e., the target of the input 1 is 3). You have to do this so that a single input/target has three elements and no overlap between the consecutive input sequences. For the previous sequence it should generate<a class="calibre8" id="marker-1134985"></a><a class="calibre8" id="marker-1134986"></a><a class="calibre8" id="marker-1134987"></a><a class="calibre8" id="marker-1134988"></a><a class="calibre8" id="marker-1134989"></a> <span class="fm-code-in-text">[([1,2,3], [3,4,5]), ([6,7,8], [8,9,0])]</span>.</p>

  <h2 class="fm-head" id="sigil_toc_id_141"><a id="pgfId-1134990"></a>10.2 GRUs in Wonderland: Generating text with deep learning</h2>

  <p class="body"><a class="calibre8" id="pgfId-1134998"></a>Now<a class="calibre8" id="marker-1134991"></a><a class="calibre8" id="marker-1134992"></a><a class="calibre8" id="marker-1134993"></a><a class="calibre8" id="marker-1134994"></a><a class="calibre8" id="marker-1134995"></a><a class="calibre8" id="marker-1134996"></a><a class="calibre8" id="marker-1134997"></a> we’re on to the rewarding part: implementing a cool machine learning model. In the last chapter, we talked about deep sequential models. Given the sequential nature of the data, you probably have guessed that we’re going to use one of the deep sequential models like LSTMs. In this chapter, we will use a slightly different model called <i class="fm-italics">gated recurrent units</i> (GRUs). The principles that drive the computations in the model remain the same as LSTMs. To maintain the clarity of our discussion, it’s worthwhile to remind ourselves how LSTM models work.</p>

  <p class="body"><a class="calibre8" id="pgfId-1134999"></a>LSTMs are a family of deep neural networks that are specifically designed to process sequential data. They process a sequence of inputs, one input at a time. An LSTM cell goes from one input to the next while producing outputs (or states) at each time step (figure 10.2). Additionally, to produce the outputs of a given time step, LSTMs uses previous outputs (or states) it produced. This property is very important for LSTMs and gives them the ability to memorize things over time.</p>

  <p class="fm-figure"><img alt="10-02" class="calibre10" src="../../OEBPS/Images/10-02.png" width="753" height="278"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1151614"></a>Figure 10.2 Overview of the LSTM model and how it processes a sequence of inputs spread over time</p>

  <p class="body"><a class="calibre8" id="pgfId-1135006"></a>Let’s summarize what we learned about LSTMs in the previous chapter, as that will help us to compare LSTMs and GRUs. An LSTM has two states known as the cell state and the output state. The cell state is responsible for maintaining long-term memory, whereas the output state can be thought of as the short-term memory. The outputs and interim results within an LSTM cell are controlled by three gates:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1135007"></a><i class="fm-italics">Input gate</i>—Controls the amount of the current input that will contribute to the final output at a given time step</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1135008"></a><i class="fm-italics">Forget gate</i>—Controls how much of the previous cell state affects the current cell state computation</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1135012"></a><i class="fm-italics">Output gate</i><a class="calibre8" id="marker-1135009"></a><a class="calibre8" id="marker-1135010"></a><a class="calibre8" id="marker-1135011"></a>—Controls how much the current cell state contributes to the final output produced by the LSTM model</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1135013"></a>The GRU model was introduced in the paper “Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation” by Cho et al. (<span class="fm-hyperlink"><a class="url" href="https://arxiv.org/pdf/1406.1078v3.pdf">https://arxiv.org/pdf/1406.1078v3.pdf</a></span>). The GRU model can be considered a simplification of the LSTM model while preserving on-par performance. The GRU cell has two gates:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1135015"></a><i class="fm-italics">Update gate</i> (<i class="fm-timesitalic1">z</i><sub class="fm-subscript">t</sub>)—Controls how much of the previous hidden state is carried to the current hidden state</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1135017"></a><i class="fm-italics">Reset gate</i><a class="calibre8" id="marker-1135016"></a> (<i class="fm-timesitalic1">r</i><sub class="fm-subscript">t</sub>)—Controls how much of the hidden state is reset with the new input</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1135018"></a>Unlike the LSTM cell, a GRU cell has only one state vector. In summary, there are two major changes in a GRU compared to an LSTM model:</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1135020"></a>Both the input gate and forget gate are combined into a single gate called the update gate<a class="calibre8" id="marker-1135019"></a> (<i class="fm-timesitalic1">z</i><sub class="fm-subscript">t</sub>). The input gate is computed as (1-<i class="fm-timesitalic1">z</i><sub class="fm-subscript">t</sub>), whereas the forget gate stays <i class="fm-timesitalic1">z</i><sub class="fm-subscript">t</sub>.</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1135021"></a>There’s only one state (<i class="fm-timesitalic1">h</i><sub class="fm-subscript">t</sub>) in contrast to the two states found in an LSMT cell (i.e., cell state and the output state).</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1135022"></a>Figure 10.3 depicts the various components of the GRU cell. Here is the full list of equations that make a GRU spin:</p>

  <p class="fm-equation"><a id="pgfId-1141610"></a><i class="fm-italics">r</i><sub class="fm-subscript">t</sub> = <span class="fm-symbol1">σ</span>(<i class="fm-italics">W</i><sub class="fm-subscript">rh</sub><i class="fm-italics">h</i><sub class="fm-subscript">t-1</sub> + <i class="fm-italics">W</i><sub class="fm-subscript">rx</sub><i class="fm-italics">x</i><sub class="fm-subscript">t</sub> + <i class="fm-italics">b</i><sub class="fm-subscript">r</sub>)</p>

  <p class="fm-equation"><a id="pgfId-1141748"></a><i class="fm-italics">z</i><sub class="fm-subscript">t</sub> = <span class="fm-symbol1">σ</span>(<i class="fm-italics">W</i><sub class="fm-subscript">zh</sub><i class="fm-italics">h</i><sub class="fm-subscript">t-1</sub> + <i class="fm-italics">W</i><sub class="fm-subscript">zx</sub><i class="fm-italics">x</i><sub class="fm-subscript">t</sub> + <i class="fm-italics">b</i><sub class="fm-subscript">z</sub>)</p>

  <p class="fm-equation"><a id="pgfId-1141772"></a><i class="fm-italics">h̃</i><sub class="fm-subscript">t</sub> = tanh(<i class="fm-italics">W</i><sub class="fm-subscript">h</sub><i class="fm-italics">(rh</i><sub class="fm-subscript">t-1</sub>) + <i class="fm-italics">W</i><sub class="fm-subscript">x</sub><i class="fm-italics">x</i><sub class="fm-subscript">t</sub> + <i class="fm-italics">b</i>)</p>

  <p class="fm-equation"><a id="pgfId-1141864"></a><i class="fm-italics">h</i><sub class="fm-subscript">t</sub> = (<i class="fm-italics">z</i><sub class="fm-subscript">th</sub><i class="fm-italics">h</i><sub class="fm-subscript">t-1</sub> + (1 - <i class="fm-italics">z</i><sub class="fm-subscript">t</sub> )<i class="fm-italics">h̃</i><sub class="fm-subscript">t</sub></p>

  <p class="fm-figure"><img alt="10-03" class="calibre10" src="../../OEBPS/Images/10-03.png" width="847" height="489"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1151648"></a>Figure 10.3 Overview of the computations transpiring in a GRU cell</p>

  <p class="body"><a class="calibre8" id="pgfId-1135032"></a>This discussion was immensely helpful to not only understand the GRU model but also to learn how it’s different from an LSTM cell. You can define a GRU cell in TensorFlow as follows:</p>
  <pre class="programlisting">tf.keras.layers.GRU(units=1024, return_state=False, return_sequences=True)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1135035"></a>The parameter units, <span class="fm-code-in-text">return_state</span> and <span class="fm-code-in-text">return_sequences</span><a class="calibre8" id="marker-1135034"></a>, have the same meaning as they do in the context of an LSTM cell. However, note that the GRU cell has only one state; therefore, if <span class="fm-code-in-text">return_state=true</span>, the same state is duplicated to mimic the output state and the cell state of the LSTM layer. Figure 10.4 shows what these parameters do for a GRU layer.</p>

  <p class="fm-figure"><img alt="10-04" class="calibre10" src="../../OEBPS/Images/10-04.png" width="1106" height="1150"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1151683"></a>Figure 10.4 Changes in results depending on the <span class="fm-code-in-figurecaption">return_state</span><a id="marker-1141561"></a><a id="marker-1151682"></a> and <span class="fm-code-in-figurecaption">return_sequences</span> arguments of the GRU cell</p>

  <p class="body"><a class="calibre8" id="pgfId-1135046"></a>We now know everything we need to define the final model (listing 10.4). Our final model will consist of</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1135047"></a>An embedding layer</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1135048"></a>A GRU layer (1,024 units) that returns all the final state vectors as a tensor that has shape [batch size, sequence length, number of units]</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1135049"></a>A <span class="fm-code-in-text">Dense</span> layer with 512 units and ReLU activation</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1135052"></a>A <span class="fm-code-in-text">Dense</span> layer<a class="calibre8" id="marker-1135050"></a><a class="calibre8" id="marker-1135051"></a> with <span class="fm-code-in-text">n_vocab</span> units and <span class="fm-code-in-text">softmax</span> activation</p>
    </li>
  </ul>

  <p class="fm-code-listing-caption"><a id="pgfId-1135054"></a>Listing 10.4 Implementing the language model</p>
  <pre class="programlisting">model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(
        input_dim=n_vocab+1, output_dim=512,input_shape=(None,)           <span class="fm-combinumeral">❶</span>
    ),
    
    tf.keras.layers.GRU(1024, return_state=False, return_sequences=True), <span class="fm-combinumeral">❷</span>
    
    tf.keras.layers.Dense(512, activation='relu'),                        <span class="fm-combinumeral">❸</span>
    
    tf.keras.layers.Dense(n_vocab, name='final_out'),                     <span class="fm-combinumeral">❹</span>
    tf.keras.layers.Activation(activation='softmax')                      <span class="fm-combinumeral">❹</span>
])</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1149789"></a><span class="fm-combinumeral">❶</span> Define an embedding layer to learn word vectors of the bigrams.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1149810"></a><span class="fm-combinumeral">❷</span> Define an LSTM layer.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1149827"></a><span class="fm-combinumeral">❸</span> Define a Dense layer.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1149844"></a><span class="fm-combinumeral">❹</span> Define a final Dense layer and softmax activation.</p>

  <p class="body"><a class="calibre8" id="pgfId-1135072"></a>You will notice that the <span class="fm-code-in-text">Dense</span> layer<a class="calibre8" id="marker-1135071"></a> after the GRU receives a three-dimensional tensor (as opposed to the typical two-dimensional tensor passed to <span class="fm-code-in-text">Dense</span> layers). <span class="fm-code-in-text">Dense</span> layers<a class="calibre8" id="marker-1135073"></a> are smart enough to work with both two-dimensional and three-dimensional inputs. If the input is three-dimensional (like in our case), then a <span class="fm-code-in-text">Dense</span> layer<a class="calibre8" id="marker-1135074"></a> that accepts a [batch size, number of units] tensor is passed through all the steps in the sequence to generate the <span class="fm-code-in-text">Dense</span> layer’s<a class="calibre8" id="marker-1135075"></a> output. Also note how we are separating the softmax activation from the <span class="fm-code-in-text">Dense</span> layer<a class="calibre8" id="marker-1135076"></a>. This is actually an equivalent of</p>
  <pre class="programlisting">.Dense(n_vocab, activation=’softmax’, name=’final_out’)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1135080"></a>We will not prolong the conversation by reiterating what’s shown in listing 10.4, as it is self-explanatory.</p>

  <p class="fm-head2"><a id="pgfId-1135081"></a>Exercise 2</p>

  <p class="body"><a class="calibre8" id="pgfId-1135082"></a>You have been given the model as follows and have been asked to add another GRU layer with 512 units that returns all the state outputs, on top of the existing GRU layer. What changes would you make to the following code?</p>
  <pre class="programlisting">model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(
        input_dim=n_vocab+1, output_dim=512,input_shape=(None,)   
    ),
    tf.keras.layers.GRU(1024, return_state=False, return_sequences=True),   
    tf.keras.layers.Dense(n_vocab, activation=’softmax’, name='final_out'), ])</pre>

  <p class="body"><a class="calibre8" id="pgfId-1135091"></a>In this section, we learned about gated recurrent units (GRUs) and how they compare to LSTMs. Finally, we defined a language model that can be trained on the data we downloaded and processed earlier. In the next section, we are going to learn about evaluation metrics for assessing the quality of generated text.</p>

  <h2 class="fm-head" id="sigil_toc_id_142"><a id="pgfId-1135099"></a>10.3 Measuring the quality of the generated<a id="marker-1135092"></a><a id="marker-1135093"></a><a id="marker-1135094"></a><a id="marker-1135095"></a><a id="marker-1135096"></a><a id="marker-1135097"></a><a id="marker-1135098"></a> text</h2>

  <p class="body"><a class="calibre8" id="pgfId-1135100"></a>Performance monitoring has been an integral part of our modeling journey in every chapter. It is no different here. Performance<a class="calibre8" id="marker-1135101"></a><a class="calibre8" id="marker-1135102"></a><a class="calibre8" id="marker-1135103"></a><a class="calibre8" id="marker-1135104"></a> monitoring is an important aspect of our language model, and we need to find metrics that are suited for language models. Naturally, given that this is a classification task, you might be thinking, “Wouldn’t accuracy be a good choice for a metric?” Well, not quite in this task.</p>

  <p class="body"><a class="calibre8" id="pgfId-1135105"></a>For example, if the language model is given the sentence “I like my pet dog,” then when asked to predict the missing word given “I like my pet ____,” the model might predict “cat,” and the accuracy would be zero. But that’s not correct; “cat” makes as much sense as “dog” in this example. Is there a better solution here?</p>

  <p class="body"><a class="calibre8" id="pgfId-1135107"></a>Enter perplexity! Intuitively, <i class="fm-italics">perplexity</i><a class="calibre8" id="marker-1135106"></a> measures how “surprised” the model was to see a target given the previous word sequence. Before understanding perplexity, you need to understand what “entropy” means.</p>

  <p class="body"><a class="calibre8" id="pgfId-1135108"></a><i class="fm-italics">Entropy</i> is a term coined by the famous Claude Shannon, who is considered the father of information theory. Entropy measures the surprise/uncertainty/randomness of an event. The event can have some outcome generated by an underlying probability distribution over all the possible outcomes. For example, if you consider tossing a coin (with a probability p of landing on heads) an event, if p = 0.5, you will have the maximum entropy, as that is the most uncertain scenario for a coin toss. If p = 1 or p = 0, then you have the minimum entropy, as you know what the outcome is before tossing the coin.</p>

  <p class="body"><a class="calibre8" id="pgfId-1135110"></a>The original interpretation of entropy<a class="calibre8" id="marker-1135109"></a> is the expected value of the number of bits required to send a signal or a message informing of an event. A bit is a unit of memory, which can be 1 or 0. For example, you are the commander of an army that’s at war with countries A and B. Now you have four possibilities: A and B both surrender, A wins and B loses, A loses and B wins, and both A and B win. If all these events are equally likely to happen, you need two bits to send a message, where each bit represents whether that country won. Entropy of a random variable X is quantified by the equation</p>

  <p class="fm-equation"><img alt="10_04a" class="calibre10" src="../../OEBPS/Images/10_04a.png" width="294" height="70"/><br class="calibre2"/>
  <a id="pgfId-1139342"></a></p>

  <p class="body"><a class="calibre8" id="pgfId-1135115"></a>where <i class="fm-timesitalic">x</i> is an outcome of <i class="fm-timesitalic">X</i>. Believe it or not, we have been using this equation without knowing it every time we used the categorical cross-entropy loss. The crux of the categorical cross entropy is this equation. Coming back to the perplexity measure, perplexity is simply</p>

  <p class="fm-equation"><a id="pgfId-1135116"></a><i class="fm-italics">Perplexity =</i> 2<sup class="fm-superscript">H(X)</sup></p>

  <p class="body"><a class="calibre8" id="pgfId-1135117"></a>Since perplexity is a function of entropy, it measures how surprised/uncertain the model was to see the target word, given the sequence of previous words. Perplexity can also be thought of as the number of all possible combinations of a given signal. For example, assume you are sending a message with two bits, where all the events are equally likely; then the entropy = 2, which means perplexity = 2^2 = 4. In other words, there are four combinations two bits can be in: 00, 01, 10, and 11.</p>

  <p class="body"><a class="calibre8" id="pgfId-1135118"></a>In more of a modeling perspective, you can think of perplexity as the number of different targets that the model thinks fit the blank as the next word, given a sequence of previous words. The smaller this number, the better, as that means the model is trying to find a word from a smaller subset, indicating signs of language understanding.</p>

  <p class="body"><a class="calibre8" id="pgfId-1135119"></a>To implement perplexity, we will define a custom metric. The computation is very simple. We compute the categorical cross entropy and then exponentiate it to get the perplexity. The categorical cross entropy is simply an extension of the entropy to measure entropy in classification problems with more than two classes. For an input example (<i class="fm-timesitalic">x</i><sub class="fm-subscript">i</sub>,<i class="fm-timesitalic">y</i><sub class="fm-subscript">i</sub>), it is typically defined as</p>

  <p class="fm-equation"><img alt="10_04b" class="calibre10" src="../../OEBPS/Images/10_04b.png" width="531" height="74"/><br class="calibre2"/>
  <a id="pgfId-1139363"></a></p>

  <p class="body"><a class="calibre8" id="pgfId-1135124"></a>where <i class="fm-timesitalic">y<sub class="fm-subscript">i</sub></i> denotes the one-hot encoded vector representing the true class the example belongs to and <i class="fm-timesitalic">ŷ</i><sub class="fm-subscript">i</sub> is the predicted class probability vector of <i class="fm-timesitalic">C</i> elements, with <i class="fm-timesitalic">ŷ</i><sub class="fm-subscript">i,c</sub> denoting the probability of the example belonging to class <i class="fm-timesitalic">c</i>. Note that in practice, an exponential (natural) base is used instead of base, 2 as the computations are faster. The following listing delineates the process.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1135132"></a>Listing 10.5 Implementation of the perplexity metric</p>
  <pre class="programlisting">import tensorflow.keras.backend as K
 
class PerplexityMetric(tf.keras.metrics.Mean):
    
    def __init__(self, name='perplexity', **kwargs):
      super().__init__(name=name, **kwargs)
      self.cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(
         from_logits=False, reduction='none'
       )
 
    def _calculate_perplexity(self, real, pred):   <span class="fm-combinumeral">❶</span>
      
      loss_ = self.cross_entropy(real, pred)       <span class="fm-combinumeral">❷</span>
      
      mean_loss = K.mean(loss_, axis=-1)           <span class="fm-combinumeral">❸</span>
      perplexity = K.exp(mean_loss)                <span class="fm-combinumeral">❹</span>
    
      return perplexity 
 
    def update_state(self, y_true, y_pred, sample_weight=None):            
      perplexity = self._calculate_perplexity(y_true, y_pred)
      super().update_state(perplexity)</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1149569"></a><span class="fm-combinumeral">❶</span> Define a function to compute perplexity given real and predicted targets.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1149590"></a><span class="fm-combinumeral">❷</span> Compute the categorical cross-entropy loss.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1149607"></a><span class="fm-combinumeral">❸</span> Compute the mean of the loss.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1149624"></a><span class="fm-combinumeral">❹</span> Compute the exponential of the mean loss (perplexity).</p>

  <p class="body"><a class="calibre8" id="pgfId-1135161"></a>What we’re doing is very simple. First, we subclass the <span class="fm-code-in-text">tf.keras.metrics.Mean</span> class<a class="calibre8" id="marker-1135160"></a>. The <span class="fm-code-in-text">tf.keras.metrics.Mean</span> class<a class="calibre8" id="marker-1135162"></a> will keep track of the mean value of any outputted metric passed into its <span class="fm-code-in-text">update_state()</span> function<a class="calibre8" id="marker-1135163"></a>. In other words, when we subclass the <span class="fm-code-in-text">tf.keras.metrics.Mean</span> class, we don’t specifically need to manually compute the mean of the accumulated perplexity metric as the training continues. It will be automatically done by that parent class<a class="calibre8" id="marker-1135164"></a>. We will define the loss function we will use in the <span class="fm-code-in-text">self.cross_entropy</span> variable. Then we write the function <span class="fm-code-in-text">_calculate_perplexity()</span>, which takes the real targets and the predictions from the model. We compute the sample-wise loss and then compute the mean. Finally, to get the perplexity, we exponentiate the mean loss. With that, we can compile the model:</p>
  <pre class="programlisting">model.compile(
    loss='sparse_categorical_crossentropy', 
    optimizer='adam', 
    metrics=['accuracy', PerplexityMetric()]
)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1135173"></a>In this section, we learned about the performance metrics, such as entropy and perplexity, used to evaluate language models. Furthermore, we implemented a custom perplexity metric that is used to compile the final model. Next, we’ll train our model on the data we have prepared and evaluate the quality of generated text.</p>

  <p class="fm-head2"><a id="pgfId-1135174"></a>Exercise 3</p>

  <p class="body"><a class="calibre8" id="pgfId-1135175"></a>Imagine a classification problem that has three outputs. There are two scenarios with different predictions:</p>

  <p class="body"><a class="calibre8" id="pgfId-1135176"></a>Scenario A: Labels [0, 2, 1]</p>

  <p class="body"><a class="calibre8" id="pgfId-1135177"></a>Predictions: [[0.6, 0.2, 0.2], [0.1, 0.1, 0.8], [0.3, 0.5, 0.2]]</p>

  <p class="body"><a class="calibre8" id="pgfId-1135178"></a>Scenario B: Labels [0, 2, 1]</p>

  <p class="body"><a class="calibre8" id="pgfId-1135179"></a>Predictions: [[0.3, 0.3, 0.4], [0.4, 0.3, 0.3], [0.3, 0.3, 0.4]]</p>

  <p class="body"><a class="calibre8" id="pgfId-1135184"></a>Which one will have the lowest<a class="calibre8" id="marker-1135180"></a><a class="calibre8" id="marker-1135181"></a><a class="calibre8" id="marker-1135182"></a><a class="calibre8" id="marker-1135183"></a> perplexity?</p>

  <h2 class="fm-head" id="sigil_toc_id_143"><a id="pgfId-1135185"></a>10.4 Training and evaluating the language model</h2>

  <p class="body"><a class="calibre8" id="pgfId-1135191"></a>In<a class="calibre8" id="marker-1135186"></a><a class="calibre8" id="marker-1135187"></a><a class="calibre8" id="marker-1135188"></a><a class="calibre8" id="marker-1135189"></a><a class="calibre8" id="marker-1135190"></a> this section, we will train the model. Before training the model, let’s instantiate the training and validation data sets using the previously implemented <span class="fm-code-in-text">get_tf_pipeline()</span> function. We will only use the first 50 stories (out of a total of 98) in the training set to save time. We will take a sequence of 100 bigrams at a time and hop the story by shifting the window by 25 bigrams. This means the starting index of the sequences for a single story is 0, 25, 50, . . ., and so on. We will use a batch size of 128:</p>
  <pre class="programlisting">n_seq = 100
train_ds = get_tf_pipeline(
    train_data_seq[:50], n_seq, stride=25, batch_size=128
)
valid_ds = get_tf_pipeline(
    val_data_seq, n_seq, stride=n_seq, batch_size=128
)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1135200"></a>To train the model, we will define the callbacks as before. We will define</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1135201"></a>A CSV logger that will log performance over time during training</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1135202"></a>A learning rate scheduler that will reduce the learning rate when performance has plateaued</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1135203"></a>An early stopping callback to terminate the training if the performance is not improving</p>
    </li>
  </ul>
  <pre class="programlisting">os.makedirs('eval', exist_ok=True)
 
csv_logger = 
<span class="fm-code-continuation-arrow">➥</span> tf.keras.callbacks.CSVLogger(os.path.join('eval','1_language_modelling.
<span class="fm-code-continuation-arrow">➥</span> log'))
 
monitor_metric = 'val_perplexity'
mode = 'min' 
print("Using metric={} and mode={} for EarlyStopping".format(monitor_metric, mode))
 
 
lr_callback = tf.keras.callbacks.ReduceLROnPlateau(
    monitor=monitor_metric, factor=0.1, patience=2, mode=mode, min_lr=1e-8
)
 
es_callback = tf.keras.callbacks.EarlyStopping(
    monitor=monitor_metric, patience=5, mode=mode, 
<span class="fm-code-continuation-arrow">➥</span> restore_best_weights=False
)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1135220"></a>Finally, it’s time to train the model. I wonder what sort of cool stories I can squeeze out from the trained model:</p>
  <pre class="programlisting">model.fit(train_ds, epochs=50,  validation_data = valid_ds, 
<span class="fm-code-continuation-arrow">➥</span> callbacks=[es_callback, lr_callback, csv_logger])</pre>

  <p class="fm-callout"><a id="pgfId-1135222"></a><span class="fm-callout-head">NOTE</span> On an Intel Core i5 machine with an NVIDIA GeForce RTX 2070 8 GB, the training took approximately 1 hour and 45 minutes to run 25 epochs.</p>

  <p class="body"><a class="calibre8" id="pgfId-1135223"></a>After training the model, you will see a validation perplexity close to 9.5. In other words, this means that, for a given word sequence, the model thinks there can be 9.5 different next words that are the right word (not exactly, but it is a close-enough approximation). Perplexity needs to be judged carefully as the goodness of the measure tends to be subjective. For example, as the size of the vocabulary increases, this number can go up. But this doesn’t necessarily mean that the model is bad. The number can go up because the model has seen more words that fit the occasion compared to when the vocabulary was smaller.</p>

  <p class="body"><a class="calibre8" id="pgfId-1135224"></a>We will evaluate the model on the test data to gauge how well our model can anticipate some of the unseen stories without being surprised:</p>
  <pre class="programlisting">batch_size = 128
test_ds = get_tf_pipeline(
    test_data_seq, n_seq, shift=n_seq, batch_size=batch_size
)
model.evaluate(test_ds)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1135230"></a>This will give you approximately</p>
  <pre class="programlisting">61/61 [==============================] - 2s 39ms/step - loss: 2.2620 - 
<span class="fm-code-continuation-arrow">➥</span> accuracy: 0.4574 - perplexity: 10.5495</pre>

  <p class="body"><a class="calibre8" id="pgfId-1135233"></a>which is on par with the validation performance we saw. Finally, save the model with</p>
  <pre class="programlisting">os.makedirs('models', exist_ok=True)
tf.keras.models.save_model(model, os.path.join('models', '2_gram_lm.h5'))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1135237"></a>In this section, you learned to train and evaluate the model. You trained the model on the training data set and evaluated the model on validation and testing sets. In the next section, you will learn how to use the trained model to generate new children’s stories. Then, in the following section, you will learn how to generate text with the model we just trained.</p>

  <p class="fm-head2"><a id="pgfId-1135238"></a>Exercise 4</p>

  <p class="body"><a class="calibre8" id="pgfId-1135240"></a>Assume you want to use the validation accuracy (<span class="fm-code-in-text">val_accuracy</span><a class="calibre8" id="marker-1135239"></a>) instead of validation perplexity (<span class="fm-code-in-text">val_perlexity</span><a class="calibre8" id="marker-1135241"></a>) to define the early stopping callback. How would you change the following<a class="calibre8" id="marker-1135242"></a><a class="calibre8" id="marker-1135243"></a><a class="calibre8" id="marker-1135244"></a><a class="calibre8" id="marker-1135245"></a><a class="calibre8" id="marker-1135246"></a> callback?</p>
  <pre class="programlisting">es_callback = tf.keras.callbacks.EarlyStopping(
    monitor=’val_perlexity’, patience=5, mode=’min’, 
<span class="fm-code-continuation-arrow">➥</span> restore_best_weights=False
)</pre>

  <h2 class="fm-head" id="sigil_toc_id_144"><a id="pgfId-1135250"></a>10.5 Generating new text from the language model: Greedy decoding</h2>

  <p class="body"><a class="calibre8" id="pgfId-1135255"></a>One<a class="calibre8" id="marker-1135251"></a><a class="calibre8" id="marker-1135252"></a><a class="calibre8" id="marker-1135253"></a><a class="calibre8" id="marker-1135254"></a> of the coolest things about a language model is the generative property it possesses. This means that the model can generate new data. In our case, the language model can generate new children’s stories using the knowledge it garnered from the training phrase.</p>

  <p class="body"><a class="calibre8" id="pgfId-1135256"></a>But to do so, we have to use some extra elbow grease. The text-generation process is different from the training process. During the training, we had the full sequence end to end. This means you can process a sequence of arbitrary length in one go. But when generating new text, you don’t have a sequence of text available to you; in fact, you are trying to generate one. You start with a random word or words and get an output word, and then recursively feed the current output as the next input to generate new text. In order to facilitate this process, we need to define a new version of the trained model. Let’s flesh out the generative process a bit more. Figure 10.5 compares the training process versus the generation/inference process.</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1135263"></a>Define an initial word <i class="fm-timesitalic1">w</i><sub class="fm-subscript">t</sub> (random word from the vocabulary).</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1135264"></a>Define an initial state vector <i class="fm-timesitalic1">h</i><sub class="fm-subscript">t</sub> (initialized with zeros).</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1135265"></a>Define a list, <span class="fm-code-in-text">words</span>, to hold the predicted words and initialize it with the initial word.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1135266"></a>For t from 1 to n:</p>

      <ul class="calibre18">
        <li class="fm-sidebar-bullet-sub">
          <p class="list"><a class="calibre8" id="pgfId-1135267"></a>Get the next word (<i class="fm-timesitalic1">w</i><sub class="fm-subscript">t + 1</sub>) and the state vector (<i class="fm-timesitalic1">h</i><sub class="fm-subscript">t + 1</sub>) from the model and assign to <i class="fm-timesitalic1">w</i><sub class="fm-subscript">t</sub> and <i class="fm-timesitalic1">h</i><sub class="fm-subscript">t</sub>, respectively. This creates a recursive process, enabling us to generate as many words as we like.</p>
        </li>

        <li class="fm-sidebar-bullet-sub">
          <p class="list"><a class="calibre8" id="pgfId-1135268"></a>Append the new word to <span class="fm-code-in-text">words.</span></p>
        </li>
      </ul>
    </li>
  </ul>

  <p class="fm-figure"><img alt="10-05" class="calibre10" src="../../OEBPS/Images/10-05.png" width="1069" height="550"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1151717"></a>Figure 10.5 Comparison between the language model at training time and the inference/decoding phrase. In the inference phase, we predict one time step at a time. In each time step, we get the predicted word as the input and the new hidden state as the previous hidden state for the next time step.</p>

  <p class="body"><a class="calibre8" id="pgfId-1135269"></a>We will use the Keras functional API to build this model, as shown in the next listing. First, let’s define two inputs.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1135271"></a>Listing 10.6 Implementation of the inference/decoding language model</p>
  <pre class="programlisting">inp = tf.keras.layers.Input(shape=(None,))                                 <span class="fm-combinumeral">❶</span>
inp_state = tf.keras.layers.Input(shape=(1024,))                           <span class="fm-combinumeral">❷</span>
 
emb_layer = tf.keras.layers.Embedding(
    input_dim=n_vocab+1, output_dim=512, input_shape=(None,)
)                                                                          <span class="fm-combinumeral">❸</span>
emb_out = emb_layer(inp)                                                   <span class="fm-combinumeral">❹</span>
 
gru_layer = tf.keras.layers.GRU(
    1024, return_state=True, return_sequences=True
)
gru_out, gru_state = gru_layer(emb_out, initial_state=inp_state)         <span class="fm-combinumeral">❺❻</span>
 
dense_layer = tf.keras.layers.Dense(512, activation='relu')                <span class="fm-combinumeral">❼</span>
dense_out = dense_layer(gru_out)                                           <span class="fm-combinumeral">❼</span>
 
final_layer = tf.keras.layers.Dense(n_vocab, name='final_out')             <span class="fm-combinumeral">❽</span>
final_out = final_layer(dense_out)                                         <span class="fm-combinumeral">❽</span>
softmax_out = tf.keras.layers.Activation(activation='softmax')(final_out)  <span class="fm-combinumeral">❽</span>
 
infer_model = tf.keras.models.Model(
    inputs=[inp, inp_state], outputs=[softmax_out, gru_state]
)                                                                          <span class="fm-combinumeral">❾</span></pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1148874"></a><span class="fm-combinumeral">❶</span> Define an input that can take an arbitrarily long sequence of word IDs.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1148898"></a><span class="fm-combinumeral">❷</span> Define another input that will feed in the previous state.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1148915"></a><span class="fm-combinumeral">❸</span> Define an embedding layer.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1148932"></a><span class="fm-combinumeral">❹</span> Get the embedding vectors from the input word ID.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1148949"></a><span class="fm-combinumeral">❺</span> Define a GRU layer that returns both the output and the state. However, note that they will be the same for a GRU.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1148966"></a><span class="fm-combinumeral">❻</span> Get the GRU output and the state from the model.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1148983"></a><span class="fm-combinumeral">❼</span> Compute the first fully connected layer output.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1149000"></a><span class="fm-combinumeral">❽</span> Define a final layer that is the same size as the vocabulary and get the final output of the model.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1149017"></a><span class="fm-combinumeral">❾</span> Define the final model that takes an input and a state vector as the inputs and produces the next word prediction and the new state vector as the outputs.</p>

  <p class="body"><a class="calibre8" id="pgfId-1135305"></a>We have to perform an important step after defining the model. We must transfer the weights from the trained model to the newly defined inference model. For that, we have to identify layers with trainable weights, get the weights for those layers from the trained model, and assign them to the new model:</p>
  <pre class="programlisting"># Copy the weights from the original model
emb_layer.set_weights(model.get_layer('embedding').get_weights())
gru_layer.set_weights(model.get_layer('gru').get_weights())
dense_layer.set_weights(model.get_layer('dense').get_weights())
final_layer.set_weights(model.get_layer('final_out').get_weights())</pre>

  <p class="body"><a class="calibre8" id="pgfId-1135313"></a>To get weights of a specific layer in the trained model, you can call</p>
  <pre class="programlisting">model.get_layer(&lt;layer name&gt;).get_weights()</pre>

  <p class="body"><a class="calibre8" id="pgfId-1135317"></a>This will return a NumPy array with the weights. Next, to assign those weights to a layer, call</p>
  <pre class="programlisting">layer.set_weights(&lt;weight matrix&gt;)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1135321"></a>We can now call the newly defined model recursively to generate as many bigrams as we like. We will discuss the process to do that in more detail. Instead of starting with a single random word, let’s start with a sequence of text. We will convert the text to bigrams and subsequently to word IDs using the <span class="fm-code-in-text">Tokenizer</span>:</p>
  <pre class="programlisting">text = get_ngrams(
    "CHAPTER I. Down the Rabbit-Hole Alice was beginning to get very tired 
<span class="fm-code-continuation-arrow">➥</span> of sitting by her sister on the bank ,".lower(), 
    ngrams
)
 
seq = tokenizer.texts_to_sequences([text])</pre>

  <p class="body"><a class="calibre8" id="pgfId-1135329"></a>Next, let’s reset the states of the model (this is not needed here because we’re starting fresh, but it’s good to know that we can do that). We will define a state vector with all zeros:</p>
  <pre class="programlisting"># Reset the state of the model initially
model.reset_states()
# Defining the initial state as all zeros
state = np.zeros(shape=(1,1024))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1135337"></a>Then we will recursively predict on each bigram in the <span class="fm-code-in-text">seq</span> variable<a class="calibre8" id="marker-1135336"></a> to update the state of the GRU model with the input sequence. Once we sweep through the whole sequence, we will get the final predicted bigram (that will be our first predicted bigram) and append that to the original bigram sequence:</p>
  <pre class="programlisting"># Recursively update the model by assining new state to state
for c in seq[0]:    
    out, state = infer_model.predict([np.array([[c]]), state])
 
# Get final prediction after feeding the input string
wid = int(np.argmax(out[0],axis=-1).ravel())
word = tokenizer.index_word[wid]
text.append(word)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1135346"></a>We will define a new input <span class="fm-code-in-text">x</span> with the last word ID that was predicted:</p>
  <pre class="programlisting"># Define first input to generate text recursively from
x = np.array([[wid]])</pre>

  <p class="body"><a class="calibre8" id="pgfId-1135351"></a>The fun begins now. We will predict 500 bigrams (i.e., 1,000 characters) using the approach discussed earlier. In every iteration, we predict a new bigram and the new state with the <span class="fm-code-in-text">infer_model</span> using the input <span class="fm-code-in-text">x</span> and the <span class="fm-code-in-text">state</span> vector state<a class="calibre8" id="marker-1135352"></a>. The new bigram and the new state recursively replace <span class="fm-code-in-text">x</span> and <span class="fm-code-in-text">state</span> variables<a class="calibre8" id="marker-1135353"></a> with these new outputs (see the next listing).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1135354"></a>Listing 10.7 Recursively predicting a new word using the previous word as an input</p>
  <pre class="programlisting">for _ in range(500):
    
    out, state = infer_model.predict([x, state])                   <span class="fm-combinumeral">❶</span>
    
    out_argsort = np.argsort(out[0], axis=-1).ravel()              <span class="fm-combinumeral">❷</span>
    wid = int(out_argsort[-1])                                     <span class="fm-combinumeral">❷</span>
    word = tokenizer.index_word[wid]                               <span class="fm-combinumeral">❷</span>
    
    
    if word.endswith(' '):                                         <span class="fm-combinumeral">❸</span>
        if np.random.normal()&gt;0.5:
            width = 3                                              <span class="fm-combinumeral">❹</span>
            i = np.random.choice(                                  <span class="fm-combinumeral">❹</span>
                list(range(-width,0)), 
                p=out_argsort[-width:]/out_argsort[-width:].sum()
            )    
            wid = int(out_argsort[i])                              <span class="fm-combinumeral">❹</span>
            word = tokenizer.index_word[wid]                       <span class="fm-combinumeral">❹</span>
    text.append(word)                                              <span class="fm-combinumeral">❺</span>
     
    x = np.array([[wid]])                                          <span class="fm-combinumeral">❻</span></pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1148327"></a><span class="fm-combinumeral">❶</span> Get the next output and state.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1148348"></a><span class="fm-combinumeral">❷</span> Get the word ID and the word from out.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1148368"></a><span class="fm-combinumeral">❸</span> If the word ends with space, we introduce a bit of randomness to break repeating text.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1148385"></a><span class="fm-combinumeral">❹</span> Essentially pick one of the top three outputs for that timestep depending on their likelihood.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1148402"></a><span class="fm-combinumeral">❺</span> Append the prediction cumulatively to text.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1148419"></a><span class="fm-combinumeral">❻</span> Recursively make the current prediction the next input.</p>

  <p class="body"><a class="calibre8" id="pgfId-1135384"></a>Note that a little bit of work is required to get the final value of <span class="fm-code-in-text">x</span> as the model predicts a probability prediction (assigned to <span class="fm-code-in-text">out</span>) as its output, not a word ID. Furthermore, we’re going to use some additional logic to improve the randomness (or entropy, one could say) in the generated text by picking a word randomly from the top three words. But we don’t pick them with equal probability. Rather, let’s use their predicted probabilities to predict the word. To make sure we don’t get too much randomness and to avoid getting random tweaks in the middle of words, let’s do this only when the last character is a space character. The final word ID (picked either as the word with the highest probability or at random) is assigned to the variable <span class="fm-code-in-text">x</span>. This process will repeat for 500 steps, and by the end, you’ll have a cool machine-generated story on your hands. You can print the final text and see how it looks. To do that, simply join the bigrams in the text sequence as follows:</p>
  <pre class="programlisting"># Print the final output    
print('\n')
print('='*60)
print("Final text: ")
print(''.join(text))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1135393"></a>This will display</p>
  <pre class="programlisting">Final text: 
chapter i. down the rabbit-hole alice was beginning to get very tired of 
<span class="fm-code-continuation-arrow">➥</span> sitting by her sister on the bank , and then they went to the shore , 
<span class="fm-code-continuation-arrow">➥</span> and then the princess was so stilling that he was a little girl , 
 
...
 
 it 's all right , and i 'll tell you how young things would n't be able to 
<span class="fm-code-continuation-arrow">➥</span> do it .
 i 'm not goin ' to think of itself , and i 'm going to be sure to see you .
 i 'm sure i can notice them .
 i 'm going to see you again , and i 'll tell you what i 've got , '</pre>

  <p class="body"><a class="calibre8" id="pgfId-1135405"></a>It’s certainly not bad for a simple single layer GRU model. Most of the time, the model spits out actual words. But there are occasional spelling mistakes and more frequent grammatical errors haunting the text. Can we do better? In the next section, we are going to learn about a new technique to generate text called beam search.</p>

  <p class="fm-head2"><a id="pgfId-1135406"></a>Exercise 5</p>

  <p class="body"><a class="calibre8" id="pgfId-1135407"></a>Assume you have the following code that chooses the next word greedily without randomness. You run this code and realize that the results are very bad:</p>
  <pre class="programlisting">for _ in range(500):
    
    out, new_s = infer_model.predict([x, s])                                    
    
    out_argsort = np.argsort(out[0], axis=-1).ravel()                               
    wid = int(out_argsort[-1])                                                      
    word = tokenizer.index_word[wid]
            
    text.append(word)                                                               
    
    x = np.array([[wid]])       </pre>

  <p class="body"><a class="calibre8" id="pgfId-1135424"></a>What do you think is the reason for the poor<a class="calibre8" id="marker-1135420"></a><a class="calibre8" id="marker-1135421"></a><a class="calibre8" id="marker-1135422"></a><a class="calibre8" id="marker-1135423"></a> performance?</p>

  <h2 class="fm-head" id="sigil_toc_id_145"><a id="pgfId-1135425"></a>10.6 Beam search: Enhancing the predictive power of sequential models</h2>

  <p class="body"><a class="calibre8" id="pgfId-1135430"></a>We<a class="calibre8" id="marker-1135426"></a><a class="calibre8" id="marker-1135427"></a><a class="calibre8" id="marker-1135428"></a><a class="calibre8" id="marker-1135429"></a> can do better that greedy decoding. Beam search is a popular decoding algorithm used in sequential/time-series tasks like this to generate more accurate predictions. The idea behind beam search is very simple. Unlike in greedy decoding, where you predict a single timestep, with beam search you predict several time steps ahead. In each time step, you get the top k predictions and branch out from them. Beam search has two important parameters: beam width and beam depth. Beam width controls how many candidates are considered at each step, whereas beam depth determines how many steps to search. For example, for a beam width of 3 and a beam depth of 5, the number of possible options become 3<sup class="fm-superscript">5</sup> = 243. Figure 10.6 further illustrates how beam search works.</p>

  <p class="fm-figure"><img alt="10-06" class="calibre10" src="../../OEBPS/Images/10-06.png" width="900" height="686"/><br class="calibre2"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1151751"></a>Figure 10.6 Beam search in action. Beam search leads to better solutions as it looks several steps into the future to make a prediction. Here, we are performing a beam search with beam width = 3 and beam depth = 5.</p>

  <p class="body"><a class="calibre8" id="pgfId-1135437"></a>First let’s define a function that will take a model, input, and state and return the output and the new state:</p>
  <pre class="programlisting">def beam_one_step(model, input_, state):    
    """ Perform the model update and output for one step"""
    output, new_state = model.predict([input_, state])
    return output, new_state</pre>

  <p class="body"><a class="calibre8" id="pgfId-1135443"></a>Then, using this function, we will define a recursive function (<span class="fm-code-in-text">recursive_fn</span>) that will recursively predict the next word from the previous prediction for a predefined depth (defined by <span class="fm-code-in-text">beam_depth</span>). At each time step, we consider the top k candidates (defined by <span class="fm-code-in-text">beam_width</span>) to branch out from. The <span class="fm-code-in-text">recursive_fn</span> function<a class="calibre8" id="marker-1135444"></a> will populate a variable called <span class="fm-code-in-text">results</span>. <span class="fm-code-in-text">results</span> will contain a list of tuples, where each tuple represents a single path in the search. Specifically, each tuple contains the</p>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1135447"></a>Elements in the path</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1135448"></a>The joint log probability of that sequence</p>
    </li>

    <li class="fm-list-bullet-last">
      <p class="list"><a class="calibre8" id="pgfId-1135449"></a>The final state vector to pass to the GRU</p>
    </li>
  </ul>

  <p class="body"><a class="calibre8" id="pgfId-1135450"></a>This function is outlined in the following listing.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1135452"></a>Listing 10.8 Implementation of the beam search as a recursive function</p>
  <pre class="programlisting">def beam_search(
    model, input_, state, beam_depth=5, beam_width=3, ignore_blank=True
):                                                                           <span class="fm-combinumeral">❶</span>
    """ Defines an outer wrapper for the computational function of beam 
<span class="fm-code-continuation-arrow">➥</span> search """
    
    def recursive_fn(input_, state, sequence, log_prob, i):                  <span class="fm-combinumeral">❷</span>
        """ This function performs actual recursive computation of the long 
<span class="fm-code-continuation-arrow">➥</span> string"""
        
        if i == beam_depth:                                                  <span class="fm-combinumeral">❸</span>
            """ Base case: Terminate the beam search """
            results.append((list(sequence), state, np.exp(log_prob)))        <span class="fm-combinumeral">❹</span>
            return sequence, log_prob, state                                 <span class="fm-combinumeral">❹</span>
        else:
            """ Recursive case: Keep computing the output using the 
<span class="fm-code-continuation-arrow">➥</span> previous outputs"""
            output, new_state = beam_one_step(model, input_, state)          <span class="fm-combinumeral">❺</span>
            
            # Get the top beam_widht candidates for the given depth
            top_probs, top_ids = tf.nn.top_k(output, k=beam_width)           <span class="fm-combinumeral">❻</span>
            top_probs, top_ids = top_probs.numpy().ravel(), 
<span class="fm-code-continuation-arrow">➥</span> top_ids.numpy().ravel()                                                   <span class="fm-combinumeral">❻</span>
            
            # For each candidate compute the next prediction
            for p, wid in zip(top_probs, top_ids):                           <span class="fm-combinumeral">❼</span>
                new_log_prob = log_prob + np.log(p)                          <span class="fm-combinumeral">❼</span>
                if len(sequence)&gt;0 and wid == sequence[-1]:                  <span class="fm-combinumeral">❽</span>
                    new_log_prob = new_log_prob + np.log(1e-1)               <span class="fm-combinumeral">❽</span>
                    
                sequence.append(wid)                                         <span class="fm-combinumeral">❾</span>
                _ = recursive_fn(
                    np.array([[wid]]), new_state, sequence, new_log_prob, i+1<span class="fm-combinumeral">❿</span>
                )                                         
                sequence.pop()
        
    results = []
    sequence = []
    log_prob = 0.0
    recursive_fn(input_, state, sequence, log_prob, 0)                       <span class="fm-combinumeral">⓫</span>
 
    results = sorted(results, key=lambda x: x[2], reverse=True)              <span class="fm-combinumeral">⓬</span>
 
    return results</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1147363"></a><span class="fm-combinumeral">❶</span> Define an outer wrapper for the computational function of beam search.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1147384"></a><span class="fm-combinumeral">❷</span> Define an inner function that is called recursively to find the beam paths.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1147401"></a><span class="fm-combinumeral">❸</span> Define the base case for terminating the recursion.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1147418"></a><span class="fm-combinumeral">❹</span> Append the result we got at the termination so we can use it later.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1147435"></a><span class="fm-combinumeral">❺</span> During recursion, get the output word and the state by calling the model.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1147452"></a><span class="fm-combinumeral">❻</span> Get the top k candidates for that step.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1147469"></a><span class="fm-combinumeral">❼</span> For each candidate, compute the joint probability. We will do this in log space to have numerical stability.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1147486"></a><span class="fm-combinumeral">❽</span> Penalize joint probability whenever the same symbol repeats.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1147503"></a><span class="fm-combinumeral">❾</span> Append the current candidate to the sequence that maintains the current search path at the time.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1147520"></a><span class="fm-combinumeral">❿</span> Call the function recursively to find the next candidates.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1147537"></a><span class="fm-combinumeral">⓫</span> Make a call to the recursive function to trigger the recursion.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1147554"></a><span class="fm-combinumeral">⓬</span> Sort the results by log probability.</p>

  <p class="body"><a class="calibre8" id="pgfId-1135507"></a>Finally, we can use this <span class="fm-code-in-text">beam_search</span> function<a class="calibre8" id="marker-1135506"></a> as follows: we will use a beam depth of 7 and a beam width of 2. Up until the <span class="fm-code-in-text">for</span> loop, things are identical to how we did things using greedy decoding. In the <span class="fm-code-in-text">for</span> loop, we get the results list (sorted high to low on joint probability). Then, similar to what we did previously, we’ll get the next prediction randomly from the top 10 predictions based on their likelihood as the next prediction. The following listing delineates the code to do so.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1135509"></a>Listing 10.9 Implementation of beam search decoding to generate a new story</p>
  <pre class="programlisting">text = get_ngrams(
    "CHAPTER I. Down the Rabbit-Hole Alice was beginning to get very tired 
<span class="fm-code-continuation-arrow">➥</span> of sitting by her sister on the bank ,".lower(),     
    ngrams
)                                                                <span class="fm-combinumeral">❶</span>
 
seq = tokenizer.texts_to_sequences([text])                       <span class="fm-combinumeral">❷</span>
 
state = np.zeros(shape=(1,1024))
for c in seq[0]:    
    out, state = infer_model.predict([np.array([[c]]), state     <span class="fm-combinumeral">❸</span>
 
wid = int(np.argmax(out[0],axis=-1).ravel())                     <span class="fm-combinumeral">❹</span>
word = tokenizer.index_word[wid]                                 <span class="fm-combinumeral">❹</span>
text.append(word)                                                <span class="fm-combinumeral">❹</span>
 
x = np.array([[wid]])
 
for i in range(100):                                             <span class="fm-combinumeral">❺</span>
    
    result = beam_search(infer_model, x, state, 7, 2)            <span class="fm-combinumeral">❻</span>
    
    n_probs = np.array([p for _,_,p in result[:10                <span class="fm-combinumeral">❼</span>
    p_j = np.random.choice(list(range(
       n_probs.size)), p=n_probs/n_probs.sum())                  <span class="fm-combinumeral">❼</span>
 
    best_beam_ids, state, _ = result[p_j]                        <span class="fm-combinumeral">❽</span>
    x = np.array([[best_beam_ids[-1]]])                          <span class="fm-combinumeral">❽</span>
            
    text.extend([tokenizer.index_word[w] for w in best_beam_ids])
 
print('\n')
print('='*60)
print("Final text: ")
print(''.join(text))</pre>

  <p class="fm-code-annotation-mob"><a id="pgfId-1146636"></a><span class="fm-combinumeral">❶</span> Define a sequence of ngrams from an initial sequence of text.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1146655"></a><span class="fm-combinumeral">❷</span> Convert the bigrams to word IDs.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1146672"></a><span class="fm-combinumeral">❸</span> Build up model state using the given string.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1146689"></a><span class="fm-combinumeral">❹</span> Get the predicted word after processing the sequence.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1146706"></a><span class="fm-combinumeral">❺</span> Predict for 100 time steps.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1146723"></a><span class="fm-combinumeral">❻</span> Get the results from beam search.</p>

  <p class="fm-code-annotation-mob"><a id="pgfId-1146740"></a><span class="fm-combinumeral">❼</span> Get one of the top 10 results based on their likelihood.</p>

  <p class="fm-code-annotation-mob"><a class="calibre8" id="pgfId-1146757"></a><span class="fm-combinumeral">❽</span> Replace x and state with the new values computed.</p>

  <p class="body"><a class="calibre8" id="pgfId-1135554"></a>Running the code in listing 10.9, you should get text similar to the following:</p>
  <pre class="programlisting">Final text: 
 
chapter i. down the rabbit-hole alice was beginning to get very tired of 
<span class="fm-code-continuation-arrow">➥</span> sitting by her sister on the bank , and there was no reason that her 
<span class="fm-code-continuation-arrow">➥</span> father had brought him the story girl 's face .
 `` i 'm going to bed , '' said the prince , `` and you can not be able 
<span class="fm-code-continuation-arrow">➥</span> to do it . ''
 `` i 'm sure i shall have to go to bed , '' he answered , with a smile 
<span class="fm-code-continuation-arrow">➥</span> .
 `` i 'm so happy , '' she said .
 `` i do n't know how to bring you into the world , and i 'll be sure 
<span class="fm-code-continuation-arrow">➥</span> that you would have thought that it would have been a long time .
 there was no time to be able to do it , and it would have been a 
<span class="fm-code-continuation-arrow">➥</span> little thing . ''
 `` i do n't know , '' she said .
  
...
 
 `` what is the matter ? ''
 `` no , '' said anne , with a smile .
 `` i do n't know what to do , '' said mary .
 `` i 'm so glad you come back , '' said mrs. march , with</pre>

  <p class="body"><a class="calibre8" id="pgfId-1135571"></a>The text generated with beam search reads much better than the text we saw with greedy decoding. You see improved grammar and less spelling mistakes when text is generated with beam search.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre8" id="pgfId-1135572"></a>Diverse beam search</p>

    <p class="fm-sidebar-text"><a id="pgfId-1135573"></a>Different alternatives to beam search have surfaced over time. One popular alternative is called <i class="fm-italics">diverse beam search</i>, introduced in the paper, “Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models” by Vijayakumar et al. (<span class="fm-hyperlink"><a class="url" href="https://arxiv.org/pdf/1610.02424.pdf">https://arxiv.org/pdf/1610.02424.pdf</a></span>). Diverse beam search overcomes a critical limitation of vanilla beam search. That is, if you analyze the most preferred candidate sequences proposed by beam search, you’ll find that they differ only by a few elements. This also can lead to repeating text that lacks variety. Diverse beam search comes up with an optimization problem that incentivizes diversity of the proposed candidates during the search. You can read more about this in the paper.</p>
  </div>

  <p class="body"><a class="calibre8" id="pgfId-1135576"></a>This concludes our discussion on language modeling. In the next chapter, we will learn about a new type of NLP problem known as a sequence-to-sequence problem. Let’s summarize the key highlights of this chapter.</p>

  <p class="fm-head2"><a id="pgfId-1135577"></a>Exercise 6</p>

  <p class="body"><a class="calibre8" id="pgfId-1135578"></a>You used the line <span class="fm-code-in-text">result = beam_search(infer_model, x, state, 7, 2)</span> to perform beam search. You want to consider five candidates at a given time and search only three levels deep into the search space. How would you change the<a class="calibre8" id="marker-1135579"></a><a class="calibre8" id="marker-1135580"></a><a class="calibre8" id="marker-1135581"></a><a class="calibre8" id="marker-1135582"></a> line?</p>

  <h2 class="fm-head" id="sigil_toc_id_146"><a id="pgfId-1135583"></a>Summary</h2>

  <ul class="calibre9">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre8" id="pgfId-1135584"></a>Language modeling is the task of predicting the next word given a sequence of words.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1135585"></a>Language modeling is the workhorse of some of the top-performing models in the field, such as BERT (a type of Transformer-based model).</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1135586"></a>To limit the size of the vocabulary and avoid computational issues, n-gram representation can be used.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1135587"></a>In n-gram representation, text is split into fixed-length tokens, as opposed to doing character-level or word-level tokenization. Then, a fixed sized window is moved over the sequence of text to generate inputs and targets for the model. In TensorFlow, you can use the <span class="fm-code-in-text">tf.data.Dataset.window()</span> function to implement this functionality.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1135588"></a>The gated recurrent unit (GRU) is a sequential model that operates similarly to an LSTM, where it jumps from one input to the next in a sequence while generating a state at each time step.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1135589"></a>The GRU is a compact version of an LSTM model that maintains a single state and two gates but delivers on-par performance.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1135590"></a>Perplexity measures how surprised the model was to see the target word given the input sequence.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1135591"></a>The computation of the perplexity measure is inspired by information theory, in which the measure entropy is used to quantify the uncertainty in a random variable that represents an event where the outcomes are generated with some underlying probability distribution.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a class="calibre8" id="pgfId-1135592"></a>The language models after training can be used to generate new text. There are two popular techniques—greedy decoding and beam search decoding:</p>

      <ul class="calibre18">
        <li class="fm-sidebar-bullet-sub">
          <p class="list"><a class="calibre8" id="pgfId-1135593"></a>Greedy decoding predicts one word at a time, where the predicted word is used as the input in the next time step.</p>
        </li>

        <li class="fm-sidebar-bullet-sub">
          <p class="list"><a class="calibre8" id="pgfId-1135594"></a>Beam search decoding predicts several steps into the future and selects the sequence that gives the highest joint probability.</p>
        </li>
      </ul>
    </li>
  </ul>

  <h2 class="fm-head" id="sigil_toc_id_147"><a id="pgfId-1135595"></a>Answers to exercises</h2>

  <p class="body"><a class="calibre8" id="pgfId-1135596"></a><b class="fm-bold">Exercise 1</b></p>
  <pre class="programlisting">ds = tf.data.Dataset.from_tensor_slices(x)
ds = ds.window(5,shift=5).flat_map(
    lambda window: window.batch(5, drop_remainder=True)
)
ds = ds.map(lambda xx: (xx[:-2], xx[2:]))</pre>

  <p class="body"><a class="calibre8" id="pgfId-1135604"></a><b class="fm-bold">Exercise 2</b></p>
  <pre class="programlisting">model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(
        input_dim=n_vocab+1, output_dim=512,input_shape=(None,)   
    ),
    
    tf.keras.layers.GRU(1024, return_state=False, return_sequences=True), 
    tf.keras.layers.GRU(512, return_state=False, return_sequences=True), 
    tf.keras.layers.Dense(n_vocab, activation=’softmax’, name='final_out'),
])</pre>

  <p class="body"><a class="calibre8" id="pgfId-1135616"></a><b class="fm-bold">Exercise 3</b></p>

  <p class="body"><a class="calibre8" id="pgfId-1135618"></a>Scenario A will have the lowest perplexity.</p>

  <p class="body"><a class="calibre8" id="pgfId-1135620"></a><b class="fm-bold">Exercise 4</b></p>
  <pre class="programlisting">es_callback = tf.keras.callbacks.EarlyStopping(
    monitor=’val_accuracy’, patience=5, mode=’max’, restore_best_weights=False
)</pre>

  <p class="body"><a class="calibre8" id="pgfId-1135626"></a><b class="fm-bold">Exercise 5</b></p>

  <p class="body"><a class="calibre8" id="pgfId-1135627"></a>The line <span class="fm-code-in-text">out, new_s = infer_model.predict([x, s])</span>, is wrong. The state is not recursively updated in the inference model. This will lead to a working model but with poor performance. It should be corrected as <span class="fm-code-in-text">out, s = infer_model.predict([x, s])</span>.</p>

  <p class="body"><a class="calibre8" id="pgfId-1135631"></a><b class="fm-bold">Exercise 6</b><a class="calibre8" id="marker-1135628"></a><a class="calibre8" id="marker-1135629"></a><a class="calibre8" id="marker-1135630"></a></p>
  <pre class="programlisting">result = beam_search(infer_model, x, state, 3, 5)</pre>
</div>
</div>
</body>
</html>